{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuuTEX1uxK-Y"
      },
      "source": [
        "# Introduction to Neural Networks:\n",
        "\n",
        "## Author:\n",
        "### [Dr. Rahul Remanan](https://www.linkedin.com/in/rahulremanan)\n",
        "\n",
        "### CEO, [Moad Computer](https://www.moad.computer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeV1hQ30OFlu"
      },
      "source": [
        "This is a hands-on workshop notebook on deep-learning using python 3. In this notebook, we will learn how to implement a neural network from scratch using numpy. Once we have implemented this network, we will visualize the predictions generated by the neural network and compare it with a logistic regression model, in the form of classification boundaries. This workshop aims to provide an intuitive understanding of neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggyt4xRWOOn0"
      },
      "source": [
        "In practical code development, there is seldom an use case for building a neural network from scratch. Neural networks in real-world are typically implemented using a deep-learning framework such as tensorflow. But, building a neural network with very minimal dependencies helps one gain an understanding of how neural networks work. This understanding is essential to designing effective neural network models. Also, towards the end of the session, we will use tensorflow deep-learning library to build a neural network, to illustrate the importance of building a neural network using a deep-learning framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVbiniy39JLk"
      },
      "source": [
        "### Architecture of the basic XOR gate neural network:\n",
        "\n",
        "![Artificial neural network architecture](https://github.com/rahulremanan/python_tutorial/raw/master/Fundamentals_of_deep-learning/media/Artificial_neural_network.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mRjefwXNXLV"
      },
      "source": [
        "### XOR gate problem and neural networks -- Background:\n",
        "\n",
        "[The XOR gate is an interesting problem in neural networks](http://www.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node19.html). [Marvin Minsky](https://en.wikipedia.org/wiki/Marvin_Minsky) and [Samuel Papert](https://en.wikipedia.org/wiki/Seymour_Papert) in their book [ 'Perceptrons' (1969)](https://en.wikipedia.org/wiki/Perceptrons_(book) showed that the XOR gate cannot be solved using a two layer perceptron, since the solution for a XOR gate was not linearly separable. This conclusion lead to a significantly reduced interest in[ Frank Rosenblatt's](https://en.wikipedia.org/wiki/Frank_Rosenblatt) perceptrons as a mechanism for building artificial intelligence applications.\n",
        "\n",
        "Some of these earliest work in AI were using networks or circuits of connected units to simulate intelligent behavior. Examples of this kind of work are called \"connectionism\". [After the publication of 'Perceptrons', the interest in connectionism significantly reduced](https://en.wikipedia.org/wiki/AI_winter#The_abandonment_of_connectionism_in_1969), till the renewed interest following the works of [John Hopfield](https://en.wikipedia.org/wiki/John_Hopfield) and [David Rumelhart](https://en.wikipedia.org/wiki/David_Rumelhart).\n",
        "\n",
        "The assertions in the book 'Perceptrons' by Minsky was inspite of his thorough knowledge that the powerful perceptrons have multiple layers and that Rosenblatt's basic feed-forward perceptrons have three layers. In the book, to deceive unsuspecting readers, Minsky defined a perceptron as a two-layer machine that can handle only linearly separable problems and, for example, cannot solve the exclusive-OR problem. [The Minsky-Papert collaboation is now believed to be a political maneuver and a hatchet job for contract funding by some knowledgeable scientists](http://csis.pace.edu/~ctappert/srd2011/rosenblatt-contributions.htm). This strong, unidimensional and misplaced criticism of perceptrons essentially halted work on practical, powerful artificial intelligence systems that were based on neural-networks for nearly a decade.\n",
        "\n",
        "Part 1 of this notebook explains how to build a very basic neural network in numpy. This perceptron like neural network is trained to predict the output of a [XOR gate](https://en.wikipedia.org/wiki/XOR_gate).\n",
        "\n",
        "![CMOS XOR Gate](https://github.com/rahulremanan/python_tutorial/raw/master/Fundamentals_of_deep-learning/media/CMOS_XOR_Gate.png)\n",
        "\n",
        "#### XOR gate table:\n",
        "\n",
        "![XOR Gate Table](https://github.com/rahulremanan/python_tutorial/raw/master/Fundamentals_of_deep-learning/media/XOR_Gate_Table.png)\n",
        "\n",
        "#### Image below shows an example of a lienarly separable dataset:\n",
        "\n",
        "![Linearly separable points](https://github.com/rahulremanan/python_tutorial/raw/master/Fundamentals_of_deep-learning/media/linearly_spearable_points.gif)\n",
        "\n",
        "#### Image below shows the XOR gate problem and no linear separation:\n",
        "\n",
        "![XOR problem](https://github.com/rahulremanan/python_tutorial/raw/master/Fundamentals_of_deep-learning/media/XOR_gate.gif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dot product\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "a_{11} & a_{12} & ... & a_{1n} \\\\\n",
        "a_{21} & a_{22} & ... & a_{2n} \\\\\n",
        "... \\\\\n",
        "a_{m1} & a_{m2} & ... & a_{mn} \\\\\n",
        "\\end{bmatrix}\n",
        "̇\n",
        "\\begin{bmatrix}\n",
        "b_{11} & b_{12} & ... & b_{1p} \\\\\n",
        "b_{21} & b_{22} & ... & b_{2p} \\\\\n",
        "... \\\\\n",
        "b_{n1} & b_{n2} & ... & b_{np} \\\\\n",
        "\\end{bmatrix}\n",
        "=\\\\\n",
        "\\begin{bmatrix}\n",
        "\\Sigma_{i=1}^na_{1i} \\times b_{i1} & \\Sigma_{i=1}^na_{1i} \\times b_{i2} & ... & \\Sigma_{i=1}^na_{1i} \\times b_{ip} \\\\\n",
        "\\Sigma_{i=1}^na_{2i} \\times b_{i1} & \\Sigma_{i=1}^na_{2i} \\times b_{i2} & ... & \\Sigma_{i=1}^na_{2i} \\times b_{ip} \\\\\n",
        "... \\\\\n",
        "\\Sigma_{i=1}^na_{mi} \\times b_{i1} & \\Sigma_{i=1}^na_{mi} \\times b_{i2} & ... & \\Sigma_{i=1}^na_{mi} \\times b_{ip} \\\\\n",
        "\\end{bmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "WxBnIX3B_C0w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWc9ZHloax_2"
      },
      "source": [
        "## Part 01a -- Simple neural network as XOR gate using sigmoid activation function:\n",
        "\n",
        "\n",
        "The XOR gate neural network implemention uses a two layer perceptron with sigmoid activation function. This portion of the notebook is a modified fork of the [neural network implementation in numpy by Milo Harper](https://github.com/miloharper/simple-neural-network)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvLSvFnXa_sF"
      },
      "source": [
        "### Import the dependent libraries -- numpy and matplotlib:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPY_MlEFbHlq"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNIxbmBXbPY8"
      },
      "source": [
        "### Create [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function):\n",
        "\n",
        "- The sigmoid function takes two input arguments: x and a boolean argument called 'derivative'.\n",
        "- When the boolean argument is set as true, the sigmoid function calculates the derivative of x.\n",
        "- The derivative of x is required when calculating error or performing back-propagation.\n",
        "- The sigmoid function runs in every single neuron.\n",
        "- The sigmoid funtion feeds forward the data by converting the numeric matrices to probablities.\n",
        "\n",
        "To implement the [logistic sigmoid function using numpy](https://stackoverflow.com/questions/3985619/how-to-calculate-a-logistic-sigmoid-function-in-python), we use the mathematical formula:\n",
        "\n",
        "[$$\n",
        "  F(x) = \\frac{1}{1+e^{-x}}\n",
        "$$](https://github.com/rahulremanan/python_tutorial/raw/master/Fundamentals_of_deep-learning/media/sigmoid_function_formula.png)\n",
        "\n",
        "### [Backpropagation](https://en.wikipedia.org/wiki/Backpropagation):\n",
        "\n",
        "- Method to make the network better.\n",
        "- [Mathematically we need to compute the derivative of the activation function](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/6dDj7/backpropagation-intuition-optional).\n",
        "\n",
        "#### If sigmoid function can be expressed as follows:\n",
        "\n",
        "![logistic sigmoid function](https://github.com/rahulremanan/python_tutorial/raw/master/Fundamentals_of_deep-learning/media/logistic_sigmoid_function.png)\n",
        "\n",
        "#### Then, the first [derivative](https://en.wikipedia.org/wiki/Derivative) of this function can be expressed as:\n",
        "\n",
        "![backpropagation](https://github.com/rahulremanan/python_tutorial/raw/master/Fundamentals_of_deep-learning/media/derivative_of_sigmoid.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpv0pAonU2_8"
      },
      "source": [
        "### Forwardpropagation and backpropagation functions using sigmoid activation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iM-3_NoSriE"
      },
      "source": [
        "### Implementing sigmoid function using math library in python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-I2UVtrSriI"
      },
      "source": [
        "import math\n",
        "x  = -1.2\n",
        "y = 1/(1+math.exp(-x))\n",
        "print (y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaaaT6BRSriS"
      },
      "source": [
        "import numpy as np\n",
        "y = 1/(1+np.exp(-x))\n",
        "print (y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NWiP4NAbOYY"
      },
      "source": [
        "def sigmoid(x, derivative=False):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "      x: input\n",
        "      derivative: boolean to specify if the derivative of the function should be computed\n",
        "    \"\"\"\n",
        "    if derivative:\n",
        "        return (x*(1-x))\n",
        "    return (1/(1+np.exp(-x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ReLU(x, h=None, derivative=False):\n",
        "  if derivative:\n",
        "    return x[h < 0]\n",
        "  x_relu = np.maximum(x, 0)\n",
        "  return x_relu"
      ],
      "metadata": {
        "id": "NIelyyTtZ_nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1TDFr6HlghF"
      },
      "source": [
        "sigmoid(-1.2, derivative=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLJoi0zRzI8B"
      },
      "source": [
        "x = -1.2\n",
        "y_d = (1/(1+np.exp(x)))*(1-(1/(1+np.exp(x))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bM9W7aYzcMZ"
      },
      "source": [
        "y_d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fTEF8Zhy-si"
      },
      "source": [
        "sigmoid(0.23147521650098238, derivative=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lg2-3qiw-QtZ"
      },
      "source": [
        "### Plotting sigmoid activation function:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj3nOOuz_JCB"
      },
      "source": [
        "xmin= -10\n",
        "xmax = 10\n",
        "ymin = -0.1\n",
        "ymax = 1.1\n",
        "step_size = 0.01\n",
        "\n",
        "x = list(np.arange(xmin, xmax, step_size))\n",
        "y = []\n",
        "for i in x:\n",
        "  y_i = sigmoid(i)\n",
        "  y.append(y_i)\n",
        "\n",
        "\n",
        "axis = [xmin, xmax, ymin, ymax]\n",
        "plt.axhline(y=0.5, color='C2', alpha=0.5)\n",
        "plt.axvline(x=0, color='C2', alpha=0.5)\n",
        "plt.axis(axis)\n",
        "plt.plot(x, y, linewidth=2.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p1UNLTLfMpQ"
      },
      "source": [
        "### Create an input data matrix as numpy array:\n",
        "- Matrix with n number of dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaIBbCr8_GeK"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the input matrix\n",
        "\\begin{bmatrix}\n",
        "0 & 0 \\\\\n",
        "1 & 1 \\\\\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{bmatrix}"
      ],
      "metadata": {
        "id": "WoOXgHDUEBPg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7BWTaxmfXP8"
      },
      "source": [
        "x = np.asarray([[0,0],\n",
        "                [1,1],\n",
        "                [1,0],\n",
        "                [0,1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eunaVEqbmZIM"
      },
      "source": [
        "print (x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LKpZI-4yBJQ"
      },
      "source": [
        "x.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0GSAgi0yEQQ"
      },
      "source": [
        "x.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXnkWkp3Srj8"
      },
      "source": [
        "x_ = (1 , 2, 3, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8QIvrcASrkG"
      },
      "source": [
        "len(x_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdOmH9j2SrkY"
      },
      "source": [
        "for i in range(len(x_)):\n",
        "    print (\"This is the {} element in the tuple\".format(i))\n",
        "    print (\"The value is: {}\".format(x_[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1mGhSApf-Ze"
      },
      "source": [
        "## Define the output data matrix as numpy array:\n",
        "\n",
        "\\begin{bmatrix}\n",
        "0 \\\\\n",
        "0 \\\\\n",
        "1 \\\\\n",
        "1\n",
        "\\end{bmatrix}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6H3nAw5f-2n"
      },
      "source": [
        "y = np.asarray([[0],\n",
        "                [0],\n",
        "                [1],\n",
        "                [1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqe0NKcTSrlG"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x[:,0], x[:,1], s=180, c=y, cmap=plt.colormaps.get_cmap(\"Spectral\"))\n",
        "for i, (i_x, i_y) in enumerate(x):\n",
        "    i_z = y[i]\n",
        "    p_x, p_y = i_x, i_y\n",
        "    if i_x == 1:\n",
        "      p_x = i_x - 0.35\n",
        "    plt.text(p_x + 0.02, p_y - 0.02, f'({i_x}, {i_y} ► {i_z})', fontsize=18)"
      ],
      "metadata": {
        "id": "IqevNNSS9f8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the neural network:\n",
        "$$\n",
        "f_n(\\begin{bmatrix}\n",
        "0 & 0 \\\\\n",
        "1 & 1 \\\\\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{bmatrix})\n",
        "\\rightarrow\n",
        "\\begin{bmatrix}\n",
        "0 \\\\\n",
        "0 \\\\\n",
        "1 \\\\\n",
        "1\n",
        "\\end{bmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "aMdw0xyZjlL8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYxrcX3uikpq"
      },
      "source": [
        "### Create a random number seed:\n",
        "\n",
        "- Random number seeding is useful for producing reproducible results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Atv5s0UaikH_"
      },
      "source": [
        "seed = 1\n",
        "np.random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz5E6sOnjAFy"
      },
      "source": [
        "### Create a synapse matrix:\n",
        "\n",
        "- A function applied to the syanpses.\n",
        "- For the first synapse, weights matrix of shape: input_shape_1 x input_shape_2 is created.\n",
        "- For the second synapse, weights matrix of shape: input_shape_2 x output_dim is created.\n",
        "-  This function also introduces the first hyper-parameter in neural network tuning called 'bias_val', which is the bias value for the synaptic function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D5YYvZ_jAlU"
      },
      "source": [
        "bias_val = 1\n",
        "\n",
        "output_dim = 1\n",
        "\n",
        "input_shape_1 = x.shape[1]\n",
        "input_shape_2 = x.shape[0]\n",
        "\n",
        "hidden_layer_size = 4 # 3 # 2 # 5 #\n",
        "\n",
        "synapse_0 = 2*np.random.random((input_shape_1, hidden_layer_size)) - bias_val\n",
        "synapse_1 = 2*np.random.random((hidden_layer_size, output_dim)) - bias_val\n",
        "\n",
        "loss_col = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBmLFJK2Srls"
      },
      "source": [
        "print (synapse_0.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the first synapse:\n",
        "\n",
        ">\n",
        "$$\n",
        "Eg.\\\\\n",
        "$$\n",
        "$$\n",
        "Synapse_0\n",
        "→\n",
        "\\begin{bmatrix}\n",
        "-0.16595599 & 0.44064899 & -0.99977125 & -0.39533485\\\\\n",
        "-0.70648822 & -0.81532281 & -0.62747958 & -0.30887855\n",
        "\\end{bmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "NpZtw1d1GqUY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrapQeblRdmv"
      },
      "source": [
        "synapse_0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OMAVFgKSrl5"
      },
      "source": [
        "print (synapse_1.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the second synapse:\n",
        "\n",
        ">\n",
        "$$\n",
        "Eg.\\\\\n",
        "$$\n",
        "$$\n",
        "Synapse_1\n",
        "→\n",
        "\\begin{bmatrix}\n",
        "-0.20646505\\\\\n",
        "0.07763347\\\\\n",
        "-0.16161097\\\\\n",
        "0.370439\n",
        "\\end{bmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "p_8Ikosj16kI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (synapse_1)"
      ],
      "metadata": {
        "id": "SJA-rbkTNJ5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InWiWkFKR4_q"
      },
      "source": [
        "## Implement a single forward pass of the XOR input table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l23aH9iXR_M7"
      },
      "source": [
        "### Create the input layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IViHlbwRnfz"
      },
      "source": [
        "layer_0 = x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute the dot product between layer 0 and synapse 0:\n",
        "$$\n",
        "Eg.\\\\\n",
        "\\begin{bmatrix}\n",
        "0 & 0 \\\\\n",
        "1 & 1 \\\\\n",
        "1 & 0 \\\\\n",
        "0 & 1\n",
        "\\end{bmatrix}\n",
        "̇\n",
        "\\begin{bmatrix}\n",
        "-0.16595599 & 0.44064899 & -0.99977125 & -0.39533485\\\\\n",
        "-0.70648822 & -0.81532281 & -0.62747958 & -0.30887855\n",
        "\\end{bmatrix}\n",
        "=\\\\\n",
        "\\begin{bmatrix}\n",
        "0 & 0 & 0 & 0 \\\\\n",
        "-0.87244421 & -0.37467382 & -1.62725083 & -0.7042134 \\\\\n",
        "-0.16595599  & 0.44064899 & -0.99977125 & -0.39533485 \\\\\n",
        "-0.70648822 & -0.81532281 & -0.62747958 & -0.30887855 \\\\\n",
        "\\end{bmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "UFAyl3q320Fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dot_product_0 = np.dot(layer_0, synapse_0)"
      ],
      "metadata": {
        "id": "qyM3edZY27mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dot_product_0)"
      ],
      "metadata": {
        "id": "FBu56nZ34wIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply bias value for dot product 0:"
      ],
      "metadata": {
        "id": "lR6Tm83K6Bi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bias_val_1 = 1\n",
        "dot_product_0 = dot_product_0 - bias_val_1"
      ],
      "metadata": {
        "id": "9FjvtD0X6FGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dot_product_0)"
      ],
      "metadata": {
        "id": "s2kH__hB6T6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a9qiOg8SDxx"
      },
      "source": [
        "### Apply the activation function for dot product 0:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2oPrwRUSCvZ"
      },
      "source": [
        "layer_1 = sigmoid(dot_product_0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veCcWQqsSWTw"
      },
      "source": [
        "layer_1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ryAjZ_5ScMe"
      },
      "source": [
        "print(layer_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq0WpB66W53S"
      },
      "source": [
        "### Create an activation function for layer 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNBC3fusSnDW"
      },
      "source": [
        "layer_2 = sigmoid(np.dot(layer_1, synapse_1) - bias_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJSj2_RvS28u"
      },
      "source": [
        "layer_2.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WcU1gKAS64s"
      },
      "source": [
        "layer_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfYQKDPw1tDZ"
      },
      "source": [
        "# Part 01b -- [Backpropagation](https://en.wikipedia.org/wiki/Backpropagation)\n",
        "\n",
        "This backpropagation example implments the logistic function:\n",
        "$ \\frac{\\partial E}{\\partial o_j} = \\frac{\\partial E}{\\partial o_j} = \\frac{\\partial }{\\partial o_j} {\\frac{1}{2}}(t-y)^2 {= y-t}$\n",
        "and computes layer delta using:\n",
        "$\\Delta w_{ij} = -\\eta \\frac{\\partial E}{\\partial w_{ij}} $\n",
        "\n",
        "In this implementation, learning rate ($\\eta$) = 1\n",
        "\n",
        "\n",
        "Read more by following the backpropogation link above.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6ZU-RCITHOW"
      },
      "source": [
        "## Implement a signle backprop pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-q92fJfTFx3"
      },
      "source": [
        "outputError = (layer_2 - y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUO7X2WOTZe9"
      },
      "source": [
        "outputError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ngqzGUqTeA8"
      },
      "source": [
        "layer_2_delta = (outputError*sigmoid(layer_2, derivative=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_URoSNTTxce"
      },
      "source": [
        "layer_2_delta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXOy2m09T-xM"
      },
      "source": [
        "layer_1_error = (layer_2_delta.dot(synapse_1.T))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPxtHndIUbEb"
      },
      "source": [
        "layer_1_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAWdKLacUhMK"
      },
      "source": [
        "layer_1_delta=layer_1_error*sigmoid(layer_1,derivative=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d308EMWHU2XB"
      },
      "source": [
        "layer_1_delta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SVDHJoYWEN6"
      },
      "source": [
        "### Updating the weights/synapses of the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHU45BMPVkcQ"
      },
      "source": [
        "synapse_1 += layer_1.T.dot(layer_2_delta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5Dfujkm4OgA"
      },
      "source": [
        "synapse_1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTXFUcBcWKZ6"
      },
      "source": [
        "synapse_0 += layer_0.T.dot(layer_1_delta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQCZOLNgWZ7C"
      },
      "source": [
        "synapse_0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgyoLCqPlIV5"
      },
      "source": [
        "### Training the simple XOR gate neural network:\n",
        "\n",
        "- Note: There is no function that defines a neuron! In practice neuron is just an abstract concept to understand the probability function.\n",
        "- Continuously feeding the data throught the neural network.\n",
        "- Updating the weights of the network through backpropagation.\n",
        "- During the training the model becomes better and better in predicting the output values.\n",
        "- The layers are just matrix multiplication functions that apply the sigmoid function to the synapse matrix and the corresponding layer.\n",
        "- Backpropagation portion of the training is the machine learning portion of this code.\n",
        "- Backpropagation function reduces the prediction errors during each training step.\n",
        "- Synapses and weights are synonymous."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfNAijZ_lI4S"
      },
      "source": [
        "training_steps = 500000\n",
        "update_freq = 10\n",
        "\n",
        "input_data = x\n",
        "output_data = y\n",
        "\n",
        "bias_val_1 = 1e-2\n",
        "bias_val_2 = 10\n",
        "\n",
        "learning_rate = 0.1 # 10 # 1e-3 #\n",
        "\n",
        "for t in range(training_steps):\n",
        "  # Creating the layers of the neural network:\n",
        "  layer_0 = input_data\n",
        "  layer_1 = sigmoid(np.dot(layer_0, synapse_0)+bias_val_1)\n",
        "  layer_2 = sigmoid(np.dot(layer_1, synapse_1)+bias_val_2)\n",
        "\n",
        "\n",
        "  # Backpropagation:\n",
        "  outputLoss_derivative = output_data - layer_2\n",
        "  loss_col.append(np.mean(np.abs(outputLoss_derivative)))\n",
        "  if ((t*update_freq) % training_steps == 0):\n",
        "    print ('Training step :' + str(t))\n",
        "    print ('Prediction error during training :' + str(np.mean(np.abs(outputLoss_derivative))))\n",
        "\n",
        "  # Layer-wsie delta function:\n",
        "  layer_2_delta = (learning_rate*outputLoss_derivative*sigmoid(layer_2, derivative = True))\n",
        "  layer_1_error = layer_2_delta.dot(synapse_1.T) # Matrix multiplication of the layer 2 delta with the transpose of the first synapse function.\n",
        "  layer_1_delta = (layer_1_error*learning_rate)*(sigmoid(layer_1, derivative = True))\n",
        "\n",
        "  # Updating synapses or weights:\n",
        "  synapse_1 += layer_1.T.dot(layer_2_delta)\n",
        "  synapse_0 += layer_0.T.dot(layer_1_delta)\n",
        "  del layer_0, layer_1\n",
        "\n",
        "print ('Training completed ...')\n",
        "print ('Predictions :' + str (layer_2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_0 = input_data\n",
        "layer_1 = sigmoid(np.dot(layer_0, synapse_0) + bias_val_1)\n",
        "layer_2 = sigmoid(np.dot(layer_1, synapse_1) + bias_val_2)\n",
        "print(synapse_0)\n",
        "print('\\n')\n",
        "print(bias_val_1)\n",
        "print('\\n')\n",
        "print(layer_1)\n",
        "print('\\n')\n",
        "print(bias_val_2)\n",
        "print('\\n')\n",
        "print(synapse_1)\n",
        "print('\\n')\n",
        "print(layer_2)"
      ],
      "metadata": {
        "id": "8hO1j95QAXKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def xor_gate_predictor(x):\n",
        "  layer_0 = x\n",
        "  layer_1 = sigmoid(np.dot(layer_0, synapse_0) + bias_val_1)\n",
        "  layer_2 = sigmoid(np.dot(layer_1, synapse_1) + bias_val_2)\n",
        "  return layer_2"
      ],
      "metadata": {
        "id": "bN2AdzqVDx-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "id": "iddIAcTkEIPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_xor_gate_decision_boundary(prediction_function, x, y):\n",
        "  # Setting minimum and maximum values for giving the plot function some padding\n",
        "  x_min, x_max = x[:, 0].min() - .5, \\\n",
        "                 x[:, 0].max() + .5\n",
        "  y_min, y_max = x[:, 1].min() - .5, \\\n",
        "                 x[:, 1].max() + .5\n",
        "  h = 0.01\n",
        "  # Generate a grid of points with distance h between them\n",
        "  xx, yy = np.meshgrid(np.arange(x_min, x_max, h), \\\n",
        "                       np.arange(y_min, y_max, h))\n",
        "  # Predict the function value for the whole grid\n",
        "  Z = prediction_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "  Z = Z.reshape(xx.shape)\n",
        "  # Plotting the contour and training examples\n",
        "  plt.contourf(xx, yy, Z, cmap=plt.colormaps.get_cmap(\"Spectral\"))\n",
        "  plt.scatter(x[:, 0], x[:, 1], c=y, s=42, cmap=plt.colormaps.get_cmap(\"Accent\"))\n",
        "  for i, (i_x, i_y) in enumerate(x):\n",
        "      i_z = y[i]\n",
        "      p_x, p_y = i_x - 0.25, i_y - 0.2\n",
        "      #if i_x == 1:\n",
        "      #    p_x = i_x - 0.25\n",
        "      plt.text(p_x , p_y , f'({i_x}, {i_y} ► {i_z})', fontsize=18)"
      ],
      "metadata": {
        "id": "hDvfa1v3EfOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn, sklearn.datasets, sklearn.linear_model\n",
        "X, Y = sklearn.datasets.make_moons(6, noise=0.20)"
      ],
      "metadata": {
        "id": "Ye8asW9fPnYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear_classifier = sklearn.linear_model.LogisticRegression()\n",
        "linear_classifier.fit(X, Y)"
      ],
      "metadata": {
        "id": "LVydJ6tpO0ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_xor_gate_decision_boundary(lambda x: linear_classifier.predict(x), x, y)\n",
        "plt.title(\"Logistic Regression\")"
      ],
      "metadata": {
        "id": "BPpk9GkJOIOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_xor_gate_decision_boundary(lambda x: xor_gate_predictor(x), x, y)\n",
        "plt.title(\"XOR gate neural network model\")"
      ],
      "metadata": {
        "id": "N2C3Zmc6Db_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcL_Dd-Gz8zJ"
      },
      "source": [
        "plt.plot(loss_col)\n",
        "plt.show()\n",
        "\n",
        "delete_model = True\n",
        "\n",
        "if delete_model:\n",
        "  try:\n",
        "    del loss_col\n",
        "  except:\n",
        "    pass\n",
        "  try:\n",
        "    del input_data\n",
        "  except:\n",
        "    pass\n",
        "  try:\n",
        "    del output_data\n",
        "  except:\n",
        "    pass\n",
        "  try:\n",
        "    del x\n",
        "  except:\n",
        "    pass\n",
        "  try:\n",
        "    del y\n",
        "  except:\n",
        "    pass\n",
        "  try:\n",
        "    del layer_2\n",
        "  except:\n",
        "    pass\n",
        "  try:\n",
        "    del output_data\n",
        "  except:\n",
        "    pass\n",
        "  try:\n",
        "    del synapse_0\n",
        "  except:\n",
        "    pass\n",
        "  try:\n",
        "    del synapse_1\n",
        "  except:\n",
        "    pass\n",
        "  import gc\n",
        "  gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3m8gTk_w6gg"
      },
      "source": [
        "## Part 01b -- Neural network based XOR gate using rectified linear units activation function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEAurIYUxvIr"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNPZt9nTIYA-"
      },
      "source": [
        "### Plotting the rectified linear units (ReLU) activation function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1HSjuTRIWuC"
      },
      "source": [
        "def ReLU(x, h = None, derivative=False):\n",
        "  if derivative:\n",
        "    return x[h < 0]\n",
        "  x_relu = np.maximum(x, 0)\n",
        "  return x_relu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjFMf2UQJlcC"
      },
      "source": [
        "x = list(np.arange(-6.0, 6.0, 0.1))\n",
        "y = []\n",
        "for i in x:\n",
        "  y_i = ReLU(i)\n",
        "  y.append(y_i)\n",
        "\n",
        "xmin= -6\n",
        "xmax = 6\n",
        "ymin = 0\n",
        "ymax = 1\n",
        "axis = [xmin, xmax, ymin, ymax]\n",
        "plt.axhline(y=0.5, color='C2', alpha=0.5)\n",
        "plt.axvline(x=0, color='C2', alpha=0.5)\n",
        "plt.axis(axis)\n",
        "plt.plot(x, y, linewidth=2.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNYTeupHcvGc"
      },
      "source": [
        "### Create input and output data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZdJ2UfPx5I2"
      },
      "source": [
        "x = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "              [1, 0],\n",
        "              [1, 1]])\n",
        "\n",
        "y = np.array([[0],\n",
        "              [1],\n",
        "              [1],\n",
        "              [0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcIskqgVaye1"
      },
      "source": [
        "### N is batch size(sample size); D_in is input dimension; H is hidden dimension; D_out is output dimension:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogzqBRKdx0SQ"
      },
      "source": [
        "D_in, H, D_out = x.shape[1], 30, 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZFRf6T1alYi"
      },
      "source": [
        "### Randomly initialize weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67qTKA1ox8uJ"
      },
      "source": [
        "w1 = np.random.randn(D_in, H)\n",
        "w2 = np.random.randn(H, D_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndP9RYonx_fd"
      },
      "source": [
        "learning_rate = 1e-3\n",
        "update_freq = 10\n",
        "training_steps = 2000\n",
        "\n",
        "loss_col = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsMtM098yUl7"
      },
      "source": [
        "### ReLu as the activation function and [squared error](https://datascience.stackexchange.com/questions/10188/why-do-cost-functions-use-the-square-error) as the loss function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DH5MZfEKw1OL"
      },
      "source": [
        "for t in range(training_steps):\n",
        "    # Forward pass: compute predicted y\n",
        "    h = x.dot(w1)\n",
        "    h_relu = np.maximum(h, 0)  # using ReLU as activation function\n",
        "    y_pred = h_relu.dot(w2)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = np.square(y_pred - y).sum() # squared error as the loss function\n",
        "    loss_col.append(loss)\n",
        "    if ((t*update_freq) % training_steps ==0):\n",
        "      print ('Training step :' + str(t))\n",
        "      print ('Loss function during training :' + str(loss))\n",
        "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
        "    grad_y_pred = 2*(y_pred - y) # the last layer's error\n",
        "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
        "    grad_h_relu = grad_y_pred.dot(w2.T) # the second layer's error\n",
        "    grad_h = grad_h_relu.copy()\n",
        "    grad_h[h < 0] = 0  # the derivate of ReLU\n",
        "    grad_w1 = x.T.dot(grad_h)\n",
        "\n",
        "    # Update weights\n",
        "    w1 -= learning_rate * grad_w1\n",
        "    w2 -= learning_rate * grad_w2\n",
        "\n",
        "print ('Training completed ...')\n",
        "print ('Predictions :' + str (y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnkaksqMz4SU"
      },
      "source": [
        "plt.plot(loss_col)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLRaVghZqmke"
      },
      "source": [
        "delete_model = True\n",
        "\n",
        "if delete_model:\n",
        "  try:\n",
        "    del loss_col\n",
        "  except:\n",
        "    pass\n",
        "  try:\n",
        "    del input_data\n",
        "  except:\n",
        "    pass\n",
        "  try:\n",
        "    del output_data\n",
        "  except:\n",
        "    pass\n",
        "  try:\n",
        "    del x\n",
        "  except:\n",
        "    pass\n",
        "  try:\n",
        "    del y\n",
        "  except:\n",
        "    pass\n",
        "  try:\n",
        "    del output_data\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  import gc\n",
        "  gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1NHH75GvL58"
      },
      "source": [
        "## Part 02 -- [Build a more complex neural network classifier using numpy](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4ZwQp-pyq_M"
      },
      "source": [
        "### Importing dependent libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rynRpfY5qjaX"
      },
      "source": [
        "import matplotlib.pyplot as plt # pip3 install matplotlib\n",
        "import numpy as np # pip3 install numpy\n",
        "import sklearn # pip3 install scikit-learn\n",
        "import sklearn.datasets\n",
        "import sklearn.linear_model\n",
        "import matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aro3itM7LyBV"
      },
      "source": [
        "### Plotting hyperbolic tan (tanh) activation function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJuQ7oPELxRF"
      },
      "source": [
        "def tanh(x, derivative=False):\n",
        "    if (derivative == True):\n",
        "        return (1 - (x ** 2))\n",
        "    return np.tanh(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw_JWRr4L7VH"
      },
      "source": [
        "x = list(np.arange(-6.0, 6.0, 0.1))\n",
        "y = []\n",
        "for i in x:\n",
        "  y_i = tanh(i)\n",
        "  y.append(y_i)\n",
        "\n",
        "xmin=-6\n",
        "xmax = 6\n",
        "ymin = -1.1\n",
        "ymax = 1.1\n",
        "axis = [xmin, xmax, ymin, ymax]\n",
        "plt.axhline(y=0, color='C2', alpha=0.5)\n",
        "plt.axvline(x=0, color='C2', alpha=0.5)\n",
        "plt.axis(axis)\n",
        "plt.plot(x, y, linewidth=2.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvrXHvqr0Gge"
      },
      "source": [
        "### Display plots inline and change default figure size:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCunSkU5zdlc"
      },
      "source": [
        "%matplotlib inline\n",
        "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdKwQYtJ0aFM"
      },
      "source": [
        "### Generate a dataset and create a plot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8lZ9n910Slo"
      },
      "source": [
        "np.random.seed(0)\n",
        "X, y = sklearn.datasets.make_moons(200, noise=0.20)\n",
        "plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuc3DSeG4dcI"
      },
      "source": [
        "### Train the logistic regression classifier:\n",
        "\n",
        "The classification problem can be summarized as creating a boundary between the red and the blue dots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ez4hJ_9S33j6"
      },
      "source": [
        "linear_classifier = sklearn.linear_model.LogisticRegressionCV()\n",
        "linear_classifier.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_gMrYdu5Ikg"
      },
      "source": [
        "### Visualize the logistic regression classifier output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUPuKKEB5Acv"
      },
      "source": [
        "def plot_decision_boundary(prediction_function):\n",
        "  # Setting minimum and maximum values for giving the plot function some padding\n",
        "  x_min, x_max = X[:, 0].min() - .5, \\\n",
        "                 X[:, 0].max() + .5\n",
        "  y_min, y_max = X[:, 1].min() - .5, \\\n",
        "                 X[:, 1].max() + .5\n",
        "  h = 0.01\n",
        "  # Generate a grid of points with distance h between them\n",
        "  xx, yy = np.meshgrid(np.arange(x_min, x_max, h), \\\n",
        "                       np.arange(y_min, y_max, h))\n",
        "  # Predict the function value for the whole grid\n",
        "  Z = prediction_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "  Z = Z.reshape(xx.shape)\n",
        "  # Plotting the contour and training examples\n",
        "  plt.contourf(xx, yy, Z, cmap=plt.colormaps.get_cmap(\"Spectral\"))\n",
        "  plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.colormaps.get_cmap(\"Spectral\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgC6Vm5072hj"
      },
      "source": [
        "### Plotting the decision boundary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH_l5suh7w1f"
      },
      "source": [
        "plot_decision_boundary(lambda x: linear_classifier.predict(x))\n",
        "plt.title(\"Logistic Regression\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzS_LhQYCXnW"
      },
      "source": [
        "### Create a neural network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWz8z03d8LPg"
      },
      "source": [
        "num_examples = len(X) # training set size\n",
        "nn_input_dim = 2 # input layer dimensionality\n",
        "nn_output_dim = 2 # output layer dimensionality"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVpwJSH7Cub9"
      },
      "source": [
        "### Gradient descent parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s7Ap7_FCsub"
      },
      "source": [
        "epsilon = 0.01 # learning rate for gradient descent\n",
        "reg_lambda = 0.01 # regularization strength"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktuTAoe_C8xd"
      },
      "source": [
        "### Compute loss function on the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3PNCNomC65a"
      },
      "source": [
        "def loss_function(model):\n",
        "  W1, b1, W2, b2 = model['W1'], \\\n",
        "                   model['b1'], \\\n",
        "                   model['W2'], \\\n",
        "                   model['b2']\n",
        "  z1 = X.dot(W1) + b1\n",
        "  a1 = np.tanh(z1)\n",
        "  z2 = a1.dot(W2) + b2\n",
        "  exp_scores = np.exp(z2)\n",
        "  probabilities = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "  # Calculating the loss function:\n",
        "  corect_logprobs = -np.log(probabilities[range(num_examples), y])\n",
        "  data_loss = np.sum(corect_logprobs)\n",
        "  # Adding the regulatization term to the loss function\n",
        "  data_loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
        "  return 1./num_examples * data_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u36QZJmtGZhu"
      },
      "source": [
        "### Function that predicts the output of either 0 or 1:\n",
        "\n",
        "Calculating predictions using forward propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOKPt6UIC5lm"
      },
      "source": [
        "def predict(model, x):\n",
        "    W1, b1, W2, b2 = model['W1'], \\\n",
        "                     model['b1'], \\\n",
        "                     model['W2'], \\\n",
        "                     model['b2']\n",
        "    # Design a network with forward propagation\n",
        "    z1 = x.dot(W1) + b1\n",
        "    a1 = np.tanh(z1)\n",
        "    z2 = a1.dot(W2) + b2\n",
        "    exp_scores = np.exp(z2)\n",
        "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "    return np.argmax(probs, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pzWRW7PHDRl"
      },
      "source": [
        "\n",
        "### This function learns parameters for the neural network and returns the model:\n",
        "- nn_hdim: Number of nodes in the hidden layer\n",
        "- num_passes: Number of passes through the training data for gradient descent\n",
        "- print_loss: If True, print the loss every 1000 iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjZsvXVMG86T"
      },
      "source": [
        "def build_model(nn_hdim, num_passes=20000, print_loss=False):\n",
        "\n",
        "    # Initialize the parameters to random values. We need to learn these.\n",
        "    np.random.seed(0)\n",
        "    W1 = np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)\n",
        "    b1 = np.zeros((1, nn_hdim))\n",
        "    W2 = np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim)\n",
        "    b2 = np.zeros((1, nn_output_dim))\n",
        "\n",
        "    # This is what we return at the end\n",
        "    model = {}\n",
        "\n",
        "    # Gradient descent. For each batch...\n",
        "    for i in range(0, num_passes):\n",
        "\n",
        "        # Forward propagation\n",
        "        z1 = X.dot(W1) + b1\n",
        "        a1 = np.tanh(z1)\n",
        "        z2 = a1.dot(W2) + b2\n",
        "        exp_scores = np.exp(z2)\n",
        "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "\n",
        "        # Backpropagation\n",
        "        delta3 = probs\n",
        "        delta3[range(num_examples), y] -= 1\n",
        "        dW2 = (a1.T).dot(delta3)\n",
        "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
        "        delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n",
        "        dW1 = np.dot(X.T, delta2)\n",
        "        db1 = np.sum(delta2, axis=0)\n",
        "\n",
        "        # Add regularization terms (b1 and b2 don't have regularization terms)\n",
        "        dW2 += reg_lambda * W2\n",
        "        dW1 += reg_lambda * W1\n",
        "\n",
        "        # Gradient descent parameter update\n",
        "        W1 += -epsilon * dW1\n",
        "        b1 += -epsilon * db1\n",
        "        W2 += -epsilon * dW2\n",
        "        b2 += -epsilon * db2\n",
        "\n",
        "        # Assign new parameters to the model\n",
        "        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
        "\n",
        "        # Optionally print the loss.\n",
        "        # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n",
        "        if print_loss and i % 1000 == 0:\n",
        "          print(\"Loss after iteration %i: %f\" %(i, loss_function(model)))\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qsFUIRzHwFF"
      },
      "source": [
        "### Build a model with 50-dimensional hidden layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsH3JKtVHni_"
      },
      "source": [
        "model = build_model(50, print_loss=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faX_dAZlPFLb"
      },
      "source": [
        "### Plot the decision boundary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0psfV-xdPAqi"
      },
      "source": [
        "plot_decision_boundary(lambda x: predict(model, x))\n",
        "plt.title(\"Decision Boundary for hidden layer size  50\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74-myoOdIQnE"
      },
      "source": [
        "### Visualizing the hidden layers with varying sizes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-cqJVZTHytt"
      },
      "source": [
        "plt.figure(figsize=(16, 32))\n",
        "hidden_layer_dimensions = [1, 2, 3, 4, 5, 20, 50]\n",
        "for i, nn_hdim in enumerate(hidden_layer_dimensions):\n",
        "    plt.subplot(5, 2, i+1)\n",
        "    plt.title('Hidden Layer size %d' % nn_hdim)\n",
        "    model = build_model(nn_hdim)\n",
        "    plot_decision_boundary(lambda x: predict(model, x))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsC5K-7ugEKx"
      },
      "source": [
        "## Part 03 -- Example illustrating the importance of[ learning rate](http://users.ics.aalto.fi/jhollmen/dippa/node22.html) in hyper-parameter tuning:\n",
        "\n",
        "- Learning rate is a decreasing function of time.\n",
        "- Two forms that are commonly used are:\n",
        "    * 1) a linear function of time\n",
        "    * 2) a function that is inversely proportional to the time t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLCy6P16h8sa"
      },
      "source": [
        "### Create a noisier, more complex dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBV41LHWIebT"
      },
      "source": [
        "np.random.seed(0)\n",
        "X, y = sklearn.datasets.make_moons(20000, noise=0.5)\n",
        "plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEn2RWkgiGOp"
      },
      "source": [
        "linear_classifier = sklearn.linear_model.LogisticRegressionCV()\n",
        "linear_classifier.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3siSKYtLiPhg"
      },
      "source": [
        "plot_decision_boundary(lambda x: linear_classifier.predict(x))\n",
        "plt.title(\"Logistic Regression\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itPMnFcLjhqb"
      },
      "source": [
        "num_examples = len(X) # training set size\n",
        "nn_input_dim = 2 # input layer dimensionality\n",
        "nn_output_dim = 2 # output layer dimensionality"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kG4EwVOjnTy"
      },
      "source": [
        "epsilon = 0.01 # learning rate for gradient descent\n",
        "reg_lambda = 0.01 # regularization strength"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb3E1Z-2iUOU"
      },
      "source": [
        "model = build_model(5, print_loss=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqDy-MxBhPv_"
      },
      "source": [
        "### Plotting output of the model that failed to learn, given a set of hyper-parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI6ZFYrmg66w"
      },
      "source": [
        "plot_decision_boundary(lambda x: predict(model, x))\n",
        "plt.title(\"Decision Boundary for hidden layer size  50\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xakIXy_qhZzc"
      },
      "source": [
        "### Adjusting the learning rate such that the neural network re-starts learning:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdxhhV16i4R_"
      },
      "source": [
        "epsilon = 1e-6 # learning rate for gradient descent\n",
        "reg_lambda = 0.01 # regularization strength"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEvPH7E8l-lI"
      },
      "source": [
        "model = build_model(5, print_loss=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r98-w9LWhjj0"
      },
      "source": [
        "### Plotting the decision boundary layer generated by an improved neural network model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfkTEZaFnlwY"
      },
      "source": [
        "plot_decision_boundary(lambda x: predict(model, x))\n",
        "plt.title(\"Decision Boundary for hidden layer size  50\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UYDOuTwScqU"
      },
      "source": [
        "## Part 04 -- Building a neural network using [tensorflow](https://www.tensorflow.org/):\n",
        "\n",
        "- A neural network that predicts the y value given an x value.\n",
        "- Implemented using tensorflow, an open-source deep-learning library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zHIZav4TOyI"
      },
      "source": [
        "### Import dependent libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pejP8ZqSjCa"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGmGnCPRTSap"
      },
      "source": [
        "### Create a synthetic dataset for training and generating predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI1b0M0UTMe8"
      },
      "source": [
        "x_data = np.float32(np.random.rand(2,500))\n",
        "y_data = np.dot([0.5, 0.7], x_data) + 0.6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyP8XW34UCUN"
      },
      "source": [
        "- Variable objects store tensors in tensorflow.\n",
        "- Tensorflow considers all input data tensors.\n",
        "- Tensors are 3 dimensional matrices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY2GHcRVVX24"
      },
      "source": [
        "### Constructing a linear model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaTUubcYT32h"
      },
      "source": [
        "bias = tf.Variable(tf.zeros([1]))\n",
        "synapses = tf.Variable(tf.random.uniform([1, 2], -1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySh6emwOVHT7"
      },
      "source": [
        "@tf.function\n",
        "def cost():\n",
        "  y = tf.matmul(synapses, x_data) + bias\n",
        "  return tf.reduce_mean(tf.square(y - y_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyRSzl0YVgsV"
      },
      "source": [
        "### Gradient descent optimizer:\n",
        "\n",
        "- Imagine the valley with a ball.\n",
        "- The goal of the optimizer is to localize the ball to the lowest point in the valley.\n",
        "- Loss function will be reduced over the training.\n",
        "- Mean squared error as the loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2nZ8XKTVxhk"
      },
      "source": [
        "lr = 0.01\n",
        "\n",
        "optimizer = tf.optimizers.RMSprop(lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDrNdKguXAyU"
      },
      "source": [
        "### Training function:\n",
        "\n",
        "- In tensorflow the computation is wrapped inside a graph.\n",
        "- Tensorflow makes it easier to visualize the training sessions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5a7Sdk6W_ka"
      },
      "source": [
        "train = optimizer.minimize(cost, var_list=[synapses, bias])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ocLrCdFX7wp"
      },
      "source": [
        "### Training the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RkVvUDIX9qt"
      },
      "source": [
        "training_steps = 60000\n",
        "\n",
        "for step in range (0, training_steps):\n",
        "  train\n",
        "  if step % 1000 == 0:\n",
        "    print ('Current training session: ' + str(step) + str(synapses.numpy())+ str(bias.numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dE8nkYHTCZu"
      },
      "source": [
        "## Part 05 -- Neural net XOR gate solver using [Tensorflow](https://tensorflow.org) and [Keras](https://keras.io)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-npA9O4TX-o"
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dA5Jh0RVa0B"
      },
      "source": [
        "MODEL_PATH = './XOR_gate_keras_network.h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRyV6uZSHbHN"
      },
      "source": [
        "! wget https://github.com/rahulremanan/python_tutorial/raw/master/Fundamentals_of_deep-learning/weights/XOR_gate_keras_network.h5 -O XOR_gate_keras_network.h5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z9yF8agToIN"
      },
      "source": [
        "### Create input and output data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea2MKtbZTBi9"
      },
      "source": [
        "x = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "              [1, 0],\n",
        "              [1, 1]])\n",
        "\n",
        "y = np.array([[0],\n",
        "              [1],\n",
        "              [1],\n",
        "              [0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMoZYatATvyy"
      },
      "source": [
        "### Create a neural network using [Keras Sequential API](https://keras.io/models/sequential/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3wvXQlDTue3"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(5, activation=\"relu\",\n",
        "                input_shape=(2,)))\n",
        "model.add(Dense(5, activation=\"relu\"))\n",
        "model.add(Dense(1, activation=\"relu\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_Nwa7xoUs5D"
      },
      "source": [
        "### Select optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn3IfLsgTsFX"
      },
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SweJ4tMKUwBB"
      },
      "source": [
        "### Compile keras model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLHkBVPsTToz"
      },
      "source": [
        "model.compile(optimizer=optimizer,\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmgU5N1rbsBz"
      },
      "source": [
        "### Load model weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6WIJX2Ybsle"
      },
      "source": [
        "if os.path.exists(MODEL_PATH):\n",
        "  model.load_weights(MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jk49XPGU0TW"
      },
      "source": [
        "### Summarize keras model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5THWrTZlTUL7"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IUSceIfVumD"
      },
      "source": [
        "### Visualize model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP8xdfygVz0v"
      },
      "source": [
        "! apt-get install -y graphviz libgraphviz-dev && pip3 install pydot graphviz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxWEHg5aV5Ur"
      },
      "source": [
        "from keras.utils import plot_model\n",
        "import pydot\n",
        "import graphviz # apt-get install -y graphviz libgraphviz-dev && pip3 install pydot graphviz\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJaJx2v5V-BM"
      },
      "source": [
        "output_dir = './'\n",
        "plot_model(model, to_file= output_dir + '/model_summary_plot.png')\n",
        "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaSe0XslU-xm"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIrjf6SyU99c"
      },
      "source": [
        "model.fit(x, y, batch_size=4,epochs=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shVEjaxVVRTn"
      },
      "source": [
        "### Save model weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kdGpbqJVUVb"
      },
      "source": [
        "model.save_weights(MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOH1-eQ0aFgT"
      },
      "source": [
        "model.predict(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5y3lrb9utuzQ"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXzw-gTgKq9k"
      },
      "source": [
        "## Extra credit --[ Activation functions in numpy](https://codereview.stackexchange.com/questions/132023/different-neural-network-activation-functions-and-gradient-descent):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bChn1I8SKvia"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x, derivative=False):\n",
        "    if (derivative == True):\n",
        "        return x * (1 - x)\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh(x, derivative=False):\n",
        "    if (derivative == True):\n",
        "        return (1 - (x ** 2))\n",
        "    return np.tanh(x)\n",
        "\n",
        "def relu(x, derivative=False):\n",
        "    if (derivative == True):\n",
        "        for i in range(0, len(x)):\n",
        "            for k in range(len(x[i])):\n",
        "                if x[i][k] > 0:\n",
        "                    x[i][k] = 1\n",
        "                else:\n",
        "                    x[i][k] = 0\n",
        "        return x\n",
        "    for i in range(0, len(x)):\n",
        "        for k in range(0, len(x[i])):\n",
        "            if x[i][k] > 0:\n",
        "                pass  # do nothing since it would be effectively replacing x with x\n",
        "            else:\n",
        "                x[i][k] = 0\n",
        "    return x\n",
        "\n",
        "def arctan(x, derivative=False):\n",
        "    if (derivative == True):\n",
        "        return (np.cos(x) ** 2)\n",
        "    return np.arctan(x)\n",
        "\n",
        "def step(x, derivative=False):\n",
        "    if (derivative == True):\n",
        "        for i in range(0, len(x)):\n",
        "            for k in range(len(x[i])):\n",
        "                if x[i][k] > 0:\n",
        "                    x[i][k] = 0\n",
        "        return x\n",
        "    for i in range(0, len(x)):\n",
        "        for k in range(0, len(x[i])):\n",
        "            if x[i][k] > 0:\n",
        "                x[i][k] = 1\n",
        "            else:\n",
        "                x[i][k] = 0\n",
        "    return x\n",
        "\n",
        "def squash(x, derivative=False):\n",
        "    if (derivative == True):\n",
        "        for i in range(0, len(x)):\n",
        "            for k in range(0, len(x[i])):\n",
        "                if x[i][k] > 0:\n",
        "                    x[i][k] = (x[i][k]) / (1 + x[i][k])\n",
        "                else:\n",
        "                    x[i][k] = (x[i][k]) / (1 - x[i][k])\n",
        "        return x\n",
        "    for i in range(0, len(x)):\n",
        "        for k in range(0, len(x[i])):\n",
        "            x[i][k] = (x[i][k]) / (1 + abs(x[i][k]))\n",
        "    return x\n",
        "\n",
        "def gaussian(x, derivative=False):\n",
        "    if (derivative == True):\n",
        "        for i in range(0, len(x)):\n",
        "            for k in range(0, len(x[i])):\n",
        "                x[i][k] = -2* x[i][k] * np.exp(-x[i][k] ** 2)\n",
        "    for i in range(0, len(x)):\n",
        "        for k in range(0, len(x[i])):\n",
        "            x[i][k] = np.exp(-x[i][k] ** 2)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEltQunn_eIO"
      },
      "source": [
        "## Concluding notes -- Frank Rosenblatt, connectionism and the Perceptron:\n",
        "\n",
        "![Frank Rosenblatt and the Perceptron](https://github.com/rahulremanan/python_tutorial/raw/master/Fundamentals_of_deep-learning/media/Frank_Rosenblatt's_Mark_I_Perceptron__Cornell_Aeronautical_Laboratory__Buffalo__New%20York.jpg)\n",
        "\n",
        "#### Image 1: Frank Rosenblatt working on his Mark 1 Perceptron at Cornell Aeronautical Laboratory in Buffalo, New York, circa 1960.\n",
        "\n",
        "This notebook is created to coincide the 90th birth anniversary of pioneering psychologist and artificial intelligence researcher, Frank Rosenblatt, born July 11, 1928 – died July 11, 1971. He is known for his work on connectionism, the incredible Mark 1 Perceptron. This notebook aims to remember the promise, the controversy and the resurgence of connectionism and neural networks as a tool in artificial intelligence.\n",
        "\n",
        "[Here is a brief biography of Frank Rosenblatt](https://en.wikipedia.org/wiki/Frank_Rosenblatt) (Via Wikipedia):\n",
        "\n",
        "Frank Rosenblatt was born in New Rochelle, New York as son of Dr. Frank and Katherine Rosenblatt. After graduating from The Bronx High School of Science in 1946, he attended Cornell University, where he obtained his A.B. in 1950 and his Ph.D. in 1956.\n",
        "\n",
        "He then went to Cornell Aeronautical Laboratory in Buffalo, New York, where he was successively a research psychologist, senior psychologist, and head of the cognitive systems section. This is also where he conducted the early work on perceptrons, which culminated in the development and hardware construction of the Mark I Perceptron in 1960. This was essentially the first computer that could learn new skills by trial and error, using a type of neural network that simulates human thought processes.\n",
        "\n",
        "Rosenblatt’s research interests were exceptionally broad. In 1959 he went to Cornell’s Ithaca campus as director of the Cognitive Systems Research Program and also as a lecturer in the Psychology Department. In 1966 he joined the Section of Neurobiology and Behavior within the newly formed Division of Biological Sciences, as associate professor. Also in 1966, he became fascinated with the transfer of learned behavior from trained to naive rats by the injection of brain extracts, a subject on which he would publish extensively in later years.\n",
        "\n",
        "In 1970 he became field representative for the Graduate Field of Neurobiology and Behavior, and in 1971 he shared the acting chairmanship of the Section of Neurobiology and Behavior. Frank Rosenblatt died in July 1971 on his 43rd birthday, in a boating accident in Chesapeake Bay.\n",
        "\n",
        "![Mark 1 Perceptron](https://github.com/rahulremanan/python_tutorial/raw/master/Fundamentals_of_deep-learning/media/Smithsonian_Perceptron.jpg)\n",
        "\n",
        "#### Image 2: Mark 1 Perceptron at Smithsonian Institute, Washington DC"
      ]
    }
  ]
}