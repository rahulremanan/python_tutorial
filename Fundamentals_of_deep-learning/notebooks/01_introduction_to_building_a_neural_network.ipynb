{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuuTEX1uxK-Y"
   },
   "source": [
    "# **Introduction to Neural Networks**:\n",
    "\n",
    "## Author: [Dr. Rahul Remanan](https://www.linkedin.com/in/rahulremanan)\n",
    "\n",
    "### CEO, [Moad Computer](https://www.moad.computer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeV1hQ30OFlu"
   },
   "source": [
    "This is a hands-on workshop notebook on deep-learning using python 3. In this notebook, we will learn how to implement a neural network from scratch using numpy. Once we have implemented this network, we will visualize the predictions generated by the neural network and compare it with a logistic regression model, in the form of classification boundaries. This workshop aims to provide an intuitive understanding of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggyt4xRWOOn0"
   },
   "source": [
    "In practical code development, there is seldom an use case for building a neural network from scratch. Neural networks in real-world are typically implemented using a deep-learning framework such as tensorflow. But, building a neural network with very minimal dependencies helps one gain an understanding of how neural networks work. This understanding is essential to designing effective neural network models. Also, towards the end of the session, we will use tensorflow deep-learning library to build a neural network, to illustrate the importance of building a neural network using a deep-learning framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVbiniy39JLk"
   },
   "source": [
    "### Architecture of the basic XOR gate neural network:\n",
    "\n",
    "![Artificial neural network architecture](https://github.com/rahulremanan/python_tutorial/raw/master/Fundamentals_of_deep-learning/media/Artificial_neural_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mRjefwXNXLV"
   },
   "source": [
    "### XOR gate problem and neural networks -- Background:\n",
    "\n",
    "[The XOR gate is an interesting problem in neural networks](http://www.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node19.html). [Marvin Minsky](https://en.wikipedia.org/wiki/Marvin_Minsky) and [Samuel Papert](https://en.wikipedia.org/wiki/Seymour_Papert) in their book [ 'Perceptrons' (1969)](https://en.wikipedia.org/wiki/Perceptrons_(book) showed that the XOR gate cannot be solved using a two layer perceptron, since the solution for a XOR gate was not linearly separable. This conclusion lead to a significantly reduced interest in[ Frank Rosenblatt's](https://en.wikipedia.org/wiki/Frank_Rosenblatt) perceptrons as a mechanism for building artificial intelligence applications.\n",
    "\n",
    "Some of these earliest work in AI were using networks or circuits of connected units to simulate intelligent behavior. Examples of this kind of work are called \"connectionism\". [After the publication of 'Perceptrons', the interest in connectionism significantly reduced](https://en.wikipedia.org/wiki/AI_winter#The_abandonment_of_connectionism_in_1969), till the renewed interest following the works of [John Hopfield](https://en.wikipedia.org/wiki/John_Hopfield) and [David Rumelhart](https://en.wikipedia.org/wiki/David_Rumelhart).\n",
    "\n",
    "The assertions in the book 'Perceptrons' by Minsky was inspite of his thorough knowledge that the powerful perceptrons have multiple layers and that Rosenblatt's basic feed-forward perceptrons have three layers. In the book, to deceive unsuspecting readers, Minsky defined a perceptron as a two-layer machine that can handle only linearly separable problems and, for example, cannot solve the exclusive-OR problem. [The Minsky-Papert collaboation is now believed to be a political maneuver and a hatchet job for contract funding by some knowledgeable scientists](http://csis.pace.edu/~ctappert/srd2011/rosenblatt-contributions.htm). This strong, unidimensional and misplaced criticism of perceptrons essentially halted work on practical, powerful artificial intelligence systems that were based on neural-networks for nearly a decade.\n",
    "\n",
    "Part 1 of this notebook explains how to build a very basic neural network in numpy. This perceptron like neural network is trained to predict the output of a [XOR gate](https://en.wikipedia.org/wiki/XOR_gate).\n",
    "\n",
    "![CMOS XOR Gate](https://github.com/rahulremanan/python_tutorial/raw/master/Fundamentals_of_deep-learning/media/CMOS_XOR_Gate.png)\n",
    "\n",
    "#### XOR gate table:\n",
    "\n",
    "![XOR Gate Table](https://github.com/rahulremanan/python_tutorial/raw/master/Fundamentals_of_deep-learning/media/XOR_Gate_Table.png)\n",
    "\n",
    "#### Image below shows an example of a lienarly separable dataset:\n",
    "\n",
    "![Linearly separable points](https://github.com/rahulremanan/python_tutorial/raw/master/Fundamentals_of_deep-learning/media/linearly_spearable_points.gif)\n",
    "\n",
    "#### Image below shows the XOR gate problem and no linear separation:\n",
    "\n",
    "![XOR problem](https://github.com/rahulremanan/python_tutorial/raw/master/Fundamentals_of_deep-learning/media/XOR_gate.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical intuition of machine learning:\n",
    "\n",
    "Consider an input matrix with the entire training data, represented as set **_X_** and the corresponding labels, represented as set **_Y_**. \n",
    "\n",
    "An ideal machine learning model can be defined as a special type of [surjective function](https://en.wikipedia.org/wiki/Surjective_function) that maps the set **_X_**, which contains all the input elements **__x__**; to the set **_Y_**, which contains all the label elements **__y__**.\n",
    "\n",
    "$${f: X \\rightarrow Y}$$\n",
    "$${where}$$\n",
    "$${\\forall x \\in X,\\hspace{1em}\\exists \\forall y \\in Y\\hspace{1em}  \\mid\\hspace{1em}f(x) = y}$$\n",
    "\n",
    "$${{For\\hspace{0.5em}all}\\hspace{1em}x \\in X,\\hspace{1em}there\\hspace{0.5em}exists\\hspace{0.5em}{for\\hspace{0.5em}all}\\hspace{1em}y \\in Y;\\hspace{1em}{a\\hspace{0.5em}function\\hspace{0.5em}such\\hspace{0.5em}that:}\\hspace{1em} f(x) = y.}$$\n",
    "\n",
    "An important result from the surjective function based definition of machine learning is that, they are not true universal function approximators; since the universe of all the mathematical operations cannot be expressed as a surjective function, due to [Cantor's paradox](https://en.wikipedia.org/wiki/Cantor%27s_paradox). \n",
    "\n",
    "Due to the very high effectiveness of machine learning models in approximating a variety of practical problems, they are understandably mischaracterized as universal function approximators.\n",
    "\n",
    "But, from a practical stand-point; machine learning models do not have to satisfy the criteria of a true universal function approximator, to be useful. \n",
    "\n",
    "Since the most commonly encountered phenomena in the universe can be treated as just a smaller subset of the universe of all the mathematical operations, machine learning models can be applied to approximate these phenomena very successfully. \n",
    "\n",
    "Therefore, machine learning models are highly effective quasi or pseudo universal function approximators; capable of solving most, but not all problems in the universe of all mathematical operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxBnIX3B_C0w"
   },
   "source": [
    "# [Dot product](https://en.wikipedia.org/wiki/Dot_product):\n",
    "\n",
    "$$\n",
    "{\\begin{bmatrix}\n",
    "a_{11} & a_{12} & ... & a_{1n} \\\\\n",
    "a_{21} & a_{22} & ... & a_{2n} \\\\\n",
    "... \\\\\n",
    "a_{m1} & a_{m2} & ... & a_{mn} \\\\\n",
    "\\end{bmatrix}\n",
    "}{.}\n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{12} & ... & b_{1p} \\\\\n",
    "b_{21} & b_{22} & ... & b_{2p} \\\\\n",
    "... \\\\\n",
    "b_{n1} & b_{n2} & ... & b_{np} \\\\\n",
    "\\end{bmatrix}\n",
    "=\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\Sigma_{i=1}^na_{1i} \\times b_{i1} & \\Sigma_{i=1}^na_{1i} \\times b_{i2} & ... & \\Sigma_{i=1}^na_{1i} \\times b_{ip} \\\\\n",
    "\\Sigma_{i=1}^na_{2i} \\times b_{i1} & \\Sigma_{i=1}^na_{2i} \\times b_{i2} & ... & \\Sigma_{i=1}^na_{2i} \\times b_{ip} \\\\\n",
    "... \\\\\n",
    "\\Sigma_{i=1}^na_{mi} \\times b_{i1} & \\Sigma_{i=1}^na_{mi} \\times b_{i2} & ... & \\Sigma_{i=1}^na_{mi} \\times b_{ip} \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTMjM2oKZ6eN"
   },
   "source": [
    "# [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)):\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & ... & a_{1n} \\\\\n",
    "a_{21} & a_{22} & ... & a_{2n} \\\\\n",
    "... \\\\\n",
    "a_{m1} & a_{m2} & ... & a_{mn} \\\\\n",
    "\\end{bmatrix}\n",
    "⊙\n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{12} & ... & b_{1n} \\\\\n",
    "b_{21} & b_{22} & ... & b_{2n} \\\\\n",
    "... \\\\\n",
    "b_{m1} & b_{m2} & ... & b_{mn} \\\\\n",
    "\\end{bmatrix}\n",
    "=\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "(a_{11})(b_{11}) & (a_{12})(b_{12}) & ... & (a_{1n})(b_{1n}) \\\\\n",
    "(a_{21})(b_{21}) & (a_{22})(b_{22}) & ... & (a_{2n})(b_{2n}) \\\\\n",
    "... \\\\\n",
    "(a_{m1})(b_{m1}) & (a_{m2})(b_{m2}) & ... & (a_{mn})(b_{mn}) \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fs3jcaAVEGHj"
   },
   "source": [
    "# [Euclidean / Cartesian norm](https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm):\n",
    "\n",
    "$$\\|(a_1, a_2, ... ,a_n ) \\|=\\sqrt{a_1^2+a_2^2+..+a_n^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvLSvFnXa_sF"
   },
   "source": [
    "## **Import the dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPY_MlEFbHlq"
   },
   "outputs": [],
   "source": [
    "import gc, os, sys, copy, gzip, glob, math, time,             \\\n",
    "       psutil, pickle, hashlib, subprocess, random, sklearn,  \\\n",
    "       sklearn.datasets, sklearn.linear_model,                \\\n",
    "       matplotlib, sklearn.linear_model,                      \\\n",
    "       numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Helper functions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linux_shell(cmd_list:list, verbose:bool=False)->list:\n",
    "    out_list = {}\n",
    "    for cmd in cmd_list:\n",
    "        if verbose: print(f'Executing linux command: {cmd}')\n",
    "        result = subprocess.run([cmd], shell=True, capture_output=True)\n",
    "        if result.returncode != 0:\n",
    "            out_list.update({f'Console error message while executing {cmd}': result.stdout.decode('utf-8')})\n",
    "        else:\n",
    "            out_list.update({f'Console output while executing {cmd}': result.stdout.decode('utf-8')})\n",
    "        del result, cmd\n",
    "\n",
    "    return out_list\n",
    "\n",
    "def list_global_var():\n",
    "    for var in globals(): print(str(var))\n",
    "\n",
    "class Dict_to_Class:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "\n",
    "def memory_utilization():\n",
    "    print(f'Current memory utilization: {psutil.virtual_memory().percent}% ...')\n",
    "\n",
    "def save_pickle(\n",
    "        var:'Python variable',\n",
    "        file:'OS path string'='file.pkl', \n",
    "        protocol:int=-1,\n",
    "        compression:bool=True,\n",
    "        verbose:bool=False\n",
    "    )->None:\n",
    "    if verbose: print(f'Memory utilization: \\n{memory_utilization()}')\n",
    "    #==Create Pickle dump==#\n",
    "    if compression:\n",
    "         with gzip.open(file, 'wb') as f:\n",
    "             pickle.dump(var, f, protocol)\n",
    "    else:\n",
    "        pickle.dump(var, open(file,'wb'))\n",
    "    if verbose: \n",
    "        print(f'Memory utilization: \\n{memory_utilization()}')\n",
    "\n",
    "def load_pickle(file:str, compression:bool=True)->'Python variable':\n",
    "    if compression:\n",
    "        with gzip.open(file, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    else:\n",
    "        return pickle.load(open(file, 'rb'))\n",
    "\n",
    "def hash_generator(\n",
    "        inp_str:str, \n",
    "        digest_size:int=None, \n",
    "        hash_algorithm:str='blake2b', \n",
    "        verbose:bool=False\n",
    "    )->str:\n",
    "    '''\n",
    "        This is a hash generator function with support for using multiple hashing algorithms and variable digest sizes.\n",
    "\n",
    "        The function takes an input string and returns a string generated using a pre-defined hashing function.\n",
    "        The hashing function is specified using the hash_algorithm keyword argument.\n",
    "        The digest size is specified using the digest_size keyword argument.\n",
    "\n",
    "        Variable digest sizes are only available for 'Blake2B' and 'Blake2S' hashing algorithms.\n",
    "            For 'Blake2B' the following condition must be satisfied: 0 < digest_size <= 64.\n",
    "            For 'Blake2S' the following condition must be satisfied: 0 < digest_size <= 32.\n",
    "\n",
    "        Setting the digest_size parameter will return the output string of length: digest_size * 2.\n",
    "\n",
    "        The default settings are: hash_algorithm argument set as 'Blake2B' and digest_size argument set as None.\n",
    "\n",
    "        Further reading: https://docs.python.org/3/library/hashlib.html#blake2\n",
    "\n",
    "        Example usage:\n",
    "\n",
    "                        hash_generator(\n",
    "                            'I want to hash this message.', \n",
    "                            digest_size=16, \n",
    "                            hash_algorithm='Blake2B'\n",
    "                        )\n",
    "                        >>>'482959abd4dd96a1dbd0095fb0f4f29a'\n",
    "    '''\n",
    "\n",
    "    hash_crypto = getattr(hashlib, hash_algorithm.lower())\n",
    "\n",
    "    if digest_size:\n",
    "        assert hash_algorithm.lower() == 'blake2b' or hash_algorithm.lower() == 'blake2s', f'Unsupported hashing algorithm: {hash_algorithm} for a variable digest size of: {digest_size}. Try passing None to the digest_size parameter ...'\n",
    "        if hash_algorithm.lower() == 'blake2b':\n",
    "            assert digest_size > 0 and digest_size <=64, f'For {hash_algorithm} must satisfy: 0 < digest_size <=64 ...'\n",
    "        if hash_algorithm.lower() == 'blake2s' :\n",
    "            assert digest_size > 0 and digest_size <=32, f'For {hash_algorithm} must satisfy: 0 < digest_size <=32 ...'\n",
    "        h = hash_crypto(digest_size=digest_size)\n",
    "    else:\n",
    "        h = hash_crypto()\n",
    "\n",
    "    h.update(inp_str.encode('ascii'))\n",
    "    hash_output = str(h.hexdigest())\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Generated a {len(hash_output)} character hash using {hash_algorithm}: {hash_output} ...')\n",
    "\n",
    "    del hash_crypto, h\n",
    "\n",
    "    return hash_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWc9ZHloax_2"
   },
   "source": [
    "## **Part 01a -- Simple neural network as XOR gate using sigmoid activation function**:\n",
    "\n",
    "\n",
    "The XOR gate neural network implemention uses a two layer perceptron with sigmoid activation function. This portion of the notebook is a modified fork of the [neural network implementation in numpy by Milo Harper](https://github.com/miloharper/simple-neural-network)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNIxbmBXbPY8"
   },
   "source": [
    "### Create [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function):\n",
    "\n",
    "- The sigmoid function takes two input arguments: x and a boolean argument called 'derivative'\n",
    "- When the boolean argument is set as true, the sigmoid function calculates the derivative of x\n",
    "- The derivative of x is required when calculating error or performing back-propagation\n",
    "- The sigmoid function runs in every single neuron\n",
    "- The sigmoid funtion feeds forward the data by converting the numeric matrices to probablities\n",
    "\n",
    "**To implement the [sigmoid activation function using numpy](https://stackoverflow.com/questions/3985619/how-to-calculate-a-logistic-sigmoid-function-in-python), we use the mathematical formula:**\n",
    "\n",
    "$$\n",
    "  f(x) = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "## **[Backpropagation](https://en.wikipedia.org/wiki/Backpropagation)**:\n",
    "\n",
    "- Method to make the network better\n",
    "- [Mathematically we need to compute the derivative of the activation function](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/6dDj7/backpropagation-intuition-optional)\n",
    "\n",
    "#### If sigmoid function can be expressed as follows:\n",
    "\n",
    "$g_{sigmoid}(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "#### Then, the first [derivative](https://en.wikipedia.org/wiki/Derivative) of this function can be expressed as:\n",
    "\n",
    "$g'_{sigmoid}(z) = g_{sigmoid}(z)(1-g_{sigmoid}(z))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpv0pAonU2_8"
   },
   "source": [
    "## **Forward pass and backpropagation using sigmoid activation:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iM-3_NoSriE"
   },
   "source": [
    "### Implementing sigmoid function using math library in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-I2UVtrSriI"
   },
   "outputs": [],
   "source": [
    "x  = -1.2\n",
    "y = 1 / (1 + math.exp(-x))\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WaaaT6BRSriS"
   },
   "outputs": [],
   "source": [
    "y = 1 / (1 + np.exp(-x))\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Activation functions**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1NWiP4NAbOYY"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    '''\n",
    "        Arguments:\n",
    "          x: input\n",
    "          derivative: boolean to specify if the derivative of the function should be computed\n",
    "    '''\n",
    "    if derivative:\n",
    "        return (x * (1 - x))\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "def ReLU(x, derivative=False):\n",
    "  if derivative:\n",
    "      return np.where(x < 0, 0, 1)\n",
    "  x_relu = np.maximum(x, 0)\n",
    "  return x_relu\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "  return ReLU(x, derivative=derivative)\n",
    "\n",
    "def relu_derivative(x, y):\n",
    "    return np.array([x])[np.array([y])<0]\n",
    "\n",
    "def tanh(x, derivative=False):\n",
    "    if derivative:\n",
    "        return (1 - (np.power(x, 2)))\n",
    "    return np.tanh(x)\n",
    "\n",
    "def arctan(x, derivative=False):\n",
    "    if derivative:\n",
    "        return (np.cos(x) ** 2)\n",
    "    return np.arctan(x)\n",
    "\n",
    "def step(x, derivative=False):\n",
    "    if derivative:\n",
    "        return x\n",
    "    return np.where(x < 0, 0, 1)\n",
    "\n",
    "def gaussian(x, derivative=False):\n",
    "    if derivative:\n",
    "        return -2 * x * np.exp(-np.power(x, 2))\n",
    "    return np.exp(-np.power(x, 2))\n",
    "\n",
    "def exp(x):\n",
    "    exp_scores = np.exp(x)\n",
    "    exp_scores_normalization = np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return exp_scores / exp_scores_normalization\n",
    "\n",
    "def linear(x, a=1.0, b=0.0):\n",
    "    return a * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of sigmoid activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1TDFr6HlghF"
   },
   "outputs": [],
   "source": [
    "sigmoid(-1.2, derivative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vLJoi0zRzI8B"
   },
   "outputs": [],
   "source": [
    "x = -1.2\n",
    "y_d = (1 / (1 + np.exp(x))) * (1 - (1 / (1 + np.exp(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bM9W7aYzcMZ"
   },
   "outputs": [],
   "source": [
    "print(y_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fTEF8Zhy-si"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    sigmoid(\n",
    "        0.23147521650098238, \n",
    "        derivative=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lg2-3qiw-QtZ"
   },
   "source": [
    "### Plotting sigmoid activation function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zj3nOOuz_JCB"
   },
   "outputs": [],
   "source": [
    "xmin, xmax = -15, 15\n",
    "ymin, ymax = -0.1, 1.1\n",
    "step_size = 0.01\n",
    "\n",
    "x = list(np.arange(xmin, xmax, step_size))\n",
    "y = [sigmoid(i) for i in x]\n",
    "\n",
    "axis = [xmin, xmax, ymin, ymax]\n",
    "plt.axhline(y=0.5, color='C2', alpha=0.5)\n",
    "plt.axvline(x=0, color='C2', alpha=0.5)\n",
    "plt.axis(axis)\n",
    "plt.plot(x, y, linewidth=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of rectified linear units (ReLU) activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([3])\n",
    "print(a)\n",
    "a[np.array([-1]) < 0] = 0\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the rectified linear units (ReLU) activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, xmax = -6.0, 6.0\n",
    "ymin, ymax = -1.1, 1.1\n",
    "\n",
    "x = list(np.arange(xmin, xmax, 0.1))\n",
    "y = [ReLU(i) for i in x]\n",
    "\n",
    "axis = [xmin, xmax, ymin, ymax]\n",
    "plt.axhline(y=0.0, color='C2', alpha=0.5)\n",
    "plt.axvline(x=0.0, color='C2', alpha=0.5)\n",
    "plt.axis(axis)\n",
    "plt.plot(x, y, linewidth=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of hyperbolic tan (tanh) activation function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting hyperbolic tan (tanh) activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, xmax = -6.0, 6.0\n",
    "ymin, ymax = -1.1, 1.1\n",
    "\n",
    "x = list(np.arange(xmin, xmax, 0.1))\n",
    "y = [tanh(i) for i in x]\n",
    "\n",
    "axis = [xmin, xmax, ymin, ymax]\n",
    "\n",
    "plt.axhline(y=0, color='C2', alpha=0.5)\n",
    "plt.axvline(x=0, color='C2', alpha=0.5)\n",
    "plt.axis(axis)\n",
    "plt.plot(x, y, linewidth=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4p1UNLTLfMpQ"
   },
   "source": [
    "### Create an input data matrix as numpy array:\n",
    "- Matrix with n number of dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WoOXgHDUEBPg"
   },
   "source": [
    "## Defining the input matrix and output data matrix as numpy arrays:\n",
    "$ x = \n",
    "\\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$ y = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7BWTaxmfXP8"
   },
   "outputs": [],
   "source": [
    "x = np.array(\n",
    "    [\n",
    "        [0, 0], [1, 1], [0, 1], [1, 0]\n",
    "    ]\n",
    ")\n",
    "\n",
    "y = np.array(\n",
    "    [\n",
    "        [0], [0], [1], [1]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vu58IidWCb8B"
   },
   "outputs": [],
   "source": [
    "print('Shape of the input matrix: ', x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LKpZI-4yBJQ"
   },
   "outputs": [],
   "source": [
    "print('Number of rows: ', x.shape[0], 'Number of columns: ', x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pqe0NKcTSrlG"
   },
   "outputs": [],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the truth table of an XOR gate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IqevNNSS9f8K"
   },
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    x[:,0],\n",
    "    x[:,1],\n",
    "    s=180,\n",
    "    c=y,\n",
    "    cmap=plt.colormaps.get_cmap('Spectral')\n",
    ")\n",
    "for i, (i_x, i_y) in enumerate(x):\n",
    "    i_z = y[i]\n",
    "    p_x, p_y = i_x, i_y\n",
    "    if i_x == 1:\n",
    "        p_x = i_x - 0.35\n",
    "    plt.text(\n",
    "        p_x + 0.02,\n",
    "        p_y - 0.02,\n",
    "        f'({i_x}, {i_y} ► {i_z})',\n",
    "        fontsize=18\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5ieANcznTxH"
   },
   "source": [
    "## [For loop in Python](https://wiki.python.org/moin/ForLoop):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXnkWkp3Srj8"
   },
   "outputs": [],
   "source": [
    "x_t = (1, 2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8QIvrcASrkG"
   },
   "outputs": [],
   "source": [
    "print(len(x_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdOmH9j2SrkY"
   },
   "outputs": [],
   "source": [
    "for i in range(len(x_t)):\n",
    "    print(\n",
    "        'This is the {} element in the tuple ...'.format(i)\n",
    "    )\n",
    "    print(\n",
    "        'The value is: {} ...'.format(x_t[i])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weIKLkmUnssa"
   },
   "source": [
    "### Using ```enumerate```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0cx1XsjncrQ"
   },
   "outputs": [],
   "source": [
    "for idx, i  in enumerate(x_t):\n",
    "    print (f'This is the {idx} element in the tuple ...')\n",
    "    print (f'The value is: {i} ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMdw0xyZjlL8"
   },
   "source": [
    "## Define the neural network:\n",
    "$$\n",
    "f_n(\\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix})\n",
    "\\rightarrow\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYxrcX3uikpq"
   },
   "source": [
    "### Create a random number seed:\n",
    "\n",
    "- Random number seeding is useful for producing reproducible results\n",
    "- Deterministic random number generator output using a seed\n",
    "- Unsuitable for experiments that require true random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Atv5s0UaikH_"
   },
   "outputs": [],
   "source": [
    "seed = 1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mz5E6sOnjAFy"
   },
   "source": [
    "### Create a synapse matrix:\n",
    "\n",
    "- A function applied to the synapses\n",
    "- For the first synapse, weights matrix of shape: input_shape_1 x input_shape_2 is created\n",
    "- For the second synapse, weights matrix of shape: input_shape_2 x output_dim is created\n",
    "-  This function also introduces the first hyper-parameter in neural network tuning called 'bias_val', which is the bias value for the synaptic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_D5YYvZ_jAlU"
   },
   "outputs": [],
   "source": [
    "bias_val = 1\n",
    "\n",
    "output_dim = 1\n",
    "\n",
    "input_shape_1 = x.shape[1]\n",
    "input_shape_0 = x.shape[0]\n",
    "\n",
    "hidden_layer_size = 4 # 3 # 2 # 5 #\n",
    "\n",
    "synapse_0 = 2 * np.random.random((input_shape_1, hidden_layer_size)) - bias_val\n",
    "synapse_1 = 2 * np.random.random((hidden_layer_size, output_dim)) - bias_val\n",
    "\n",
    "loss_col = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBmLFJK2Srls"
   },
   "outputs": [],
   "source": [
    "print(x.shape, input_shape_0, input_shape_1, synapse_0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpZtw1d1GqUY"
   },
   "source": [
    "## Define the first synapse (synapse 0):\n",
    "\n",
    ">\n",
    "$$\n",
    "Eg.\\\\\n",
    "$$\n",
    "$$\n",
    "Synapse_0\n",
    "→\n",
    "\\begin{bmatrix}\n",
    "-0.16595599 & 0.44064899 & -0.99977125 & -0.39533485\\\\\n",
    "-0.70648822 & -0.81532281 & -0.62747958 & -0.30887855\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrapQeblRdmv"
   },
   "outputs": [],
   "source": [
    "print(synapse_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OMAVFgKSrl5"
   },
   "outputs": [],
   "source": [
    "print(synapse_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_8Ikosj16kI"
   },
   "source": [
    "## Define the second synapse (synapse 1):\n",
    "\n",
    ">\n",
    "$$\n",
    "Eg.\\\\\n",
    "$$\n",
    "$$\n",
    "Synapse_1\n",
    "→\n",
    "\\begin{bmatrix}\n",
    "-0.20646505\\\\\n",
    "0.07763347\\\\\n",
    "-0.16161097\\\\\n",
    "0.370439\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJA-rbkTNJ5W"
   },
   "outputs": [],
   "source": [
    "print(synapse_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InWiWkFKR4_q"
   },
   "source": [
    "### **Implement a single forward pass of the XOR input table**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l23aH9iXR_M7"
   },
   "source": [
    "### Create the input layer (layer 0):\n",
    "\n",
    "$$\n",
    "layer_0 = \\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2IViHlbwRnfz"
   },
   "outputs": [],
   "source": [
    "layer_0 = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFAyl3q320Fp"
   },
   "source": [
    "### Compute the dot product between layer 0 and synapse 0:\n",
    "$$\n",
    "{\n",
    "Eg.\\\\\n",
    "\\begin{bmatrix}\n",
    "0 & 0 \\\\\n",
    "1 & 1 \\\\\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "}{.}{\n",
    "\\begin{bmatrix}\n",
    "-0.16595599 & 0.44064899 & -0.99977125 & -0.39533485\\\\\n",
    "-0.70648822 & -0.81532281 & -0.62747958 & -0.30887855\n",
    "\\end{bmatrix}\n",
    "}\n",
    "{=}\n",
    "{\\\\\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "-0.87244421 & -0.37467382 & -1.62725083 & -0.7042134 \\\\\n",
    "-0.16595599  & 0.44064899 & -0.99977125 & -0.39533485 \\\\\n",
    "-0.70648822 & -0.81532281 & -0.62747958 & -0.30887855 \\\\\n",
    "\\end{bmatrix}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qyM3edZY27mY"
   },
   "outputs": [],
   "source": [
    "dot_product_0 = np.dot(layer_0, synapse_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FBu56nZ34wIx"
   },
   "outputs": [],
   "source": [
    "print(dot_product_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lR6Tm83K6Bi-"
   },
   "source": [
    "### Apply bias value for dot product 0:\n",
    "\n",
    "$$\n",
    "Eg. \\\\\n",
    "bias {\\space} value_0 = 1 \\\\\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "-0.87244421  & -0.37467382 & -1.62725083 & -0.7042134 \\\\\n",
    "-0.16595599  & 0.44064899  & -0.99977125 & -0.39533485 \\\\\n",
    "-0.70648822  & -0.81532281 & -0.62747958 & -0.30887855 \\\\\n",
    "\\end{bmatrix}\n",
    "- bias {\\space} value_0 =\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-1.          & -1.         & -1.         & -1.        \\\\\n",
    " -1.87244421 & -1.37467382 & -2.62725083 & -1.7042134 \\\\\n",
    " -1.16595599 & -0.55935101 & -1.99977125 & -1.39533485\\\\\n",
    " -1.70648822 & -1.81532281 & -1.62747958 & -1.30887855 \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FjvtD0X6FGS"
   },
   "outputs": [],
   "source": [
    "bias_val_0 = 1\n",
    "dot_product_0 = dot_product_0 - bias_val_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2kH__hB6T6U"
   },
   "outputs": [],
   "source": [
    "print(dot_product_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1a9qiOg8SDxx"
   },
   "source": [
    "### Create layer 1 by applying the activation function to dot product 0:\n",
    "$$\n",
    "g_{sigmoid}(z) = \\frac{1}{1+e^{-z}}\\\\\n",
    "$$\n",
    "$$\n",
    "Eg.\\\\\n",
    "g_{sigmoid}(\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "-0.87244421 & -0.37467382 & -1.62725083 & -0.7042134  \\\\\n",
    "-0.16595599  & 0.44064899 & -0.99977125 & -0.39533485 \\\\\n",
    "-0.70648822 & -0.81532281 & -0.62747958 & -0.30887855 \\\\\n",
    "\\end{bmatrix}) = \\\\\n",
    "$$\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0.26894142 & 0.26894142 & 0.26894142 & 0.26894142 \\\\\n",
    "0.13325916 & 0.20186577 & 0.06740506 & 0.15391577 \\\\\n",
    "0.23758674 & 0.36369764 & 0.11922694 & 0.19855744 \\\\\n",
    "0.15361976 & 0.13999605 & 0.16417593 & 0.21267456 \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2oPrwRUSCvZ"
   },
   "outputs": [],
   "source": [
    "layer_1 = sigmoid(dot_product_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "veCcWQqsSWTw"
   },
   "outputs": [],
   "source": [
    "print(layer_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ryAjZ_5ScMe"
   },
   "outputs": [],
   "source": [
    "print(layer_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDgoIZe1541C"
   },
   "source": [
    "### Compute dot product between layer 1 and synapse 1:\n",
    "$$\n",
    "{Eg.}\\\\\n",
    "{\n",
    "\\begin{bmatrix}\n",
    "0.26894142 & 0.26894142 & 0.26894142 & 0.26894142 \\\\\n",
    "0.13325916 & 0.20186577 & 0.06740506 & 0.15391577 \\\\\n",
    "0.23758674 & 0.36369764 & 0.11922694 & 0.19855744 \\\\\n",
    "0.15361976 & 0.13999605 & 0.16417593 & 0.21267456 \\\\\n",
    "\\end{bmatrix}\n",
    "}{.}\n",
    "{\n",
    "\\begin{bmatrix}\n",
    "-0.20646505\\\\\n",
    "0.07763347\\\\\n",
    "-0.16161097\\\\\n",
    "0.370439\n",
    "\\end{bmatrix} = \\\\\n",
    "}\n",
    "{\n",
    "\\begin{bmatrix}\n",
    "0.02151436 \\\\\n",
    "0.03428119 \\\\\n",
    "0.03346679 \\\\\n",
    "0.03140159 \\\\\n",
    "\\end{bmatrix}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jl0KP5G4537s"
   },
   "outputs": [],
   "source": [
    "dot_product_1 = np.dot(layer_1, synapse_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xIvQcIm47SAS"
   },
   "outputs": [],
   "source": [
    "print(dot_product_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSL-QFUCHSWY"
   },
   "source": [
    "### Apply bias value for the dot product 1:\n",
    "$$\n",
    "Eg.\\\\\n",
    "bias{\\space}value_1= 1 \\\\\n",
    "\\begin{bmatrix}\n",
    "0.02151436 \\\\\n",
    "0.03428119 \\\\\n",
    "0.03346679 \\\\\n",
    "0.03140159 \\\\\n",
    "\\end{bmatrix}-bias{\\space}value_1=\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-0.97848564\\\\\n",
    "-0.96571881\\\\\n",
    "-0.96653321\\\\\n",
    "-0.96859841\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "019g-slSHXze"
   },
   "outputs": [],
   "source": [
    "bias_val_1 = 1\n",
    "dot_product_1 = dot_product_1 - bias_val_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GoyOnIbXHxrj"
   },
   "outputs": [],
   "source": [
    "print(dot_product_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iq0WpB66W53S"
   },
   "source": [
    "### Create layer 2 by passing the dot product 2 through the activation function:\n",
    "\n",
    "$$\n",
    "g_{sigmoid}(z) = \\frac{1}{1+e^{-z}}\\\\\n",
    "$$\n",
    "$$\n",
    "Eg.\\\\\n",
    "g_{sigmoid}(\\begin{bmatrix}\n",
    "-0.97848564\\\\\n",
    "-0.96571881\\\\\n",
    "-0.96653321\\\\\n",
    "-0.96859841\n",
    "\\end{bmatrix})=\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0.27319237\\\\\n",
    "0.27573466\\\\\n",
    "0.27557205\\\\\n",
    "0.27515996\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNBC3fusSnDW"
   },
   "outputs": [],
   "source": [
    "layer_2 = sigmoid(dot_product_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJSj2_RvS28u"
   },
   "outputs": [],
   "source": [
    "print(layer_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8WcU1gKAS64s"
   },
   "outputs": [],
   "source": [
    "print(layer_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfYQKDPw1tDZ"
   },
   "source": [
    "## **Part 01b -- [Backpropagation](https://en.wikipedia.org/wiki/Backpropagation)**:\n",
    "\n",
    "This backpropagation example is a naive implmentation of the\n",
    "$\\frac{1}{2}$ of mean squared error as the loss function.\n",
    "\n",
    "Therefore, the derivative of the loss function is computed first, which is:\n",
    "$ (\\frac{\\partial E}{\\partial o_j}) $:\n",
    "$ \\\\\n",
    "\\frac{\\partial E}{\\partial o_j} = \\frac{\\partial }{\\partial o_j} {\\frac{1}{2}}(t-y)^2 {= y-t}\n",
    "$\n",
    "\n",
    "Layer delta is computed using:\n",
    "$\n",
    "\\\\\n",
    "\\Delta w_{ij} = -\\eta \\frac{\\partial E}{\\partial w_{ij}} $\n",
    "\n",
    "In this implementation, learning rate ($\\eta$) = 1\n",
    "\n",
    "\n",
    "Read more by following the backpropogation link above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6ZU-RCITHOW"
   },
   "source": [
    "## Implement a single backprop pass:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLg9oOIHavmk"
   },
   "source": [
    "### Compute the partial first derivative of the loss function:\n",
    "$\\frac{\\partial }{\\partial o_j} {(\\frac{1}{2}}(t-y)^2) \\text{ } { = y-t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-q92fJfTFx3"
   },
   "outputs": [],
   "source": [
    "output_loss_derivative = (layer_2 - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUO7X2WOTZe9"
   },
   "outputs": [],
   "source": [
    "print(output_loss_derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U91Ypxc3s8QZ"
   },
   "source": [
    "### Compute the partial first derivative of the activation function for the back-propagation step:\n",
    "$g_{sigmoid}(z) = \\frac{1}{1+e^{-z}}$ \n",
    "\n",
    "$g'_{sigmoid}(z) = \\frac{\\partial}{\\partial z} {(\\frac{1}{1+e^{-z}})}$\n",
    "\n",
    "${\\text{Let }f(z) \\text{ } = 1 + e^{-z}}$\n",
    "\n",
    "${\\therefore g_{sigmoid}(z) \\text{ } = {f(z)}^{-1}}$\n",
    "\n",
    "${\\therefore \\frac{\\partial{(g_{sigmoid}(z))}}{\\partial{z}} \\text{ } = \\frac{\\partial{({f(z)}^{-1})}}{\\partial{z}}}$\n",
    "\n",
    "${\\therefore \\text{By applying chain rule, } \\frac{\\partial{({f(z)}^{-1})}}{\\partial{z}} = \\frac{\\partial{({f(z)}^{-1})}}{\\partial{(f(z))}}{.}\\frac{\\partial{({f(z)})}}{\\partial{z}}}$\n",
    "\n",
    "$\\frac{\\partial{({f(z)}^{-1})}}{\\partial{f(z)}} = - {f(z)}^{-2}$\n",
    "\n",
    "$\\frac{\\partial{({f(z)})}}{\\partial{z}} = {\\frac{\\partial{(1)}}{\\partial{z}}} + {\\frac{\\partial{(e^{-z})}}{\\partial{z}}} $\n",
    "\n",
    "${\\because \\frac{\\partial{(e^{x})}}{\\partial{x}} = e^{x}, \\text{by applying chain rule; } \\frac{\\partial{(e^{-z})}}{\\partial{z}} = \\frac{\\partial{(e^{-z})}}{\\partial{(-z)}}{.}\\frac{\\partial{(-z)}}{\\partial{z}}}$\n",
    "\n",
    "${\\therefore {\\frac{\\partial{(e^{-z})}}{\\partial{z}}} = -e^{-z}}$\n",
    "\n",
    "${\\therefore \\frac{\\partial{({f(z)})}}{\\partial{z}} = -e^{-z}}$\n",
    "\n",
    "${\\therefore \\frac{\\partial{({f(z)}^{-1})}}{\\partial{z}} = {- {f(z)}^{-2}}{.}{-e^{-z}}}$\n",
    "\n",
    "${\\therefore \\frac{\\partial{({f(z)}^{-1})}}{\\partial{z}} = \\frac{e^{-z}}{{(1 + e^{-z})}^{2}}}$\n",
    "\n",
    "${\\therefore \\frac{\\partial{({f(z)}^{-1})}}{\\partial{z}} = \\frac{(1 + e^{-z}) - 1}{{(1 + e^{-z})}^{2}}}$\n",
    "\n",
    "${\\therefore \\frac{\\partial{({f(z)}^{-1})}}{\\partial{z}} = \\frac{(1 + e^{-z})}{{(1 + e^{-z})}^{2}} - (\\frac{1}{(1 + e^{-z})})^{2}}$\n",
    "\n",
    "${\\therefore \\frac{\\partial{({f(z)}^{-1})}}{\\partial{z}} = \\frac{1}{{(1 + e^{-z})}} - (\\frac{1}{(1 + e^{-z})})^{2}}$\n",
    "\n",
    "${\\therefore \\frac{\\partial{({f(z)}^{-1})}}{\\partial{z}} = \\frac{1}{{(1 + e^{-z})}} {.} (1 - \\frac{1}{(1 + e^{-z})})}$\n",
    "\n",
    "${\\because g_{sigmoid}(z) = \\frac{1}{1+e^{-z}} \\text{; } \\frac{\\partial{({f(z)}^{-1})}}{\\partial{z}} = g_{sigmoid}(z) {.} (1 - g_{sigmoid}(z))}$\n",
    "\n",
    "${\\therefore \\frac{\\partial{(g_{sigmoid}(z))}}{\\partial{z}} = g_{sigmoid}(z) {.} (1 - g_{sigmoid}(z))}$\n",
    "\n",
    "${\\therefore g'_{sigmoid}(z) = g_{sigmoid}(z)(1 - g_{sigmoid}(z))}$\n",
    "\n",
    "Described in this notebook is the naive implementation for computing: $$g'_{sigmoid}(z)$$\n",
    "\n",
    "In this implementation, the first derivative is defined as an additional optional output within the activation function; which is executed when the ```derivative``` flag is set to ```True```.\n",
    "\n",
    "$$\n",
    "Eg.\\\\\n",
    "g'_{sigmoid}(\n",
    "  \\begin{bmatrix}\n",
    "      0.27319237\\\\\n",
    "      0.27573466\\\\\n",
    "      0.27557205\\\\\n",
    "      0.27515996\n",
    "  \\end{bmatrix})=\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    0.1985583\\\\\n",
    "    0.19970506\\\\\n",
    "    0.19963209\\\\\n",
    "    0.19944695\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTBV1KHAs3OQ"
   },
   "outputs": [],
   "source": [
    "layer_2_derivative = sigmoid(layer_2, derivative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x5OJg838Wmqj"
   },
   "outputs": [],
   "source": [
    "print(layer_2_derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ngqzGUqTeA8"
   },
   "outputs": [],
   "source": [
    "layer_2_delta = (output_loss_derivative * layer_2_derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0_URoSNTTxce"
   },
   "outputs": [],
   "source": [
    "print(layer_2_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXOy2m09T-xM"
   },
   "outputs": [],
   "source": [
    "layer_1_error = (layer_2_delta.dot(synapse_1.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPxtHndIUbEb"
   },
   "outputs": [],
   "source": [
    "print(layer_1_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mAWdKLacUhMK"
   },
   "outputs": [],
   "source": [
    "layer_1_delta = layer_1_error * sigmoid(layer_1, derivative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d308EMWHU2XB"
   },
   "outputs": [],
   "source": [
    "print(layer_1_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SVDHJoYWEN6"
   },
   "source": [
    "### Updating the weights/synapses of the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHU45BMPVkcQ"
   },
   "outputs": [],
   "source": [
    "synapse_1 += layer_1.T.dot(layer_2_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5Dfujkm4OgA"
   },
   "outputs": [],
   "source": [
    "print(synapse_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTXFUcBcWKZ6"
   },
   "outputs": [],
   "source": [
    "synapse_0 += layer_0.T.dot(layer_1_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQCZOLNgWZ7C"
   },
   "outputs": [],
   "source": [
    "print(synapse_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 01d -- Auto-differentiation**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the partial first derivative of the activation function, for the back-propagation step; non-deterministically:\n",
    "\n",
    "In the previous section, the first derivative of the activation function was computed naively by defining an additional output that corresponded to the function which represented the first derivative of the associated activation function.\n",
    "\n",
    "Since this naive approach severely limits the practical utility of neural networks, as it requires a practitioner that develops a neural network; to understand the necessary calculus, to deterministically define the formula of the first derivative of any arbitrary activation function that they want to implement.\n",
    "\n",
    "An alternate approach to this problem is to compute the first derivative value for a activation function, using a technique called auto-differentiation.\n",
    "\n",
    "Auto-differentiation can be implemented in multiple ways.\n",
    "\n",
    "In this implementation, a random sampling approach is used to compute the approximate value to the first derivative; using Riemann differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stochastic auto differentiation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stochastically_evaluated_function_derivative(\n",
    "        inputs_list,\n",
    "        input_fn,\n",
    "        epsilon,\n",
    "        epsilon_regularization_factor=100,\n",
    "        num_eval_steps=30,\n",
    "        order=1,\n",
    "        enable_stochastic_partial_derivative=True,\n",
    "        enable_backprop_activation_nesting=False,\n",
    "        enable_normalization=True\n",
    "    ):\n",
    "\n",
    "    assert epsilon > 0 and epsilon < 1, f'Expected epsilon value to be a positive float between 0 and 1, instead received: {epsilon} ...'\n",
    "\n",
    "    if enable_backprop_activation_nesting:\n",
    "\n",
    "        assert (type(epsilon_regularization_factor) == int or type(epsilon_regularization_factor) == float) and epsilon_regularization_factor > 1, f'Expected the value passed using argument: epsilon_regularization_factor {epsilon_regularization_factor} to be greater than 1 ...'\n",
    "\n",
    "    num_auto_diff_steps, auto_diff_y_out = 0, 0\n",
    "\n",
    "    y_list = copy.deepcopy(inputs_list)\n",
    "\n",
    "    fn_y = input_fn(y_list)\n",
    "\n",
    "    y_rnd_idx_list = [\n",
    "\n",
    "        i for i in range(len(y_list))\n",
    "\n",
    "    ]\n",
    "\n",
    "    random.SystemRandom().shuffle(y_rnd_idx_list)\n",
    "\n",
    "    for i in range(math.ceil(num_eval_steps/len(y_list))):\n",
    "\n",
    "        for j in range(len(y_list)):\n",
    "\n",
    "            rnd_epsilon, prev_rnd_epsilon = None, None\n",
    "\n",
    "            rnd_epsilon_avg, rnd_epsilon_delta = None, None\n",
    "\n",
    "            num_pos_eps, num_neg_eps = 0, 0\n",
    "\n",
    "            output_neg_eps = False\n",
    "\n",
    "            for k in range(8):\n",
    "\n",
    "                y_rnd_list, pos_neg_list = [], []\n",
    "\n",
    "                for y_idx, y in enumerate(y_list):\n",
    "\n",
    "                    loop_cond = random.SystemRandom().choice(\n",
    "\n",
    "                        [\n",
    "\n",
    "                            True, \n",
    "\n",
    "                            False\n",
    "\n",
    "                        ]\n",
    "\n",
    "                    ) or y_idx == y_rnd_idx_list[j]              \\\n",
    "                    if enable_stochastic_partial_derivative else \\\n",
    "                    y_idx == y_rnd_idx_list[j] \n",
    "\n",
    "                    if loop_cond:\n",
    "\n",
    "                        if rnd_epsilon_avg is not None:\n",
    "\n",
    "                            rnd_epsilon = rnd_epsilon_avg\n",
    "\n",
    "                            prev_rnd_epsilon, rnd_epsilon_avg = None, None\n",
    "\n",
    "                        elif rnd_epsilon_delta is not None:\n",
    "\n",
    "                            rnd_epsilon = rnd_epsilon_delta\n",
    "\n",
    "                            assert rnd_epsilon_avg is None\n",
    "\n",
    "                            prev_rnd_epsilon, rnd_epsilon_delta = None, None\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            prev_rnd_epsilon = copy.deepcopy(rnd_epsilon)\n",
    "\n",
    "                            if output_neg_eps:\n",
    "\n",
    "                                num_pos_eps = 0\n",
    "\n",
    "                                rnd_epsilon = random.SystemRandom().uniform(\n",
    "\n",
    "                                    -abs(epsilon),\n",
    "\n",
    "                                    -(abs(epsilon) / 100)\n",
    "\n",
    "                                )\n",
    "\n",
    "                                num_neg_eps += 1\n",
    "\n",
    "                                if num_neg_eps == 2:\n",
    "\n",
    "                                    output_neg_eps = False\n",
    "\n",
    "                            else:\n",
    "\n",
    "                                num_neg_eps = 0\n",
    "\n",
    "                                rnd_epsilon = random.SystemRandom().uniform(\n",
    "\n",
    "                                    (abs(epsilon) / 100),\n",
    "\n",
    "                                    abs(epsilon)\n",
    "\n",
    "                                )\n",
    "\n",
    "                                num_pos_eps += 1\n",
    "\n",
    "                                if num_pos_eps == 2:\n",
    "\n",
    "                                    output_neg_eps = True\n",
    "\n",
    "                        if rnd_epsilon_avg is None and prev_rnd_epsilon is not None:\n",
    "\n",
    "                            if num_neg_eps == 2 or num_pos_eps == 2:\n",
    "\n",
    "                                rnd_epsilon_avg = (\n",
    "                                    \n",
    "                                    rnd_epsilon + prev_rnd_epsilon\n",
    "                                \n",
    "                                ) / 2\n",
    "\n",
    "                                rnd_epsilon_delta = rnd_epsilon - prev_rnd_epsilon\n",
    "\n",
    "                                num_pos_eps, num_neg_eps = 0, 0\n",
    "\n",
    "                                if rnd_epsilon_delta == 0 and prev_rnd_epsilon is not None and rnd_epsilon is not None:\n",
    "\n",
    "                                    rnd_epsilon_delta = random.SystemRandom().choice(\n",
    "\n",
    "                                        [\n",
    "\n",
    "                                            rnd_epsilon, \n",
    "\n",
    "                                            prev_rnd_epsilon\n",
    "\n",
    "                                        ]\n",
    "\n",
    "                                    )\n",
    "\n",
    "                                elif rnd_epsilon is not None:\n",
    "\n",
    "                                    rnd_epsilon_delta = rnd_epsilon\n",
    "\n",
    "                                prev_rnd_epsilon = None\n",
    "\n",
    "                            elif rnd_epsilon_delta == 0 and rnd_epsilon is not None:\n",
    "\n",
    "                                prev_rnd_epsilon = copy.deepcopy(rnd_epsilon)\n",
    "\n",
    "                        else:\n",
    "                            rnd_epsilon_avg = None\n",
    "\n",
    "                        y_rnd_list.append(\n",
    "                            \n",
    "                            y + rnd_epsilon\n",
    "                        \n",
    "                        )\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        y_rnd_list.append(y)\n",
    "\n",
    "                num_auto_diff_steps += 1\n",
    "\n",
    "                auto_diff_y_out += (\n",
    "\n",
    "                    input_fn(y_rnd_list) - fn_y\n",
    "\n",
    "                ) / rnd_epsilon\n",
    "\n",
    "                if enable_backprop_activation_nesting:\n",
    "\n",
    "                    for o in range(order):\n",
    "\n",
    "                        y_nested_rnd_list = []\n",
    "\n",
    "                        for y_idx, y_rnd in enumerate(y_rnd_list):\n",
    "\n",
    "                            loop_cond = random.SystemRandom().choice(\n",
    "\n",
    "                                [\n",
    "\n",
    "                                    True, \n",
    "\n",
    "                                    False\n",
    "\n",
    "                                ]\n",
    "\n",
    "                            ) or y_idx == y_rnd_idx_list[j]              \\\n",
    "                            if enable_stochastic_partial_derivative else \\\n",
    "                            y_idx == y_rnd_idx_list[j]\n",
    "\n",
    "                            if loop_cond:\n",
    "\n",
    "                                y_nested_rnd_list.append(\n",
    "\n",
    "                                    y_rnd * random.SystemRandom().uniform(\n",
    "\n",
    "                                        1 - (epsilon / epsilon_regularization_factor),\n",
    "\n",
    "                                        1 + (epsilon / epsilon_regularization_factor)\n",
    "\n",
    "                                    )\n",
    "\n",
    "                                )\n",
    "\n",
    "                            else:\n",
    "\n",
    "                                y_nested_rnd_list.append(y_rnd)\n",
    "\n",
    "                        num_auto_diff_steps += 1\n",
    "\n",
    "                        auto_diff_y_out += (input_fn(y_nested_rnd_list) - fn_y) / rnd_epsilon\n",
    "\n",
    "    auto_diff_y_out /= num_auto_diff_steps\n",
    "\n",
    "    if enable_normalization and enable_backprop_activation_nesting:\n",
    "\n",
    "        auto_diff_y_out /= order\n",
    "\n",
    "    return auto_diff_y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stochastic_auto_differentiation(\n",
    "        x,\n",
    "        input_fn,\n",
    "        epsilon,\n",
    "        epsilon_regularization_factor=100,\n",
    "        num_eval_steps=30,\n",
    "        order=1,\n",
    "        enable_backprop_activation_nesting=False,\n",
    "        enable_normalization=False\n",
    "    ):\n",
    "\n",
    "    def input_fn_for_list_input(\n",
    "            \n",
    "            x, \n",
    "        \n",
    "            input_fn=input_fn\n",
    "    \n",
    "        ):\n",
    "\n",
    "        assert type(x) == list, f'Expected the auto differentiation input to be a list, instead reaceived: {type(x)} ...'\n",
    "        assert len(x) == 1, f'Expected the auto differentiation input to be a list of length 1, instead received a list of length: {len(x)} ...'\n",
    "        return input_fn(x[0])\n",
    "\n",
    "    return compute_stochastically_evaluated_function_derivative(\n",
    "               [x],\n",
    "               input_fn=input_fn_for_list_input,\n",
    "               epsilon=epsilon,\n",
    "               epsilon_regularization_factor=epsilon_regularization_factor,\n",
    "               num_eval_steps=num_eval_steps,\n",
    "               order=order,\n",
    "               enable_stochastic_partial_derivative=True,\n",
    "               enable_backprop_activation_nesting=enable_backprop_activation_nesting,\n",
    "               enable_normalization=enable_normalization\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2_derivative = sigmoid(layer_2, derivative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nLayer 2 values: \\n', layer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nLayer 2 derivative:', '\\n', layer_2_derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_diff_layer_2_derivative = compute_stochastic_auto_differentiation(\n",
    "    layer_2,\n",
    "    sigmoid,\n",
    "    1e-8,\n",
    "    100,\n",
    "    order=1\n",
    ")\n",
    "print(f'\\nLayer 2 derivative using stochastic auto differentiation: \\n {auto_diff_layer_2_derivative}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment comparing stochastic automatic derivatives with deterministically evaluated ones, using sigmoid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, xmax = -12, 12 # 0.272, 0.274 #\n",
    "x_val_list, derivative_error_list = [], []\n",
    "\n",
    "y_derivative_list, y_auto_derivative_list = [], []\n",
    "\n",
    "verbose = False\n",
    "num_trials = 100\n",
    "\n",
    "x_scr_val = [val[0] for val in layer_2]\n",
    "\n",
    "for i in range(num_trials):\n",
    "    x_rnd_val_list = copy.deepcopy(x_scr_val)\n",
    "\n",
    "    x_rnd_val_list.append(\n",
    "        random.SystemRandom().uniform(\n",
    "            xmin, xmax\n",
    "        )\n",
    "    )\n",
    "\n",
    "    x_sig_eval = random.SystemRandom().choice(x_rnd_val_list)\n",
    "\n",
    "    y_sig_eval = (1 / (1 + np.exp(x_sig_eval)))\n",
    "\n",
    "    x_val_list.append(x_sig_eval)\n",
    "\n",
    "    y_derivative_eval = (1 / (1 + np.exp(x_sig_eval))) * (1 - (1 / (1 + np.exp(x_sig_eval))))\n",
    "\n",
    "    y_fn_derivative_eval = sigmoid(\n",
    "        y_sig_eval,\n",
    "        derivative=True\n",
    "    )\n",
    "\n",
    "    y_derivative_list.append(\n",
    "        y_fn_derivative_eval\n",
    "    )\n",
    "\n",
    "    y_auto_derivative_eval = compute_stochastic_auto_differentiation(\n",
    "        x_sig_eval, \n",
    "        sigmoid, \n",
    "        1e-8, \n",
    "        10,\n",
    "        order=1\n",
    "    )\n",
    "\n",
    "    y_auto_derivative_list.append(\n",
    "        y_auto_derivative_eval\n",
    "    )\n",
    "\n",
    "    y_derivative_error = y_auto_derivative_eval - y_fn_derivative_eval\n",
    "    derivative_error_list.append(\n",
    "        y_derivative_error\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f'Sigmoid output for {x_sig_eval}: {y_sig_eval} ...'\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            'Sigmoid derivative computed using the formula: ' +\n",
    "            str(y_derivative_eval) +\n",
    "            ' ...'\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            'Sigmoid derivative computed deterministically: ' +\n",
    "            str(y_fn_derivative_eval) +\n",
    "            ' ...'\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            'Sigmoid derivative computed using stochastic auto differentiation: ' +\n",
    "            str(y_auto_derivative_eval) +\n",
    "            ' ...'\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f'Error between stochastically computed derivative and deterministic derivative: {np.abs(y_derivative_error)} ... '\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_abs_err = np.mean(np.abs(derivative_error_list))\n",
    "stdev_err = np.std(derivative_error_list)\n",
    "min_err, max_err = np.min(np.array(derivative_error_list)), np.max(np.array(derivative_error_list))\n",
    "\n",
    "print(\n",
    "    f'Number of trials comparing stochastically computed derivatives and deterministically computed ones: {num_trials} ...'\n",
    ")\n",
    "\n",
    "print(\n",
    "    f'Mean absolute error between stochastically computed derivative and deterministic derivative: {mean_abs_err} ... '\n",
    ")\n",
    "\n",
    "print(\n",
    "    f'Standard deviation: {stdev_err}, minimum error: {min_err} and maximum error: {max_err} ...'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_sorted_list = copy.deepcopy(x_val_list)\n",
    "derivative_error_sorted_list = copy.deepcopy(derivative_error_list)\n",
    "tuples = sorted(\n",
    "    zip(\n",
    "        x_val_sorted_list, \n",
    "        derivative_error_sorted_list\n",
    "    )\n",
    ")\n",
    "x_val_list_sorted, derivative_error_list_sorted = [t[0] for t in tuples], [t[1] for t in tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaler(inp_arr, verbose=False):\n",
    "    assert isinstance(inp_arr, np.ndarray), f'Expected the input to be a Numpy array, instead received an input of type: {type(inp_arr)} ...'\n",
    "    max_val, min_val = np.max(inp_arr), np.min(inp_arr)\n",
    "    if max_val - min_val != 0:\n",
    "        out_arr = (inp_arr - min_val) / (max_val - min_val)\n",
    "        return out_arr\n",
    "    if verbose:\n",
    "        print(\n",
    "            'Returning original array due to encountering zero division during min-max scaling ...'\n",
    "        )\n",
    "    return inp_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_err_idx = np.argmax(derivative_error_list)\n",
    "print(max_err_idx)\n",
    "print(x_val_list[max_err_idx], derivative_error_list[max_err_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    sigmoid(\n",
    "        -11.998887618734113, \n",
    "        derivative=False\n",
    "    ),\n",
    "    sigmoid(\n",
    "        sigmoid(\n",
    "            -11.998887618734113\n",
    "        ), \n",
    "        derivative=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    y_derivative_list[max_err_idx], \n",
    "    y_auto_derivative_list[max_err_idx],\n",
    "    y_derivative_list[max_err_idx] - y_auto_derivative_list[max_err_idx]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Error plot using raw error values**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_max = max(\n",
    "    [\n",
    "        abs(min_err), \n",
    "        abs(max_err)\n",
    "    ]\n",
    ")\n",
    "\n",
    "axis = [\n",
    "    xmin, xmax,\n",
    "    -axis_max * 1.25, axis_max * 1.25\n",
    "]\n",
    "\n",
    "plt.axvline(\n",
    "    x=0, \n",
    "    color='C2', \n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "plt.axhline(\n",
    "    y=mean_abs_err, \n",
    "    color='C1', \n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "plt.axhline(\n",
    "    y=0, \n",
    "    color='C2', \n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "plt.axis(axis)\n",
    "\n",
    "plt.plot(\n",
    "    x_val_list_sorted, \n",
    "    derivative_error_list_sorted, \n",
    "    linewidth=2.0\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Min-Max normalized error scatter plot**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axhline(\n",
    "    y=(mean_abs_err - min_err)/ (max_err - min_err), \n",
    "    color='C2',\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    x_val_list,\n",
    "    min_max_scaler(np.array(derivative_error_list)),\n",
    "    s=None,\n",
    "    c=min_max_scaler(np.array(derivative_error_list)),\n",
    "    marker=None,\n",
    "    cmap=None,\n",
    "    norm=None,\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    alpha=0.9,\n",
    "    linewidths=None,\n",
    "    edgecolors=None,\n",
    "    colorizer=None,\n",
    "    plotnonfinite=False,\n",
    "    data=None\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "    f'Normalized error plot with min: {min_err:.2e} and max: {max_err:.2e} ...'\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgyoLCqPlIV5"
   },
   "source": [
    "## **Part 01e -- Training the simple XOR gate neural network**:\n",
    "\n",
    "- Note: There is no function that defines a neuron! In practice neuron is just an abstract concept to understand the probability function\n",
    "- Continuously feeding the data throught the neural network\n",
    "- Updating the weights of the network through backpropagation\n",
    "- During the training the model becomes better and better in predicting the output values\n",
    "- The layers are just matrix multiplication functions that apply the sigmoid function to the synapse matrix and the corresponding layer\n",
    "- Backpropagation portion of the training is the machine learning portion of this code\n",
    "- Backpropagation function reduces the prediction errors during each training step\n",
    "- Synapses and weights are synonymous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfNAijZ_lI4S"
   },
   "outputs": [],
   "source": [
    "training_steps = 500000\n",
    "update_freq = 10\n",
    "\n",
    "input_data, output_data = x, y\n",
    "\n",
    "bias_val_1 = 1e-2\n",
    "bias_val_2 = 0.2 # 1e-4 # 0.5 # 1 # 1e-3 # 10\n",
    "\n",
    "learning_rate = 0.1 # 10 # 1 # 1e-3 # 1000 #\n",
    "\n",
    "for t in range(training_steps):\n",
    "    # Creating the layers of the neural network:\n",
    "    layer_0 = input_data\n",
    "    layer_1 = sigmoid(np.dot(layer_0, synapse_0) + bias_val_1)\n",
    "    layer_2 = sigmoid(np.dot(layer_1, synapse_1) + bias_val_2)\n",
    "\n",
    "    # Backpropagation:\n",
    "    outputLoss_derivative = output_data - layer_2\n",
    "    loss_col.append(np.mean(np.abs(outputLoss_derivative)))\n",
    "    if ((t * update_freq) % training_steps == 0):\n",
    "        print(\n",
    "            f'Training step: {str(t)} ', \n",
    "            f'Prediction error: {str(np.mean(np.abs(outputLoss_derivative)))} ...',\n",
    "            end= ''\n",
    "        )\n",
    "        print('', end='\\r')\n",
    "\n",
    "    # Layer-wsie delta function:\n",
    "    layer_2_delta = (\n",
    "        learning_rate * outputLoss_derivative * sigmoid(layer_2, derivative=True)\n",
    "    )\n",
    "    layer_1_error = layer_2_delta.dot(synapse_1.T) # Matrix multiplication of the layer 2 delta with the transpose of the first synapse function.\n",
    "    layer_1_delta = (layer_1_error * learning_rate) * (sigmoid(layer_1, derivative=True))\n",
    "\n",
    "    # Updating synapses or weights:\n",
    "    synapse_1 += layer_1.T.dot(layer_2_delta)\n",
    "    synapse_0 += layer_0.T.dot(layer_1_delta)\n",
    "    del layer_0, layer_1\n",
    "\n",
    "print('Training completed ...')\n",
    "print(f'Predictions : \\n{str(layer_2)} ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hO1j95QAXKr"
   },
   "outputs": [],
   "source": [
    "layer_0 = input_data\n",
    "layer_1 = sigmoid(\n",
    "    np.dot(\n",
    "        layer_0, \n",
    "        synapse_0\n",
    "    ) + bias_val_1\n",
    ")\n",
    "layer_2 = sigmoid(\n",
    "    np.dot(\n",
    "        layer_1, \n",
    "        synapse_1\n",
    "    ) + bias_val_2\n",
    ")\n",
    "\n",
    "print(synapse_0)\n",
    "print('\\n')\n",
    "print(bias_val_1)\n",
    "print('\\n')\n",
    "print(layer_1)\n",
    "print('\\n')\n",
    "print(bias_val_2)\n",
    "print('\\n')\n",
    "print(synapse_1)\n",
    "print('\\n')\n",
    "print(layer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bN2AdzqVDx-9"
   },
   "outputs": [],
   "source": [
    "def xor_gate_predictor(x):\n",
    "    layer_0 = x\n",
    "    layer_1 = sigmoid(\n",
    "        np.dot(layer_0, synapse_0) + bias_val_1\n",
    "    )\n",
    "    layer_2 = sigmoid(\n",
    "        np.dot(layer_1, synapse_1) + bias_val_2\n",
    "    )\n",
    "\n",
    "    return layer_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hDvfa1v3EfOI"
   },
   "outputs": [],
   "source": [
    "def plot_xor_gate_decision_boundary(prediction_function, x, y, enable_scatter_plot=False):\n",
    "    # Setting minimum and maximum values for giving the plot function some padding\n",
    "    x_min, x_max = x[:, 0].min() - .5, \\\n",
    "                   x[:, 0].max() + .5\n",
    "\n",
    "    y_min, y_max = x[:, 1].min() - .5, \\\n",
    "                   x[:, 1].max() + .5\n",
    "\n",
    "    h = 0.01\n",
    "\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(x_min, x_max, h), \\\n",
    "        np.arange(y_min, y_max, h)\n",
    "    )\n",
    "\n",
    "    # Predict the function value for the whole grid\n",
    "    Z = prediction_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plotting the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.colormaps.get_cmap('Spectral'))\n",
    "    if enable_scatter_plot:\n",
    "        plt.scatter(\n",
    "            x[:, 0], \n",
    "            x[:, 1], \n",
    "            c=y, \n",
    "            s=42, \n",
    "            cmap=plt.colormaps.get_cmap('Accent')\n",
    "        )\n",
    "    \n",
    "        for i, (i_x, i_y) in enumerate(x):\n",
    "            i_z = y[i]\n",
    "            p_x, p_y = i_x - 0.25, i_y - 0.2\n",
    "    \n",
    "            plt.text(\n",
    "                p_x, \n",
    "                p_y,\n",
    "                f'({i_x}, {i_y} ► {i_z})', \n",
    "                fontsize=18\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ye8asW9fPnYZ"
   },
   "outputs": [],
   "source": [
    "X, Y = sklearn.datasets.make_moons(6, noise=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVydJ6tpO0ba"
   },
   "outputs": [],
   "source": [
    "linear_classifier = sklearn.linear_model.LogisticRegression()\n",
    "linear_classifier.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPpk9GkJOIOf"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(\n",
    "    figsize=(6, 6), \n",
    "    layout='constrained'\n",
    ")\n",
    "\n",
    "spec = fig.add_gridspec(\n",
    "    ncols=1, \n",
    "    nrows=1\n",
    ")\n",
    "\n",
    "fig.add_subplot(\n",
    "    spec[0, 0]\n",
    ")\n",
    "\n",
    "plot_xor_gate_decision_boundary(\n",
    "    lambda x: linear_classifier.predict(x), \n",
    "    x, y\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "    'Logistic Regression'\n",
    ")\n",
    "\n",
    "plt.close()\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "del fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2C3Zmc6Db_m"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(\n",
    "    figsize=(6, 6), \n",
    "    layout='constrained'\n",
    ")\n",
    "\n",
    "spec = fig.add_gridspec(\n",
    "    ncols=1, \n",
    "    nrows=1\n",
    ")\n",
    "\n",
    "fig.add_subplot(\n",
    "    spec[0, 0]\n",
    ")\n",
    "\n",
    "plot_xor_gate_decision_boundary(\n",
    "    lambda x: xor_gate_predictor(x), \n",
    "    x, y, enable_scatter_plot=True\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "    'XOR gate neural network model'\n",
    ")\n",
    "\n",
    "plt.close()\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "del fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gcL_Dd-Gz8zJ"
   },
   "outputs": [],
   "source": [
    "delete_model = True\n",
    "\n",
    "if delete_model:\n",
    "  try:\n",
    "    del loss_col\n",
    "  except:\n",
    "    pass\n",
    "  try:\n",
    "    del input_data\n",
    "  except:\n",
    "    pass\n",
    "  try:\n",
    "    del output_data\n",
    "  except:\n",
    "    pass\n",
    "  try:\n",
    "    del x\n",
    "  except:\n",
    "    pass\n",
    "  try:\n",
    "    del y\n",
    "  except:\n",
    "    pass\n",
    "  try:\n",
    "    del layer_2\n",
    "  except:\n",
    "    pass\n",
    "  try:\n",
    "    del output_data\n",
    "  except:\n",
    "    pass\n",
    "  try:\n",
    "    del synapse_0\n",
    "  except:\n",
    "    pass\n",
    "  try:\n",
    "    del synapse_1\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3m8gTk_w6gg"
   },
   "source": [
    "## **Part 01f -- Neural network based XOR gate using rectified linear units activation function**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNYTeupHcvGc"
   },
   "source": [
    "### Create input and output data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZdJ2UfPx5I2"
   },
   "outputs": [],
   "source": [
    "x = np.array(\n",
    "    [\n",
    "        [0, 0], [1, 1], [0, 1], [1, 0]\n",
    "    ]\n",
    ")\n",
    "\n",
    "y = np.array(\n",
    "    [\n",
    "        [0], [0], [1], [1]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcIskqgVaye1"
   },
   "source": [
    "### N is batch size(sample size); D_in is input dimension; H is hidden dimension; D_out is output dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogzqBRKdx0SQ"
   },
   "outputs": [],
   "source": [
    "D_in, H, D_out = x.shape[1], 30, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZFRf6T1alYi"
   },
   "source": [
    "### Randomly initialize weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67qTKA1ox8uJ"
   },
   "outputs": [],
   "source": [
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ndP9RYonx_fd"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "update_freq = 10\n",
    "training_steps = 2000\n",
    "\n",
    "loss_col = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TsMtM098yUl7"
   },
   "source": [
    "### ReLu as the activation function and [squared error](https://datascience.stackexchange.com/questions/10188/why-do-cost-functions-use-the-square-error) as the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DH5MZfEKw1OL"
   },
   "outputs": [],
   "source": [
    "for t in range(training_steps):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)  # using ReLU as activation function\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum() # squared error as the loss function\n",
    "    loss_col.append(loss)\n",
    "    if ((t * update_freq) % training_steps ==0):\n",
    "        print(f'Training step: {str(t)}', f' Loss function: {str(loss)} ...', end= '')\n",
    "        print('', end='\\r')  \n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2 * (y_pred - y) # the last layer's error\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T) # the second layer's error\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0  # the derivate of ReLU\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2\n",
    "\n",
    "print ('Training completed ...')\n",
    "print (f'Predictions: \\n{str(y_pred)} ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnkaksqMz4SU"
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FLRaVghZqmke"
   },
   "outputs": [],
   "source": [
    "delete_model = True\n",
    "\n",
    "if delete_model:\n",
    "  try:\n",
    "    del loss_col\n",
    "  except:\n",
    "    pass\n",
    "  try:\n",
    "    del input_data\n",
    "  except:\n",
    "    pass\n",
    "  try:\n",
    "    del output_data\n",
    "  except:\n",
    "    pass\n",
    "  try:\n",
    "    del x\n",
    "  except:\n",
    "    pass\n",
    "  try:\n",
    "    del y\n",
    "  except:\n",
    "    pass\n",
    "  try:\n",
    "    del output_data\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 01g -- Training using stochastic auto gradients**:\n",
    "\n",
    "This section describes the training of a neural network through updation of each of its layers using automated gradient computation by performing stochastic auto-differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stochastic automatic gradient (SAG) using multi order automatic derivatives (MOAD) for a neural network**:\n",
    "Function for computing the layer gradient by using the **multi order automatic derivatives (MOAD)**. \n",
    "\n",
    "In the traditional neural networks, the backpropagation is computed using just the partial first derivative. \n",
    "\n",
    "In this novel **MOAD computation** based updation strategy for the neural network layer, by using a parameterized function; a series of multi order partial derivatives are computed and are used additively to calculate the layer gradient.\n",
    "\n",
    "${{\\text{layer gradient}}_{MOAD}={\\text{layer error} {\\times} {\\displaystyle{\\sum_{i=1}^{n}}} {\\frac{{\\partial}^n}{{\\partial {\\textbf{z}}}^n} {f({\\textbf{z}})}}}}$\n",
    "\n",
    "$\\text{Where, }{{\\textbf{z}} \\text{ is the input of the neuron, }}{f({\\textbf{z}})}\\text{ is the activation function; corresponding to the output of the neuron, }{n>0},{\\text{ }}{n{\\in{\\textbf{Z}}}.}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_gradient(\n",
    "        layer_error, \n",
    "        layer_derivative\n",
    "    ):\n",
    "    return layer_error * layer_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stochastic_auto_gradient(\n",
    "        layer,\n",
    "        layer_error,\n",
    "        activation_fn,\n",
    "        epsilon,\n",
    "        epsilon_regularization_factor=100,\n",
    "        order=3,\n",
    "        order_weights=None,\n",
    "        num_auto_diff_steps=100,\n",
    "        enable_backprop_activation_nesting=False\n",
    "    ):\n",
    "    '''\n",
    "        Stochastic automatic gradients (SAG) using multi order automatic derivatives (MOAD).\n",
    "\n",
    "        Constraints for the current implementation: n > 0 and n ∈ Z, where n is specified using the argument: order.\n",
    "    '''\n",
    "    assert isinstance(order, int) and order > 0, f'An integer greater than 0 is the expected value for the order to partial differentiate, instead received: {order} ...'\n",
    "    if order_weights is not None:\n",
    "        assert isinstance(order_weights, list) and len(order_weights) == order, f'A list item with size: {order} was expected for the order_weights argument, instead received: {order_weights} ...'\n",
    "    else:\n",
    "        order_weights = [1 for i in range(order)]\n",
    "\n",
    "    ow_divisor = sum(\n",
    "        [\n",
    "            abs(order_weight) for order_weight in order_weights\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if ow_divisor == 0: \n",
    "        order_weights = [1 for i in range(order)]\n",
    "    else:\n",
    "        order_weights = [(order_weight / ow_divisor) * order for order_weight in order_weights]\n",
    "\n",
    "    stoch_auto_diff_list = []\n",
    "    for i in range(order):\n",
    "        layer = compute_stochastic_auto_differentiation(\n",
    "            layer,\n",
    "            activation_fn,\n",
    "            epsilon,\n",
    "            epsilon_regularization_factor,\n",
    "            num_auto_diff_steps,\n",
    "            order=order,\n",
    "            enable_backprop_activation_nesting=enable_backprop_activation_nesting,\n",
    "            enable_normalization=True\n",
    "        )\n",
    "\n",
    "        order_weight = order_weights[i]\n",
    "\n",
    "        stoch_auto_diff_list.append(\n",
    "            layer * order_weight\n",
    "        )\n",
    "\n",
    "    del layer\n",
    "\n",
    "    for i,  stoch_auto_diff in enumerate(stoch_auto_diff_list):\n",
    "        if i == 0:\n",
    "            layer_derivative  = stoch_auto_diff\n",
    "        else:\n",
    "            layer_derivative += stoch_auto_diff\n",
    "\n",
    "        del stoch_auto_diff\n",
    "\n",
    "    del stoch_auto_diff_list\n",
    "\n",
    "    layer_grad = get_layer_gradient(\n",
    "        layer_error, \n",
    "        layer_derivative\n",
    "    )\n",
    "\n",
    "    del layer_error, layer_derivative\n",
    "\n",
    "    return layer_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to create synapses, perform forward pass and back-propagation to create a neural network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synapse(\n",
    "        hidden_units_list, \n",
    "        bias_val_list\n",
    "    ):\n",
    "    synapse_list = []\n",
    "    for i, unit_size in enumerate(hidden_units_list):\n",
    "        if i + 1 < len(hidden_units_list):\n",
    "            synapse_list.append(\n",
    "                2 * np.random.random(\n",
    "                    (\n",
    "                        unit_size, \n",
    "                        hidden_units_list[i + 1]\n",
    "                    )\n",
    "                ) - bias_val_list[i]\n",
    "            )\n",
    "\n",
    "    return synapse_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical considerations for implementing the learning algorithm using auto differentiation in neural networks:\n",
    "\n",
    "Theoretical learning models to describe the neural networks rely on the explanation based on computing the partial derivatives for the corresponding input at each neuron.\n",
    "\n",
    "In practice however, computing the partial derivative of the output of the activation function, corresponding to the output of the neuron; is the most effective learning strategy, when a loss function derivative with respect to the input is either unavailable or cannot be computed. \n",
    "\n",
    "This can be achieved by setting the ```enable_strict_derivative_order``` argument in the ```compute_forward_pass``` function as ```False```.\n",
    "\n",
    "This more effective learning strategy of evaluating the output of a neuron that foregoes the computation of loss function derivative, that is also computationally simpler; can be mathematically described as follows:\n",
    "\n",
    "${{\\text{layer gradient}}_{MOAD}={\\text{layer error} {\\times} {\\displaystyle{\\sum_{i=1}^{n}}} {\\frac{{\\partial}^n}{{\\partial{(f({\\textbf{z}}))}}^n} {f{^{2}}({\\textbf{z}})}}}}$\n",
    "\n",
    "${\\text{Where, }{{\\textbf{z}} \\text{ is the input of the neuron, }}{{f}({\\textbf{z}})}\\text{ is the activation function; corresponding to the output of the neuron, }{n>0},{\\text{ }}{n{\\in{{\\textbf{Z}}}}}.}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_layer_output(\n",
    "        layer_input,\n",
    "        layer_synapse,\n",
    "        layer_bias,\n",
    "        activation_fn,\n",
    "        order=1,\n",
    "        order_weights=None,\n",
    "        epsilon=1e-8,\n",
    "        nan=12,\n",
    "        neginf=-1/12,\n",
    "        posinf=1/12,\n",
    "        enable_normalization=False,\n",
    "        enable_nan_to_num=True\n",
    "    ):\n",
    "    if order_weights is not None:\n",
    "        assert len(order_weights) == order, print(len(order_weights), order)\n",
    "\n",
    "    assert (isinstance(epsilon, float) or isinstance(epsilon, int)) and epsilon > 0, f'Expected epsilon to be a positive float value, instead received: {epsilon} ...'\n",
    "\n",
    "    z = copy.deepcopy(layer_input)\n",
    "\n",
    "    del layer_input\n",
    "\n",
    "    if layer_synapse is not None:\n",
    "        z = np.dot(\n",
    "            z, \n",
    "            layer_synapse\n",
    "        )\n",
    "\n",
    "        del layer_synapse\n",
    "\n",
    "    if layer_bias is not None:\n",
    "        z += layer_bias\n",
    "\n",
    "        del layer_bias\n",
    "\n",
    "    if activation_fn is not None:\n",
    "        act_layer = copy.deepcopy(z)\n",
    "        act_layer = activation_fn(act_layer)\n",
    "\n",
    "        if enable_nan_to_num:\n",
    "            if nan is None:\n",
    "                nan = 1 / epsilon\n",
    "            if posinf is None:\n",
    "                posinf = 1 / epsilon\n",
    "            if neginf is None:\n",
    "                neginf = -1/epsilon\n",
    "            act_layer = np.nan_to_num(\n",
    "                act_layer, \n",
    "                nan=nan, \n",
    "                posinf=posinf, \n",
    "                neginf=neginf\n",
    "            )\n",
    "\n",
    "        if order_weights is not None:\n",
    "            act_layer *=  order_weights[0]\n",
    "\n",
    "    if order is not None and activation_fn is not None:\n",
    "        for j in range(1, order):\n",
    "            act_layer_inp = copy.deepcopy(z)\n",
    "            act_layer_out = activation_fn(\n",
    "                random.SystemRandom().uniform(\n",
    "                    1 - (abs(epsilon) / 4), \n",
    "                    1 + (abs(epsilon) / 4)\n",
    "                ) * act_layer_inp\n",
    "            )\n",
    "\n",
    "            if enable_nan_to_num:\n",
    "                act_layer_out = np.nan_to_num(\n",
    "                    act_layer_out, \n",
    "                    nan=nan, \n",
    "                    posinf=posinf, \n",
    "                    neginf=neginf\n",
    "                )\n",
    "\n",
    "            if order_weights is not None:\n",
    "                act_layer_out *=  order_weights[j]\n",
    "\n",
    "            act_layer += act_layer_out\n",
    "\n",
    "            del act_layer_inp, act_layer_out\n",
    "\n",
    "        if enable_normalization: \n",
    "            act_layer /= order\n",
    "\n",
    "    return z, act_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_forward_pass(\n",
    "        input_data,\n",
    "        synapse_list,\n",
    "        bias_val_list,\n",
    "        activation_fn_list,\n",
    "        epsilon=1e-24,\n",
    "        order=3,\n",
    "        order_weights_list=None,\n",
    "        min_output_layer_value=0.0,\n",
    "        max_output_layer_value=1.0,\n",
    "        enable_training=True,\n",
    "        enable_strict_derivative_order=True,\n",
    "        enable_normalization=False,\n",
    "        enable_min_max_scaled_output_layer=True\n",
    "    ):\n",
    "    if order is not None:\n",
    "        assert isinstance(order, int) and order > 0, f'Expected the order value to be a positive integer, instead received: {order} ...'\n",
    "\n",
    "    if order_weights_list is not None:\n",
    "        assert isinstance(order_weights_list, list), f'Expected a list to be supplied for order_weights_list argument, instead received: {order_weights_list}'\n",
    "        if len(order_weights_list) > 0 :\n",
    "            assert len(order_weights_list) == len(synapse_list), f'Mismatch in the number of list items in order_weights_list: {len(order_weights_list)} and layer_list: {len(layer_list)} ...'\n",
    "\n",
    "    assert isinstance(activation_fn_list, list), f'Expected the input passed using the argument activation_fn_list to be a list, instead received: {type(activation_fn_list)} ...'\n",
    "    assert isinstance(synapse_list, list), f'Expected the input passed using the argument synapse_list to be a list, instead received: {type(synapse_list)} ...'\n",
    "    assert len(activation_fn_list) == len(synapse_list), f'Expected the length of the activation function list: {len(activation_fn_list)} to be of the same length as the number of synapses: {len(synapse_list) + 1} in the neural network ...'\n",
    "\n",
    "    if enable_min_max_scaled_output_layer and min_output_layer_value is not None and max_output_layer_value is not None:\n",
    "        assert isinstance(min_output_layer_value, int) or isinstance(min_output_layer_value, float), f'Expected the value passed using the argument: min_output_layer_value to be an integer or a float, instead reaceived: {type(min_output_layer_value)} ...'\n",
    "        assert isinstance(max_output_layer_value, int) or isinstance(max_output_layer_value, float), f'Expected the value passed using the argument: max_output_layer_value to be an integer or a float, instead reaceived: {type(max_output_layer_value)} ...'\n",
    "        assert min_output_layer_value != max_output_layer_value, f'Expected min_output_layer_value: {min_output_layer_value} to be less than max_output_layer_value: {min_output_layer_value} ...'\n",
    "        if max_output_layer_value < min_output_layer_value:\n",
    "            tmp_min_output_layer_value = copy.deepcopy(max_output_layer_value)\n",
    "            tmp_max_output_layer_value = copy.deepcopy(min_output_layer_value)\n",
    "            del min_output_layer_value, max_output_layer_value\n",
    "            min_output_layer_value = tmp_min_output_layer_value\n",
    "            max_output_layer_value = tmp_max_output_layer_value\n",
    "            \n",
    "    \n",
    "    if enable_strict_derivative_order:\n",
    "        input_layer_list, layer_list = [input_data], [input_data]\n",
    "    else:\n",
    "        layer_list = [input_data]\n",
    "\n",
    "    for i, synapse in enumerate(synapse_list):\n",
    "        order_weights = None\n",
    "        if order_weights_list is not None:\n",
    "            order_weights = order_weights_list[i]\n",
    "\n",
    "        input_layer, act_layer = get_activation_layer_output(\n",
    "\n",
    "            layer_list[i],\n",
    "            copy.deepcopy(synapse),\n",
    "            bias_val_list[i],\n",
    "            activation_fn_list[i],\n",
    "            order=order,\n",
    "            order_weights=order_weights,\n",
    "            epsilon=epsilon,\n",
    "            enable_normalization=enable_normalization\n",
    "\n",
    "        )\n",
    "\n",
    "        del i, synapse\n",
    "\n",
    "        if enable_strict_derivative_order:\n",
    "            if enable_training:\n",
    "                input_layer_list.append(input_layer)\n",
    "\n",
    "        del input_layer\n",
    "\n",
    "        layer_list.append(act_layer)\n",
    "\n",
    "        del act_layer\n",
    "\n",
    "    del input_data, synapse_list\n",
    "\n",
    "    if enable_min_max_scaled_output_layer:\n",
    "        layer_list[-1] = min_max_scaler(layer_list[-1])\n",
    "        if min_output_layer_value is not None and max_output_layer_value is not None:\n",
    "            min_max_delta = max_output_layer_value - min_output_layer_value\n",
    "            layer_list[-1] = (layer_list[-1] * min_max_delta) + min_output_layer_value\n",
    "\n",
    "    output_layer = copy.deepcopy(layer_list[-1])\n",
    "\n",
    "    preds = np.asarray(output_layer)\n",
    "\n",
    "    if enable_strict_derivative_order:\n",
    "        if enable_training:\n",
    "            return preds, output_layer, input_layer_list, layer_list, bias_val_list\n",
    "        try:\n",
    "            del input_layer_list\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        if enable_training:\n",
    "            return preds, output_layer, None, layer_list, bias_val_list\n",
    "\n",
    "    try:\n",
    "        del layer_list, bias_val_list\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return output_layer, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_error(\n",
    "        layer_delta:np.ndarray,\n",
    "        synapse:np.ndarray\n",
    "    )->np.ndarray:\n",
    "    layer_error = layer_delta.dot(synapse.T)\n",
    "\n",
    "    return layer_error\n",
    "\n",
    "def get_synapse_update(\n",
    "        input_layer:np.ndarray,\n",
    "        layer_delta:np.ndarray\n",
    "    )->np.ndarray:\n",
    "    '''\n",
    "        Computes the values to update the synapse.\n",
    "            Arguments:\n",
    "                input_layer : Input layer of the synapse \n",
    "                layer_delta : Gradient corresponding to the output of the synapse\n",
    "        Returns a Numpy array\n",
    "    '''\n",
    "    synapse_update = input_layer.T.dot(layer_delta)\n",
    "\n",
    "    return synapse_update\n",
    "\n",
    "def get_bias_update(\n",
    "        layer_delta:np.ndarray\n",
    "    )->np.ndarray:\n",
    "    bias_update = np.sum(\n",
    "        layer_delta, \n",
    "        axis=0, \n",
    "        keepdims=True\n",
    "    )\n",
    "\n",
    "    return bias_update\n",
    "\n",
    "def get_stochastic_layer_update(\n",
    "        layer,\n",
    "        input_layer,\n",
    "        synapse_input_layer,\n",
    "        synapse,\n",
    "        layer_error,\n",
    "        learning_rate,\n",
    "        activation_fn,\n",
    "        epsilon,\n",
    "        epsilon_regularization_factor,\n",
    "        order,\n",
    "        order_weights,\n",
    "        num_auto_diff_steps,\n",
    "        enable_backprop_activation_nesting,\n",
    "        enable_derivative_normalization\n",
    "    ):\n",
    "\n",
    "    rnd_lr = random.SystemRandom().uniform(\n",
    "        0.99975 * learning_rate, \n",
    "        1.00025 * learning_rate\n",
    "    )\n",
    "\n",
    "    layer_delta = get_stochastic_auto_gradient(\n",
    "        input_layer if input_layer is not None else layer, \n",
    "        layer_error * rnd_lr, \n",
    "        activation_fn, \n",
    "        epsilon,\n",
    "        epsilon_regularization_factor,\n",
    "        order,\n",
    "        order_weights,\n",
    "        num_auto_diff_steps,\n",
    "        enable_backprop_activation_nesting\n",
    "    )\n",
    "\n",
    "    layer_error = get_layer_error(\n",
    "        layer_delta,\n",
    "        synapse\n",
    "    )\n",
    "\n",
    "    synapse_update = get_synapse_update(\n",
    "        synapse_input_layer,\n",
    "        layer_delta\n",
    "    ) * rnd_lr\n",
    "\n",
    "    bias_update = get_bias_update(\n",
    "        layer_delta\n",
    "    ) * rnd_lr\n",
    "\n",
    "    if order_weights is not None and len(order_weights) == order:\n",
    "        order_weights = [\n",
    "            random.SystemRandom().uniform(\n",
    "               - min([1, order_weight]),\n",
    "                 max([1, order_weight])\n",
    "            ) for order_weight in order_weights\n",
    "        ]\n",
    "    else:\n",
    "        order_weights = [\n",
    "            1 for i in range(order)\n",
    "        ]\n",
    "\n",
    "    return synapse_update, \\\n",
    "           bias_update,    \\\n",
    "           layer_error,    \\\n",
    "           order_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_back_propagation(\n",
    "        layer_error,\n",
    "        layer_loss,\n",
    "        layer_list,\n",
    "        input_layer_list,\n",
    "        synapse_list,\n",
    "        bias_val_list,\n",
    "        learning_rate,\n",
    "        epsilon,\n",
    "        epsilon_regularization_factor,\n",
    "        activation_fn_list,\n",
    "        order,\n",
    "        order_weights_list,\n",
    "        num_auto_diff_steps=100,\n",
    "        enable_backprop_activation_nesting=False,\n",
    "        enable_probabilistic_moad=True,\n",
    "        enable_derivative_normalization=False,\n",
    "        verbose=False\n",
    "    ):\n",
    "\n",
    "    if order_weights_list is not None:\n",
    "        assert isinstance(order_weights_list, list), f'Expected a list to be supplied for order_weights_list argument, instead received: {order_weights_list}'\n",
    "        if len(order_weights_list) > 0 :\n",
    "            assert len(order_weights_list) == len(synapse_list), f'Mismatch in the number of list items in order_weights_list: {len(order_weights_list)} and layer_list: {len(layer_list)} ...'\n",
    "        else:\n",
    "            order_weights_list = []\n",
    "    else:\n",
    "        order_weights_list = []\n",
    "\n",
    "    assert isinstance(synapse_list, list), f'Expected the input passed using synapse_list argument to be a list, instead received: {type(synapse_list)} ...'\n",
    "    assert isinstance(bias_val_list, list), f'Expected the input passed using bias_val_list argument to be a list, instead received: {type(bias_val_list)} ...'\n",
    "    assert len(bias_val_list) == len(synapse_list), f'Expected the length of the lists passed using synapse_list and bias_val_list to be the same, instead received synapse_list of length: {len(synapse_list)} bias_val_list of length: {len(bias_val_list)} ...'\n",
    "    assert isinstance(learning_rate, float) and learning_rate > 0, f'Expected learning rate to be a positive float value, instead received: {learning_rate} ...'\n",
    "\n",
    "    bprop_loss = np.sum(abs(layer_loss))\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Prediction error during training: {str(bprop_loss)} ...', end='')\n",
    "        print('', end='\\r')\n",
    "\n",
    "    synapse_list = list(reversed(synapse_list))\n",
    "    layer_list = list(reversed(layer_list))\n",
    "\n",
    "    if input_layer_list is not None and len(input_layer_list) > 1:\n",
    "        input_layer_list = list(reversed(input_layer_list))\n",
    "\n",
    "    bias_val_list = list(reversed(bias_val_list))\n",
    "    activation_fn_list = list(reversed(activation_fn_list))\n",
    "\n",
    "    if enable_probabilistic_moad:\n",
    "        new_order_weights_list = []\n",
    "\n",
    "    for i, layer in enumerate(layer_list):\n",
    "        if i + 1 < len(layer_list):\n",
    "            activation_fn = activation_fn_list[i]\n",
    "\n",
    "            synapse = copy.deepcopy(synapse_list[i])\n",
    "            synapse_input_layer = copy.deepcopy(layer_list[i + 1])\n",
    "\n",
    "            input_layer = None\n",
    "            if input_layer_list is not None:\n",
    "                input_layer = copy.deepcopy(input_layer_list[i])\n",
    "\n",
    "            if len(order_weights_list) > 0:\n",
    "                order_weights = order_weights_list[i]\n",
    "                if len(order_weights) != order:\n",
    "                    order_weights = None\n",
    "            else:\n",
    "                order_weights = None\n",
    "\n",
    "            synapse_update, \\\n",
    "            bias_update,    \\\n",
    "            layer_error,    \\\n",
    "            new_order_weights = get_stochastic_layer_update(\n",
    "\n",
    "                layer,\n",
    "                input_layer,\n",
    "                synapse_input_layer,\n",
    "                synapse,\n",
    "                layer_error,\n",
    "                learning_rate,\n",
    "                activation_fn, \n",
    "                epsilon,\n",
    "                epsilon_regularization_factor,\n",
    "                order,\n",
    "                order_weights,\n",
    "                num_auto_diff_steps,\n",
    "                enable_backprop_activation_nesting,\n",
    "                enable_derivative_normalization\n",
    "\n",
    "            )\n",
    "\n",
    "            synapse_list[i] += copy.deepcopy(synapse_update)\n",
    "            del synapse_update\n",
    "            if isinstance(bias_val_list[i], float) or isinstance(bias_val_list[i], int):\n",
    "                bias_val_list[i] = bias_update + (np.ones_like(bias_update) * bias_val_list[i])\n",
    "            elif isinstance(bias_val_list[i], np.ndarray):\n",
    "                bias_val_list[i] += bias_update\n",
    "\n",
    "            if enable_probabilistic_moad:\n",
    "                new_order_weights_list.append(\n",
    "\n",
    "                    new_order_weights\n",
    "\n",
    "                )\n",
    "\n",
    "    del layer_error\n",
    "\n",
    "    if enable_probabilistic_moad:\n",
    "        if len(order_weights_list) > 0:\n",
    "            order_weights_list = random.SystemRandom().choice(\n",
    "\n",
    "                [\n",
    "\n",
    "                    new_order_weights_list, \n",
    "                    order_weights_list\n",
    "\n",
    "                ]\n",
    "\n",
    "            )\n",
    "        else:\n",
    "            order_weights_list = new_order_weights_list\n",
    "\n",
    "        del new_order_weights_list\n",
    "\n",
    "    synapse_list = list(reversed(synapse_list))\n",
    "    bias_val_list = list(reversed(bias_val_list))\n",
    "    order_weights_list = list(reversed(order_weights_list))\n",
    "\n",
    "    return synapse_list,      \\\n",
    "           bias_val_list,     \\\n",
    "           bprop_loss,        \\\n",
    "           order_weights_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic genetic algorithm based function for model survival:\n",
    "\n",
    "* Model survival based on the improvement in loss function\n",
    "* Lesser optimal models can survive for a period determined by ```loss_plateau_patience```\n",
    "* Stochastic adjustment of the learning rate based on the rate of model evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_survival(\n",
    "        iter_step,\n",
    "        synapse_list,\n",
    "        curr_synapse_list,\n",
    "        best_synapse_list,\n",
    "        bias_val_list,\n",
    "        curr_bias_val_list,\n",
    "        best_bias_val_list,\n",
    "        bprop_loss,\n",
    "        best_bprop_loss,\n",
    "        learning_rate,\n",
    "        best_lr,\n",
    "        init_lr,\n",
    "        order_weights_list,\n",
    "        curr_order_weights_list,\n",
    "        best_order_weights_list,\n",
    "        num_no_loss_change_steps,\n",
    "        loss_plateau_patience,\n",
    "        remanan_stochastic_expansion,\n",
    "        enable_backprop_activation_nesting,\n",
    "        best_enable_backprop_activation_nesting,\n",
    "        enable_moad_forward_pass,\n",
    "        best_enable_moad_forward_pass,\n",
    "        enable_strict_derivative_order,\n",
    "        best_enable_strict_derivative_order,\n",
    "        enable_derivative_normalization,\n",
    "        best_enable_derivative_normalization,\n",
    "        verbose=False\n",
    "    ):\n",
    "    if loss_plateau_patience is not None:\n",
    "        assert isinstance(loss_plateau_patience, int) and loss_plateau_patience > 0, f'Expected loss plateau patience value to be a non zero positive integer, instead received: {loss_plateau_patience} ...'\n",
    "    else:\n",
    "        if verbose and ((num_no_loss_change_steps is not None and num_no_loss_change_steps == 0) or num_no_loss_change_steps is None):\n",
    "            print(\n",
    "                'No model survival optimization will be performed since the loss plateau patience value is None, but a best synapse output will be generated ...', \n",
    "                end=''\n",
    "            )\n",
    "            print('', end='\\r')\n",
    "        elif verbose and ((num_no_loss_change_steps is not None and num_no_loss_change_steps == 1)):\n",
    "            print(\n",
    "                ' ' * 256,\n",
    "                end=''\n",
    "            )\n",
    "            print('', end='\\r')\n",
    "\n",
    "    if remanan_stochastic_expansion is not None:\n",
    "        assert (isinstance(remanan_stochastic_expansion, float) or isinstance(remanan_stochastic_expansion, int)) and 0 <= remanan_stochastic_expansion <= 1, f'Expected the value passed using the argument: remanan_stochastic_expansion to be a probability, instead reaceived: {remanan_stochastic_expansion} ...'\n",
    "    \n",
    "\n",
    "    if best_bprop_loss is None or best_bprop_loss > bprop_loss:\n",
    "        if best_bprop_loss:\n",
    "            best_lr = copy.deepcopy(learning_rate)\n",
    "\n",
    "            if verbose:\n",
    "                print(\n",
    "\n",
    "                    f'Best loss: {best_bprop_loss} at iteration: {iter_step + 1} ...',\n",
    "\n",
    "                    ' ' * 128, \n",
    "\n",
    "                    end=''\n",
    "\n",
    "                )\n",
    "\n",
    "                print('', end='\\r')\n",
    "\n",
    "            num_no_loss_change_steps = 0\n",
    "\n",
    "        best_bprop_loss = copy.deepcopy(bprop_loss)\n",
    "\n",
    "        synapse_list = curr_synapse_list        \n",
    "        bias_val_list = curr_bias_val_list\n",
    "        order_weights_list = curr_order_weights_list\n",
    "\n",
    "        best_enable_moad_forward_pass = enable_moad_forward_pass\n",
    "        best_enable_strict_derivative_order = enable_strict_derivative_order\n",
    "\n",
    "        best_synapse_list = copy.deepcopy(curr_synapse_list)\n",
    "        best_bias_val_list = copy.deepcopy(curr_bias_val_list)\n",
    "        best_order_weights_list = copy.deepcopy(curr_order_weights_list)\n",
    "\n",
    "        best_enable_derivative_normalization = copy.deepcopy(enable_derivative_normalization)\n",
    "        best_enable_backprop_activation_nesting = copy.deepcopy(enable_backprop_activation_nesting)\n",
    "    \n",
    "        del curr_synapse_list, curr_bias_val_list, curr_order_weights_list\n",
    "        \n",
    "    else:\n",
    "\n",
    "        if num_no_loss_change_steps is not None:\n",
    "            num_no_loss_change_steps += 1\n",
    "\n",
    "        if best_lr is not None:\n",
    "            learning_rate = random.SystemRandom().choice(\n",
    "                [\n",
    "                    random.SystemRandom().uniform(\n",
    "                        best_lr / 2,\n",
    "                        best_lr\n",
    "                    ), \n",
    "                    init_lr\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        if loss_plateau_patience is not None and num_no_loss_change_steps > loss_plateau_patience:\n",
    "            if best_synapse_list is not None:\n",
    "                synapse_list = random.SystemRandom().choice(\n",
    "                    [\n",
    "                        copy.deepcopy(best_synapse_list),\n",
    "                        curr_synapse_list,\n",
    "                        synapse_list\n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                synapse_list = curr_synapse_list\n",
    "\n",
    "            if best_bias_val_list is not None:\n",
    "                bias_val_list = random.SystemRandom().choice(\n",
    "                    [\n",
    "                        copy.deepcopy(best_bias_val_list), \n",
    "                        curr_bias_val_list,\n",
    "                        bias_val_list\n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                bias_val_list = curr_bias_val_list\n",
    "    \n",
    "            if best_order_weights_list is not None:\n",
    "                order_weights_list = random.SystemRandom().choice(\n",
    "                    [\n",
    "                        copy.deepcopy(best_order_weights_list), \n",
    "                        curr_order_weights_list,\n",
    "                        order_weights_list\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "                num_no_loss_change_steps = 0\n",
    "            else:\n",
    "                order_weights_list = curr_order_weights_list\n",
    "\n",
    "            del curr_synapse_list, curr_bias_val_list, curr_order_weights_list\n",
    "\n",
    "            if remanan_stochastic_expansion is not None:\n",
    "                if random.SystemRandom().random() < remanan_stochastic_expansion:\n",
    "                    enable_strict_derivative_order = random.SystemRandom().choice([False, True, best_enable_strict_derivative_order, enable_strict_derivative_order])\n",
    "                if random.SystemRandom().random() < remanan_stochastic_expansion:\n",
    "                    enable_backprop_activation_nesting = random.SystemRandom().choice([False, True, best_enable_backprop_activation_nesting, enable_backprop_activation_nesting])\n",
    "                if random.SystemRandom().random() < remanan_stochastic_expansion:\n",
    "                    enable_moad_forward_pass = random.SystemRandom().choice([False, True, best_enable_moad_forward_pass, enable_moad_forward_pass])\n",
    "                if random.SystemRandom().random() < remanan_stochastic_expansion:\n",
    "                    enable_derivative_normalization = random.SystemRandom().choice([False, True, best_enable_derivative_normalization, enable_derivative_normalization])\n",
    "\n",
    "    return synapse_list,                             \\\n",
    "           best_synapse_list,                        \\\n",
    "           bias_val_list,                            \\\n",
    "           best_bias_val_list,                       \\\n",
    "           best_bprop_loss,                          \\\n",
    "           learning_rate,                            \\\n",
    "           order_weights_list,                       \\\n",
    "           best_order_weights_list,                  \\\n",
    "           num_no_loss_change_steps,                 \\\n",
    "           enable_backprop_activation_nesting,       \\\n",
    "           best_enable_backprop_activation_nesting,  \\\n",
    "           enable_moad_forward_pass,                 \\\n",
    "           best_enable_moad_forward_pass,            \\\n",
    "           enable_strict_derivative_order,           \\\n",
    "           best_enable_strict_derivative_order,      \\\n",
    "           enable_derivative_normalization,          \\\n",
    "           best_enable_derivative_normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stochastically evaluated loss function (SELF):**\n",
    "\n",
    "Loss function used to compute the prediction errors is evaluated stochastically to determine its derivative and subsiquently its gradient.\n",
    "\n",
    "This gradient is then used in the training of the neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stochastically_evaluated_loss_function(\n",
    "        labels,\n",
    "        preds,\n",
    "        input_fn,\n",
    "        epsilon,\n",
    "        epsilon_regularization_factor=100,\n",
    "        num_eval_steps=30,\n",
    "        order=1,\n",
    "        enable_backprop_activation_nesting=False,\n",
    "        enable_normalization=True\n",
    "    ):\n",
    "    assert epsilon > 0 and epsilon < 1, f'Expected epsilon value to be a positive float between 0 and 1, instead received: {epsilon} ...'\n",
    "\n",
    "    def input_fn_for_list_input(\n",
    "\n",
    "            inputs_list,\n",
    "\n",
    "            labels=labels,\n",
    "\n",
    "            input_fn=input_fn\n",
    "    \n",
    "        ):\n",
    "\n",
    "        assert type(inputs_list) == list, f'Expected the auto differentiation input for the loss function to be a list, instead reaceived: {type(labels_list)} ...'\n",
    "        assert 1 <= len(inputs_list) <= 2, f'Expected the auto differentiation input for the loss function to be a list of length between 1 and 2, instead received a list of length: {len(inputs_list)} ...'\n",
    "        return input_fn(\n",
    "\n",
    "                    inputs_list[-1],\n",
    "\n",
    "                    #labels,\n",
    "                    random.SystemRandom().choice(\n",
    "\n",
    "                        [\n",
    "\n",
    "                            labels, \n",
    "\n",
    "                            inputs_list[0]\n",
    "\n",
    "                        ]\n",
    "\n",
    "                    ) if len(inputs_list) == 2 else labels\n",
    "\n",
    "               )\n",
    "\n",
    "    return compute_stochastically_evaluated_function_derivative(\n",
    "               [labels, preds],\n",
    "               input_fn=input_fn_for_list_input,\n",
    "               epsilon=epsilon,\n",
    "               epsilon_regularization_factor=epsilon_regularization_factor,\n",
    "               num_eval_steps=num_eval_steps,\n",
    "               order=order,\n",
    "               enable_stochastic_partial_derivative=True,\n",
    "               enable_backprop_activation_nesting=enable_backprop_activation_nesting,\n",
    "               enable_normalization=enable_normalization\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function and its stochastically evaluated derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_squared(x):\n",
    "    return np.pow(x, 2)\n",
    "\n",
    "def get_absolute_error(\n",
    "        preds,\n",
    "        labels,\n",
    "        enable_normalization=True\n",
    "    ):\n",
    "    if enable_normalization:\n",
    "        return np.abs(labels - preds) / len(labels)\n",
    "\n",
    "    return  np.abs(labels - preds)\n",
    "    \n",
    "def get_absolute_error_loss(\n",
    "        preds,\n",
    "        labels\n",
    "    ):\n",
    "    ae = get_absolute_error(\n",
    "        preds,\n",
    "        labels,\n",
    "        enable_normalization=True\n",
    "    )\n",
    "\n",
    "    return np.sum(ae), ae\n",
    "\n",
    "def get_squared_error(\n",
    "        preds,\n",
    "        labels\n",
    "    ):\n",
    "    return  np.abs(array_squared(labels) - array_squared(preds))\n",
    "    \n",
    "def get_squared_error_loss(\n",
    "        preds,\n",
    "        labels\n",
    "    ):\n",
    "    sqe = get_squared_error(preds, labels)  / len(labels)\n",
    "\n",
    "    return np.sum(sqe), sqe\n",
    "\n",
    "def get_mean_squared_error(\n",
    "        preds,\n",
    "        labels,\n",
    "        enable_normalization=True\n",
    "    ):\n",
    "    if enable_normalization:\n",
    "        return array_squared(labels - preds) / len(labels)\n",
    "\n",
    "    return array_squared(labels - preds)\n",
    "\n",
    "def get_mean_squared_error_loss(\n",
    "        preds,\n",
    "        labels\n",
    "    ):\n",
    "    mse = get_mean_squared_error(\n",
    "        preds,\n",
    "        labels,\n",
    "        enable_normalization=True\n",
    "    )\n",
    "\n",
    "    return np.sum(mse), mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_function_derivative(\n",
    "        output_labels,\n",
    "        output_preds,\n",
    "        loss_fn,\n",
    "        epsilon,\n",
    "        epsilon_regularization_factor,\n",
    "        order,\n",
    "        num_auto_diff_steps,\n",
    "        enable_backprop_activation_nesting,\n",
    "        enable_normalization\n",
    "    ):\n",
    "    output_derivative = compute_stochastically_evaluated_loss_function(\n",
    "        output_labels,\n",
    "        output_preds,\n",
    "        loss_fn,\n",
    "        epsilon,\n",
    "        epsilon_regularization_factor,\n",
    "        num_auto_diff_steps,\n",
    "        order,\n",
    "        enable_backprop_activation_nesting,\n",
    "        enable_normalization\n",
    "    )\n",
    "\n",
    "    return output_derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to train the MOAD MLP and create predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moad_mlp_train(\n",
    "        X,\n",
    "        Y,\n",
    "        synapse_list,\n",
    "        bias_val_list,\n",
    "        activation_fn_list,\n",
    "        learning_rate,\n",
    "        epsilon,\n",
    "        epsilon_regularization_factor,\n",
    "        num_epochs,\n",
    "        order,\n",
    "        order_weights_list,\n",
    "        loss_plateau_patience,\n",
    "        num_auto_diff_steps=100,\n",
    "        remanan_stochastic_expansion=0.51,\n",
    "        return_best_synapse=True,\n",
    "        enable_backprop_activation_nesting=False,\n",
    "        enable_moad_forward_pass=False,\n",
    "        enable_strict_derivative_order=False,\n",
    "        enable_derivative_normalization=False,\n",
    "        enable_min_max_scaled_output_layer=True,\n",
    "        enable_model_selection=True,\n",
    "        verbose=True\n",
    "    ):\n",
    "    assert num_epochs > 0 and isinstance(num_epochs, int), f'Expected the number of epochs to train value passed using the num_epochs argument to be a non-zero positive integer, instead received: {num_epochs} ...'\n",
    "    \n",
    "    best_bprop_loss = None\n",
    "    best_lr = None\n",
    "    best_synapse_list = None\n",
    "    best_bias_val_list = None\n",
    "    best_order_weights_list = None\n",
    "\n",
    "    best_enable_backprop_activation_nesting = None\n",
    "    best_enable_moad_forward_pass = None\n",
    "    best_enable_strict_derivative_order = None\n",
    "    best_enable_derivative_normalization = None\n",
    "\n",
    "    num_no_loss_change_steps = 0\n",
    "\n",
    "    init_lr = copy.deepcopy(learning_rate)\n",
    "\n",
    "    if verbose:\n",
    "        print('\\n', 'Training start ... \\n')\n",
    "\n",
    "    bprop_loss_col = []\n",
    "\n",
    "    batch_synapse_list = copy.deepcopy(synapse_list)\n",
    "    batch_bias_val_list = copy.deepcopy(bias_val_list)\n",
    "    batch_order_weights_list = copy.deepcopy(order_weights_list)\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        batch_loss_list = []\n",
    "        batch_size = 16\n",
    "        num_steps = batch_size\n",
    "        if (len(X) / batch_size) > batch_size:\n",
    "            num_steps = math.ceil(len(X) / batch_size)\n",
    "        for n in range(num_steps):\n",
    "            batch_idx_list = [i for i in range(len(X))]\n",
    "            if len(X) < batch_size:\n",
    "                batch_idx_list *= batch_size\n",
    "                \n",
    "            random.SystemRandom().shuffle(batch_idx_list)\n",
    "\n",
    "            batch_idx_list = random.SystemRandom().sample(\n",
    "                batch_idx_list, \n",
    "                k=batch_size\n",
    "            )\n",
    "            input_data = np.array([X[i] for i in batch_idx_list])\n",
    "            output_data = np.array([Y[i] for i in batch_idx_list])\n",
    "            learning_rate = random.SystemRandom().uniform(0.9975, 1.0025) * init_lr\n",
    "    \n",
    "            preds,                  \\\n",
    "            output_layer,           \\\n",
    "            batch_input_layer_list, \\\n",
    "            batch_layer_list,       \\\n",
    "            batch_bias_val_list = compute_forward_pass(\n",
    "    \n",
    "                input_data,\n",
    "                batch_synapse_list,\n",
    "                batch_bias_val_list,\n",
    "                activation_fn_list,\n",
    "                epsilon=0.01 * epsilon,\n",
    "                order=order if enable_moad_forward_pass else 1,\n",
    "                order_weights_list=batch_order_weights_list if enable_moad_forward_pass else None,\n",
    "                enable_training=True,\n",
    "                enable_strict_derivative_order=enable_strict_derivative_order,\n",
    "                enable_normalization=enable_derivative_normalization,\n",
    "                enable_min_max_scaled_output_layer=enable_min_max_scaled_output_layer\n",
    "    \n",
    "            )\n",
    "    \n",
    "            if enable_strict_derivative_order:\n",
    "                loss_fn = get_mean_squared_error_loss\n",
    "                err_fn  = get_mean_squared_error\n",
    "    \n",
    "                batch_loss, output_preds_error = loss_fn(output_data, preds)\n",
    "    \n",
    "                output_layer_delta = output_data - output_layer\n",
    "                output_derivative  = get_loss_function_derivative(\n",
    "    \n",
    "                    output_data,\n",
    "                    preds,\n",
    "                    err_fn,\n",
    "                    0.01 * epsilon,\n",
    "                    epsilon_regularization_factor,\n",
    "                    order,\n",
    "                    num_auto_diff_steps,\n",
    "                    enable_backprop_activation_nesting,\n",
    "                    enable_derivative_normalization\n",
    "    \n",
    "                )\n",
    "\n",
    "                output_gradient = np.mean(np.abs(output_derivative).T.dot(np.abs(output_layer_delta))) * output_layer_delta\n",
    "            else:\n",
    "                loss_fn = get_mean_squared_error_loss\n",
    "                #loss_fn = get_absolute_error_loss\n",
    "                batch_loss, output_layer_error = loss_fn(output_data, preds)\n",
    "                output_layer_delta = output_data - output_layer\n",
    "    \n",
    "            rnd_lr = learning_rate * random.SystemRandom().uniform(0.9975, 1.0025)\n",
    "    \n",
    "            loss_function_regularizer = 10\n",
    "            batch_bprop_synapse_list,  \\\n",
    "            batch_bprop_bias_val_list, \\\n",
    "            batch_loss,                \\\n",
    "            batch_bprop_order_weights_list = compute_back_propagation(\n",
    "\n",
    "                output_gradient if enable_strict_derivative_order else output_layer_delta,\n",
    "                batch_loss,\n",
    "                batch_layer_list,\n",
    "                batch_input_layer_list,\n",
    "                batch_synapse_list,\n",
    "                batch_bias_val_list,\n",
    "                rnd_lr,\n",
    "                0.01 * epsilon,\n",
    "                epsilon_regularization_factor,\n",
    "                activation_fn_list,\n",
    "                order,\n",
    "                order_weights_list=batch_order_weights_list,\n",
    "                num_auto_diff_steps=num_auto_diff_steps,\n",
    "                enable_backprop_activation_nesting=enable_backprop_activation_nesting,\n",
    "                enable_derivative_normalization=enable_derivative_normalization,\n",
    "                verbose=False\n",
    "\n",
    "            )\n",
    "\n",
    "            batch_loss = np.nan_to_num(\n",
    "\n",
    "                batch_loss, \n",
    "                copy=True,\n",
    "                nan=1 / epsilon, \n",
    "                posinf=1 / epsilon, \n",
    "                neginf=-1 / epsilon\n",
    "\n",
    "            )\n",
    "\n",
    "            batch_loss_list.append(batch_loss)\n",
    "\n",
    "        bprop_loss = np.mean(batch_loss_list)\n",
    "        bprop_loss_col.append(bprop_loss)\n",
    "\n",
    "        if enable_model_selection:\n",
    "            batch_synapse_list,                       \\\n",
    "            best_synapse_list,                        \\\n",
    "            batch_bias_val_list,                      \\\n",
    "            best_bias_val_list,                       \\\n",
    "            best_bprop_loss,                          \\\n",
    "            learning_rate,                            \\\n",
    "            batch_order_weights_list,                 \\\n",
    "            best_order_weights_list,                  \\\n",
    "            num_no_loss_change_steps,                 \\\n",
    "            enable_backprop_activation_nesting,       \\\n",
    "            best_enable_backprop_activation_nesting,  \\\n",
    "            enable_moad_forward_pass,                 \\\n",
    "            best_enable_moad_forward_pass,            \\\n",
    "            enable_strict_derivative_order,           \\\n",
    "            best_enable_strict_derivative_order,      \\\n",
    "            enable_derivative_normalization,          \\\n",
    "            best_enable_derivative_normalization = model_survival(\n",
    "\n",
    "                i,\n",
    "                batch_synapse_list,\n",
    "                batch_bprop_synapse_list,\n",
    "                best_synapse_list,\n",
    "                batch_bias_val_list,\n",
    "                batch_bprop_bias_val_list, \n",
    "                best_bias_val_list,\n",
    "                bprop_loss, \n",
    "                best_bprop_loss,\n",
    "                learning_rate,\n",
    "                best_lr,\n",
    "                init_lr,\n",
    "                batch_order_weights_list,\n",
    "                batch_bprop_order_weights_list,\n",
    "                best_order_weights_list,\n",
    "                num_no_loss_change_steps,\n",
    "                loss_plateau_patience,\n",
    "                remanan_stochastic_expansion,\n",
    "                enable_backprop_activation_nesting,\n",
    "                best_enable_backprop_activation_nesting,\n",
    "                enable_moad_forward_pass,\n",
    "                best_enable_moad_forward_pass,\n",
    "                enable_strict_derivative_order,\n",
    "                best_enable_strict_derivative_order,\n",
    "                enable_derivative_normalization,\n",
    "                best_enable_derivative_normalization,\n",
    "                verbose=verbose\n",
    "\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                del batch_layer_list\n",
    "            except Exception as e:\n",
    "                print(f'Failed deleting layer_list variable due to: {e} ...', end='')\n",
    "                print('', end='\\r')\n",
    "    \n",
    "            clear = lambda: os.system('cls' if os.name=='nt' else 'clear')\n",
    "            clear()\n",
    "    \n",
    "            if (i + 1) % int(math.ceil(num_epochs * 0.1)) == 0 and verbose:\n",
    "                if best_bprop_loss is not None:\n",
    "                    print(\n",
    "                        'Best loss: %f '%(best_bprop_loss), \n",
    "                        f'till iteration: {i + 1} ', \n",
    "                        'with current loss: %f ...' %(bprop_loss), \n",
    "                        ' ' * 128, \n",
    "                        end=''\n",
    "                    )\n",
    "                    print('', end='\\r')\n",
    "\n",
    "        else:\n",
    "            \n",
    "            batch_synapse_list = batch_bprop_synapse_list\n",
    "            batch_bias_val_list = batch_bprop_bias_val_list\n",
    "            batch_order_weights_list = batch_bprop_order_weights_list\n",
    "            \n",
    "    if best_synapse_list is not None and return_best_synapse:\n",
    "        if verbose:\n",
    "            print(\n",
    "\n",
    "                f'Copying best synapse list with loss: {best_bprop_loss} as the trained output ...', \n",
    "                ' ' * 64,\n",
    "                end=''\n",
    "\n",
    "            )\n",
    "            print('', end='\\r')\n",
    "\n",
    "        synapse_list = copy.deepcopy(best_synapse_list)\n",
    "\n",
    "        enable_moad_forward_pass = best_enable_moad_forward_pass\n",
    "        enable_strict_derivative_order = best_enable_strict_derivative_order\n",
    "\n",
    "        enable_derivative_normalization = best_enable_derivative_normalization\n",
    "        enable_backprop_activation_nesting = best_enable_backprop_activation_nesting\n",
    "\n",
    "        del best_synapse_list\n",
    "    else:\n",
    "        synapse_list = copy.deepcopy(batch_synapse_list)\n",
    "\n",
    "    if best_bias_val_list is not None and return_best_synapse:\n",
    "        if verbose:\n",
    "            print(\n",
    "\n",
    "                f'Copying bias values list with loss: {best_bprop_loss} as the trained output ...', \n",
    "                ' ' * 64,\n",
    "                end=''\n",
    "\n",
    "            )\n",
    "            print('', end='\\r')\n",
    "\n",
    "        bias_val_list = copy.deepcopy(best_bias_val_list)\n",
    "\n",
    "        del best_bias_val_list\n",
    "    else:\n",
    "         bias_val_list = copy.deepcopy(batch_bias_val_list)\n",
    "\n",
    "    if best_order_weights_list is not None and return_best_synapse:\n",
    "        if verbose:\n",
    "            print(\n",
    "                f'Copying order weights list with loss: {best_bprop_loss} as the trained output ...', \n",
    "                ' ' * 64,\n",
    "                end=''\n",
    "            )\n",
    "            print('', end='\\r')\n",
    "\n",
    "        if enable_moad_forward_pass:\n",
    "            order_weights_list = copy.deepcopy(best_order_weights_list)\n",
    "\n",
    "        del best_order_weights_list\n",
    "    else:\n",
    "         if enable_moad_forward_pass:\n",
    "             order_weights_list = copy.deepcopy(batch_order_weights_list)\n",
    "\n",
    "    if verbose:\n",
    "        if best_bprop_loss is not None:\n",
    "            print(\n",
    "\n",
    "                '\\n', 'Training completed ...',\n",
    "                '\\n', 'Loss at the end of training: %f and best loss: %f '%(bprop_loss, best_bprop_loss)\n",
    "\n",
    "            )\n",
    "\n",
    "    del best_bprop_loss\n",
    "\n",
    "    return synapse_list,                       \\\n",
    "           bias_val_list,                      \\\n",
    "           order_weights_list,                 \\\n",
    "           bprop_loss_col,                     \\\n",
    "           enable_backprop_activation_nesting, \\\n",
    "           enable_moad_forward_pass,           \\\n",
    "           enable_strict_derivative_order,     \\\n",
    "           enable_derivative_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moad_mlp_predict(\n",
    "        x,\n",
    "        synapse_list,\n",
    "        bias_val_list,\n",
    "        activation_fn_list,\n",
    "        epsilon=1e-24,\n",
    "        order=3,\n",
    "        order_weights_list=None,\n",
    "        enable_moad_forward_pass=False,\n",
    "        enable_strict_derivative_order=False,\n",
    "        enable_normalization=False,\n",
    "        enable_output_layer_output=False,\n",
    "        enable_min_max_scaled_output_layer=False\n",
    "    ):\n",
    "    out_layer, preds = compute_forward_pass(\n",
    "\n",
    "        x,\n",
    "        synapse_list,\n",
    "        bias_val_list,\n",
    "        activation_fn_list,\n",
    "        epsilon=epsilon,\n",
    "        order=order if enable_moad_forward_pass else 1,\n",
    "        order_weights_list=order_weights_list,\n",
    "        enable_training=False,\n",
    "        enable_strict_derivative_order=enable_strict_derivative_order,\n",
    "        enable_normalization=enable_normalization,\n",
    "        enable_min_max_scaled_output_layer=enable_min_max_scaled_output_layer\n",
    "\n",
    "    )\n",
    "\n",
    "    if enable_output_layer_output:\n",
    "        return out_layer, preds\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a MOAD MLP XOR gate solver:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specify XOR gate logic table as input and labels:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(\n",
    "    [\n",
    "        [0, 0], [1, 1], [0, 1], [1, 0]\n",
    "    ]\n",
    ")\n",
    "\n",
    "y = np.array(\n",
    "    [\n",
    "        [0], [0], [1], [1]\n",
    "    ]\n",
    ")\n",
    "\n",
    "input_dim, output_dim = x.shape[-1], y.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data, output_data = x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specify model architecture and hyper-parameters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon_regularization_factor(epsilon):\n",
    "    return math.ceil(\n",
    "\n",
    "        50 * (\n",
    "\n",
    "            1 / (\n",
    "\n",
    "                (\n",
    "\n",
    "                    (\n",
    "\n",
    "                        abs(\n",
    "\n",
    "                            math.log(epsilon)\n",
    "\n",
    "                        ) / math.log(10)\n",
    "\n",
    "                    ) / 8\n",
    "\n",
    "                ) + 1)\n",
    "\n",
    "        )\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_units_list = [\n",
    "\n",
    "    input_dim, \n",
    "\n",
    "    12,\n",
    "\n",
    "    8,\n",
    "\n",
    "    output_dim\n",
    "\n",
    "]\n",
    "\n",
    "bias_val_list = [\n",
    "\n",
    "    0.1, \n",
    "\n",
    "    0.1,\n",
    "\n",
    "    0.1\n",
    "\n",
    "]\n",
    "\n",
    "activation_fn_list = [\n",
    "\n",
    "    sigmoid for i in range(len(dense_units_list) - 1)\n",
    "\n",
    "]\n",
    "\n",
    "learning_rate = math.pi / 5 #1.149957544058746093905526\n",
    "epsilon = learning_rate * 1e-2 if learning_rate < 1e-7  else 1e-8\n",
    "epsilon_regularization_factor = get_epsilon_regularization_factor(epsilon)\n",
    "lr_epsilon = learning_rate * 1e-3 if learning_rate < 1e-7  else 1e-8\n",
    "activation_fn_list = [sigmoid, sigmoid, sigmoid]\n",
    "num_epochs = 1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "\n",
    "    epsilon,\n",
    "\n",
    "    epsilon_regularization_factor\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to create predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor_moad_predict(\n",
    "        x,\n",
    "        synapse_list,\n",
    "        bias_val_list,\n",
    "        activation_fn_list,\n",
    "        epsilon=1e-24,\n",
    "        order=4,\n",
    "        order_weights_list=None,\n",
    "        enable_moad_forward_pass=False,\n",
    "        enable_strict_derivative_order=True,\n",
    "        enable_normalization=False,\n",
    "        enable_min_max_scaled_output_layer=False\n",
    "    ):\n",
    "    '''\n",
    "    '''\n",
    "    out_layer = compute_forward_pass(\n",
    "        x,\n",
    "        synapse_list,\n",
    "        bias_val_list,\n",
    "        activation_fn_list,\n",
    "        epsilon=epsilon,\n",
    "        order=order if enable_moad_forward_pass else 1,\n",
    "        order_weights_list=order_weights_list if enable_moad_forward_pass else None,\n",
    "        enable_training=False,\n",
    "        enable_strict_derivative_order=enable_strict_derivative_order,\n",
    "        enable_normalization=enable_normalization,\n",
    "        enable_min_max_scaled_output_layer=enable_min_max_scaled_output_layer\n",
    "    )[1]\n",
    "\n",
    "    return np.asarray([out_layer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Initiate empty synapses:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synapse_list = create_synapse(\n",
    "\n",
    "    dense_units_list,\n",
    "\n",
    "    bias_val_list\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Use a randomization kernel for initializing training:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_kernel(\n",
    "        synapse_list,\n",
    "        k=1024,\n",
    "        selection_probability=0.5,\n",
    "        epsilon=1e-8\n",
    "    ):\n",
    "    assert k > 0 and (isinstance(k, float) or isinstance(k, int)), f'Expected kernel noise factor to be a positive non-zero float, instead received: {k} ...'\n",
    "    if selection_probability is not None:\n",
    "        assert 0 <= selection_probability <= 1 and (isinstance(selection_probability, float) or isinstance(selection_probability, int)), f'Expected 0 <= selection_probability <= 1, instead received: {selection_probability} ...'\n",
    "    assert epsilon >= 0 and (isinstance(epsilon, float) or isinstance(epsilon, int)), f'Expected epsilon to be positive float value, instead received: {type(epsilon)} of value: {epsilon} ...'\n",
    " \n",
    "    out_synapse_list = []\n",
    "    for s in synapse_list:\n",
    "        s_shape = s.shape\n",
    "        s_flat = s.flatten()\n",
    "        for idx, i in enumerate(s_flat):\n",
    "            if (\n",
    "                selection_probability is None or (\n",
    "                    selection_probability is not None and random.SystemRandom().random() < selection_probability\n",
    "                )\n",
    "            ):\n",
    "                s_flat[idx] = (\n",
    "                    1 + (\n",
    "                        np.cos(\n",
    "                            math.pi * 2 * (\n",
    "                                np.exp(\n",
    "                                    - 1 + (\n",
    "                                        (i + 1) / 10\n",
    "                                      )\n",
    "                                )\n",
    "                            )\n",
    "                        )\n",
    "                    ) * k * random.SystemRandom().uniform(\n",
    "                            1 - (epsilon / 2), \n",
    "                            1 + (epsilon / 2)\n",
    "                        )\n",
    "                )\n",
    "\n",
    "        s = s_flat.reshape(\n",
    "            s_shape\n",
    "        )\n",
    "\n",
    "        out_synapse_list.append(s)\n",
    "\n",
    "        del s\n",
    "\n",
    "    del synapse_list\n",
    "\n",
    "    return out_synapse_list\n",
    "\n",
    "def random_noise_addition_kernel(\n",
    "        synapse_list,\n",
    "        k=1024,\n",
    "        selection_probability=0.5 \n",
    "    ):\n",
    "    assert k > 0 and (isinstance(k, float) or isinstance(k, int)), f'Expected kernel noise factor to be a positive non-zero float, instead received: {k} ...'\n",
    "    if selection_probability is not None:\n",
    "        assert 0 <= selection_probability <= 1 and (isinstance(selection_probability, float) or isinstance(selection_probability, int)), f'Expected 0 <= selection_probability <= 1, instead received: {selection_probability} ...'\n",
    "\n",
    "    out_synapse_list = []\n",
    "    for s in synapse_list:\n",
    "        s_shape = s.shape\n",
    "        s_flat = s.flatten()\n",
    "        for idx, i in enumerate(s_flat):\n",
    "            if (\n",
    "                selection_probability is None or (\n",
    "                    selection_probability is not None and random.SystemRandom().random() < selection_probability\n",
    "                )\n",
    "            ):\n",
    "                s_flat[idx] = random.SystemRandom().random() * k\n",
    "        s = s_flat.reshape(s_shape)\n",
    "        out_synapse_list.append(s)\n",
    "        del s\n",
    "    del synapse_list\n",
    "\n",
    "    return out_synapse_list\n",
    "\n",
    "def random_noise_multiplication_kernel(\n",
    "        synapse_list, \n",
    "        k=1,\n",
    "        selection_probability=0.5\n",
    "    ):\n",
    "    assert k > 0 and (isinstance(k, float) or isinstance(k, int)), f'Expected kernel noise factor to be a positive non-zero float, instead received: {k} ...'\n",
    "    if selection_probability is not None:\n",
    "        assert 0 <= selection_probability <= 1 and (isinstance(selection_probability, float) or isinstance(selection_probability, int)), f'Expected 0 <= selection_probability <= 1, instead received: {selection_probability} ...'\n",
    "    out_synapse_list = []\n",
    "    for s in synapse_list:\n",
    "        s_shape = s.shape\n",
    "        s_flat = s.flatten()\n",
    "        for idx, i in enumerate(s_flat):\n",
    "            div_val = k + s_flat[idx]\n",
    "            if div_val != 0 and (\n",
    "                selection_probability is None or (\n",
    "                    selection_probability is not None and random.SystemRandom().random() < selection_probability\n",
    "                )\n",
    "            ):\n",
    "                s_flat[idx] = random.SystemRandom().random() *  (\n",
    "                    (s_flat[idx] + div_val) / div_val\n",
    "                )\n",
    "\n",
    "        s = s_flat.reshape(s_shape)\n",
    "        out_synapse_list.append(s)\n",
    "        del s\n",
    "    del synapse_list\n",
    "\n",
    "    return out_synapse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synapse_list = random_noise_addition_kernel(\n",
    "    cosine_kernel(\n",
    "        random_noise_multiplication_kernel(\n",
    "            synapse_list,\n",
    "            selection_probability=0.125\n",
    "        ),\n",
    "        selection_probability=0.125\n",
    "    ),\n",
    "    selection_probability=0.125\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 5\n",
    "order_weights_list = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Apply separate min max scaling for each of the synapses prior to training:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaler_layer(\n",
    "\n",
    "        inp_arr:np.ndarray,\n",
    "\n",
    "        instance_id:str\n",
    "\n",
    "    )->np.ndarray:\n",
    "\n",
    "    assert instance_id is not None, 'Expected a min max scaler layer instance identification string ...'\n",
    "\n",
    "    assert type(instance_id) == str, f'Expected the value passed using the argument instance_id to be a string, instead received an input of type: {type(instance_id)} ...'\n",
    "\n",
    "    try:\n",
    "\n",
    "        global_min_val = globals()[f'global_min_max_scaler_dict_{instance_id}'][instance_id]['global_min_val']\n",
    "\n",
    "        global_max_val = globals()[f'global_min_max_scaler_dict_{instance_id}'][instance_id]['global_max_val']\n",
    "\n",
    "    except KeyError:\n",
    "\n",
    "        empty_dict = dict()\n",
    "\n",
    "        empty_dict.update(\n",
    "\n",
    "            {\n",
    "\n",
    "                'global_min_val' : None\n",
    "\n",
    "            }\n",
    "\n",
    "        )\n",
    "\n",
    "        empty_dict.update(\n",
    "\n",
    "            {\n",
    "\n",
    "                'global_max_val' : None\n",
    "\n",
    "            }\n",
    "            \n",
    "        )\n",
    "\n",
    "        instance_dict = {\n",
    "\n",
    "            instance_id : empty_dict\n",
    "\n",
    "        }\n",
    "\n",
    "        globals().update(\n",
    "\n",
    "            {\n",
    "\n",
    "                f'global_min_max_scaler_dict_{instance_id}' : instance_dict\n",
    "\n",
    "            }\n",
    "\n",
    "        )\n",
    "\n",
    "        global_min_val = globals()[f'global_min_max_scaler_dict_{instance_id}'][instance_id]['global_min_val']\n",
    "\n",
    "        global_max_val = globals()[f'global_min_max_scaler_dict_{instance_id}'][instance_id]['global_max_val']\n",
    "\n",
    "    min_val, max_val = np.min(inp_arr), np.max(inp_arr)\n",
    "\n",
    "    if global_min_val is None:\n",
    "\n",
    "        globals()[f'global_min_max_scaler_dict_{instance_id}'][instance_id]['global_min_val'] = min_val\n",
    "\n",
    "        global_min_val = min_val\n",
    "\n",
    "    if global_max_val is None:\n",
    "\n",
    "        globals()[f'global_min_max_scaler_dict_{instance_id}'][instance_id]['global_max_val'] = max_val\n",
    "\n",
    "        global_max_val = max_val\n",
    "\n",
    "    if min_val < global_min_val:\n",
    "\n",
    "        globals()[f'global_min_max_scaler_dict_{instance_id}'][instance_id]['global_min_val'] = min_val\n",
    "\n",
    "    else:\n",
    "\n",
    "        min_val = globals()[f'global_min_max_scaler_dict_{instance_id}'][instance_id]['global_min_val']\n",
    "\n",
    "    if max_val > global_max_val:\n",
    "\n",
    "        globals()[f'global_min_max_scaler_dict_{instance_id}'][instance_id]['global_max_val'] = max_val\n",
    "\n",
    "    else:\n",
    "\n",
    "        max_val = globals()[f'global_min_max_scaler_dict_{instance_id}'][instance_id]['global_max_val']\n",
    "\n",
    "    if max_val - min_val != 0:\n",
    "\n",
    "        out_arr = (inp_arr - min_val) / (max_val - min_val)\n",
    "\n",
    "        del min_val, max_val, inp_arr\n",
    "\n",
    "        return out_arr\n",
    "\n",
    "    return inp_arr, instance_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synapse_list = [\n",
    "        min_max_scaler_layer(\n",
    "            np.nan_to_num(\n",
    "                s, posinf=1/12, neginf=-1/12, nan=0),\n",
    "            instance_id=f'synapse_{s_idx}'\n",
    "        ) for s_idx, s in enumerate(synapse_list)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model training:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = learning_rate * random.SystemRandom().uniform(\n",
    "    0.9975,\n",
    "    1.0025\n",
    ")\n",
    "print(\n",
    "    'Training using learning rate: ',\n",
    "    lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_synapse_list_crypto_sign = '46b111a1925dcb39310a9233511e1af1'\n",
    "\n",
    "saved_synapse_list_file_prefix = 'saved_synapse_list_xor_no_opt_'\n",
    "\n",
    "saved_synapse_list_dir = '../weights/'\n",
    "\n",
    "saved_synapse_list_file = os.path.realpath(\n",
    "\n",
    "    saved_synapse_list_dir + \n",
    "\n",
    "    saved_synapse_list_file_prefix +\n",
    "\n",
    "    saved_synapse_list_crypto_sign + \n",
    "\n",
    "    '.pkl'\n",
    "\n",
    ")\n",
    "\n",
    "load_saved_synapse_list = True # False #\n",
    "\n",
    "if os.path.exists(saved_synapse_list_file) and load_saved_synapse_list:\n",
    "\n",
    "    [synapse_list, bias_val_list,  order_weights_list] = load_pickle(saved_synapse_list_file)\n",
    "\n",
    "    loaded_synapse_list_crypto_sign = hash_generator(\n",
    "\n",
    "        str([synapse_list, bias_val_list,  order_weights_list]),\n",
    "\n",
    "        digest_size=16,\n",
    "\n",
    "        hash_algorithm='blake2b',\n",
    "\n",
    "        verbose=True\n",
    "\n",
    "    )\n",
    "\n",
    "    assert saved_synapse_list_crypto_sign == loaded_synapse_list_crypto_sign\n",
    "\n",
    "    print(f'Loaded saved synapse list from: {saved_synapse_list_file} ...')\n",
    "\n",
    "    num_epochs = 32\n",
    "\n",
    "    lr = 0.001 * lr\n",
    "\n",
    "    print(f'Reducing training epochs step to: {num_epochs}  and learning rate to: {lr} ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "synapse_list,                       \\\n",
    "bias_val_list,                      \\\n",
    "order_weights_list,                 \\\n",
    "loss_col,                           \\\n",
    "enable_backprop_activation_nesting, \\\n",
    "enable_moad_forward_pass,           \\\n",
    "enable_strict_derivative_order,     \\\n",
    "enable_derivative_normalization = moad_mlp_train(\n",
    "\n",
    "    input_data,\n",
    "    output_data,\n",
    "    synapse_list,\n",
    "    bias_val_list,\n",
    "    activation_fn_list,\n",
    "    lr,\n",
    "    epsilon * random.SystemRandom().uniform(0.9975, 1.0025),\n",
    "    epsilon_regularization_factor,\n",
    "    num_epochs,\n",
    "    order=order,\n",
    "    order_weights_list=order_weights_list,\n",
    "    loss_plateau_patience=int(math.ceil(num_epochs * 0.01)),\n",
    "    num_auto_diff_steps=1,\n",
    "    remanan_stochastic_expansion=0.1,\n",
    "    return_best_synapse=True,                  #False,#\n",
    "    enable_backprop_activation_nesting=False,  #True, #\n",
    "    enable_moad_forward_pass=False,            #True, #\n",
    "    enable_strict_derivative_order=True,       #False,#\n",
    "    enable_derivative_normalization=True,      #False,#\n",
    "    enable_min_max_scaled_output_layer=False,  #True, #\n",
    "    verbose=True                               #False #\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Secure management of saved models using cryptographic signature:**\n",
    "\n",
    "The trained synapse list file is passed through a cryptographic hash generator function: ```hash_generator```, to create a signature of length: ${2 \\times \\text{digest size}}$.\n",
    "\n",
    "The cryptographic hash generator currently supports: ```blake2b``` and ```blake2s``` cryptographic hash functions.\n",
    "\n",
    "$\\def\\multiset#1#2{\\left(\\!\\left({#1\\atopwithdelims..#2}\\right)\\!\\right)}$\n",
    "\n",
    "${\\text{Weights of the neural network is defined as the ordered multiset: }}{{\\multiset{n}{k}}^{W{\\degree}}}{\\text{, where: }}$\n",
    "\n",
    "${\\hspace{1cm}{n{\\mapsto}\\text{number of unique weights for all the trainable neurons, }}}$\n",
    "\n",
    "${\\hspace{1cm}{n {\\leq} k}{\\text{, }}}$\n",
    "\n",
    "${\\hspace{1cm}{\\text{k = number of trainable neurons}}}$\n",
    "\n",
    "${\\text{Hash function: } \\textbf{H}(x) \\text{ is defined such that: }}$\n",
    "\n",
    "${\\hspace{1cm}{p(\\textbf{H}(A) = \\textbf{H}(B)) \\approx 0}{\\text{, }}}$\n",
    "\n",
    "${\\hspace{1cm}{\\text{where: } {A {\\neq} B}}\\text{, }{\\textbf{H}(A) {\\neq} A}\\text{ and }{\\textbf{H}(B) {\\neq} B}}$\n",
    "\n",
    "$\\therefore \\text{Crytpographic signature } {(C_{\\text{signature }{{\\multiset{n}{k}}^{W{\\degree}}}})} = {\\textbf{H}({{\\multiset{n}{k}}^{W{\\degree}}})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synapse_list_crypto_sign = hash_generator(\n",
    "\n",
    "    str(\n",
    "        \n",
    "        [\n",
    "\n",
    "            synapse_list, \n",
    "\n",
    "            bias_val_list, \n",
    "\n",
    "            order_weights_list\n",
    "\n",
    "        ]\n",
    "\n",
    "    ),\n",
    "\n",
    "    digest_size=16,\n",
    "\n",
    "    hash_algorithm='blake2b',\n",
    "\n",
    "    verbose=True\n",
    "\n",
    ")\n",
    "\n",
    "print(\n",
    "\n",
    "    '\\n',\n",
    "\n",
    "    'Generated a cryptographic signature of the synapse list: ',\n",
    "\n",
    "    '\\n',\n",
    "\n",
    "    synapse_list_crypto_sign\n",
    "\n",
    ")\n",
    "\n",
    "save_pickle(\n",
    "\n",
    "    [synapse_list, bias_val_list, order_weights_list],\n",
    "\n",
    "    f'{saved_synapse_list_file_prefix}{synapse_list_crypto_sign}.pkl',\n",
    "\n",
    "    protocol=-1,\n",
    "\n",
    "    compression=True,\n",
    "\n",
    "    verbose=True\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_file = f'../media/xor_moad_mlp_{synapse_list_crypto_sign}_no_opt.png'\n",
    "\n",
    "save_plot = True # False #\n",
    "plot_resolution = 160 # 3200 #\n",
    "enable_scatter_plot_overlay = True # False #\n",
    "\n",
    "fig = plt.figure(\n",
    "    figsize=(6, 6),\n",
    "    layout='constrained'\n",
    ")\n",
    "\n",
    "spec = fig.add_gridspec(\n",
    "    ncols=1,\n",
    "    nrows=1\n",
    ")\n",
    "\n",
    "fig.add_subplot(\n",
    "    spec[0, 0]\n",
    ")\n",
    "\n",
    "plot_xor_gate_decision_boundary(\n",
    "    lambda x: xor_moad_predict(\n",
    "                x,\n",
    "                synapse_list,\n",
    "                bias_val_list,\n",
    "                activation_fn_list,\n",
    "                order=5,\n",
    "                enable_moad_forward_pass=enable_moad_forward_pass,\n",
    "                enable_strict_derivative_order=enable_strict_derivative_order,\n",
    "                enable_normalization=enable_derivative_normalization,\n",
    "                enable_min_max_scaled_output_layer=True\n",
    "            ),\n",
    "    x, y, enable_scatter_plot=enable_scatter_plot_overlay\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "    'XOR gate neural network model using multi order automatic derivatives'\n",
    ")\n",
    "\n",
    "if save_plot:\n",
    "    plt.savefig(\n",
    "        plot_file, \n",
    "        bbox_inches='tight', \n",
    "        dpi=plot_resolution\n",
    "    )\n",
    "\n",
    "plt.close()\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_moad_preds = xor_moad_predict(\n",
    "\n",
    "    x,\n",
    "    synapse_list,\n",
    "    bias_val_list,\n",
    "    activation_fn_list,\n",
    "    epsilon=epsilon,\n",
    "    order=order,\n",
    "    enable_moad_forward_pass=enable_moad_forward_pass,\n",
    "    enable_strict_derivative_order=enable_strict_derivative_order,\n",
    "    enable_normalization=enable_derivative_normalization,\n",
    "    enable_min_max_scaled_output_layer=True\n",
    "\n",
    ")\n",
    "\n",
    "print(\n",
    "    '\\n',\n",
    "    x,\n",
    "    '\\n',\n",
    "    '\\n',\n",
    "    min_max_scaler(xor_moad_preds),\n",
    "    '\\n',\n",
    "    '\\n',\n",
    "    xor_moad_preds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_file = f'../media/xor_moad_mlp_{synapse_list_crypto_sign}_min_max_scaled_no_opt.png'\n",
    "\n",
    "save_plot = False # True # \n",
    "plot_resolution = 160 # 3200 # \n",
    "enable_scatter_plot_overlay = True # False #\n",
    "\n",
    "fig = plt.figure(\n",
    "\n",
    "    figsize=(6, 6),\n",
    "    layout='constrained'\n",
    "\n",
    ")\n",
    "\n",
    "spec = fig.add_gridspec(\n",
    "\n",
    "    ncols=1, \n",
    "    nrows=1\n",
    "\n",
    ")\n",
    "\n",
    "fig.add_subplot(\n",
    "\n",
    "    spec[0, 0]\n",
    "\n",
    ")\n",
    "\n",
    "plot_xor_gate_decision_boundary(\n",
    "\n",
    "    lambda x: min_max_scaler(\n",
    "\n",
    "                  xor_moad_predict(\n",
    "\n",
    "                      x,\n",
    "                      synapse_list,\n",
    "                      bias_val_list,\n",
    "                      activation_fn_list,\n",
    "                      order=order,\n",
    "                      enable_moad_forward_pass=enable_moad_forward_pass,\n",
    "                      enable_strict_derivative_order=enable_strict_derivative_order,\n",
    "                      enable_normalization=enable_derivative_normalization\n",
    "\n",
    "                  )\n",
    "\n",
    "              ),\n",
    "\n",
    "    x, y, enable_scatter_plot=enable_scatter_plot_overlay\n",
    "\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "\n",
    "    'XOR gate neural network model using multi order automatic derivatives'\n",
    "\n",
    ")\n",
    "\n",
    "if save_plot:\n",
    "    plt.savefig(\n",
    "        plot_file,\n",
    "        bbox_inches='tight',\n",
    "        dpi=plot_resolution\n",
    "    )\n",
    "\n",
    "plt.close()\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del fig, synapse_list, bias_val_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Graduated MOAD learning:**\n",
    "\n",
    "In the graduated MOAD learning strategy for training the MOAD MLP model, different phases of the model learning processes are finally combined together in the concluding learning phase.\n",
    "\n",
    "The practical application of graduated MOAD learning is to converge to the widest possible decision boundary using carefully controlled stochastic pertubations in each of the learning phases.\n",
    "\n",
    "In this implementation, the initial training phases with the nested activation function during the back-propagation; specified using the: ```enable_backprop_activation_nesting``` argument in the training function and the  multi-order automatic derivatives phase during the forward propagation, specified using the: ```enable_moad_forward_pass``` argument, are combined together in the final training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_units_list = [input_dim, 12,  8, output_dim]\n",
    "bias_val_list = [0.1, 0.1, 0.1]\n",
    "\n",
    "learning_rate = 0.6499779277\n",
    "epsilon = learning_rate * 1e-2 if learning_rate < 1e-7  else 1e-8\n",
    "epsilon_regularization_factor = get_epsilon_regularization_factor(epsilon)\n",
    "lr_epsilon = learning_rate * 1e-3 if learning_rate < 1e-7  else 1e-8\n",
    "activation_fn = sigmoid # tanh # relu #\n",
    "num_epochs = 480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synapse_list = create_synapse(\n",
    "\n",
    "    dense_units_list,\n",
    "\n",
    "    bias_val_list\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = 5\n",
    "order_weights_list = [[1 for i in range(order)] for _ in synapse_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_synapse_list_crypto_sign = '2d8c257b326e0d8e395475ba3dec6abf'\n",
    "\n",
    "saved_synapse_list_file_prefix = 'saved_synapse_list_xor_gml_'\n",
    "\n",
    "saved_synapse_list_dir = '../weights/'\n",
    "\n",
    "load_saved_synapse_list = True # False # \n",
    "\n",
    "saved_synapse_list_file = os.path.realpath(\n",
    "\n",
    "    saved_synapse_list_dir + \n",
    "\n",
    "    saved_synapse_list_file_prefix +\n",
    "\n",
    "    saved_synapse_list_crypto_sign + \n",
    "\n",
    "    '.pkl'\n",
    "\n",
    ")\n",
    "\n",
    "if os.path.exists(saved_synapse_list_file) and load_saved_synapse_list:\n",
    "\n",
    "    [synapse_list, bias_val_list,  order_weights_list] = load_pickle(saved_synapse_list_file)\n",
    "\n",
    "    loaded_synapse_list_crypto_sign = hash_generator(\n",
    "\n",
    "        str([synapse_list, bias_val_list,  order_weights_list]),\n",
    "\n",
    "        digest_size=16,\n",
    "\n",
    "        hash_algorithm='blake2b',\n",
    "\n",
    "        verbose=True\n",
    "\n",
    "    )\n",
    "\n",
    "    assert saved_synapse_list_crypto_sign == loaded_synapse_list_crypto_sign\n",
    "\n",
    "    print(f'Loaded saved synapse list from: {saved_synapse_list_file} ...')\n",
    "\n",
    "    num_epochs = 16\n",
    "\n",
    "    print(f'Reducing training epochs per graduated MOAD learning step to: {num_epochs} ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = None\n",
    "best_synapse_list = None\n",
    "best_bias_val_list = None\n",
    "best_order_weights_list = None\n",
    "\n",
    "enable_backprop_activation_nesting = False\n",
    "enable_moad_forward_pass = False\n",
    "enable_strict_derivative_order = True\n",
    "enable_derivative_normalization = True\n",
    "enable_min_max_scaled_output_layer = False\n",
    "\n",
    "best_enable_backprop_activation_nesting = None\n",
    "best_enable_moad_forward_pass = None\n",
    "best_enable_strict_derivative_order = None\n",
    "best_enable_derivative_normalization = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loss_col = []\n",
    "\n",
    "for d_idx in range(2):\n",
    "\n",
    "    clear_output(\n",
    "\n",
    "        wait=False\n",
    "\n",
    "    )\n",
    "\n",
    "    for g_idx in range(2):\n",
    "\n",
    "        synapse_list,                       \\\n",
    "        bias_val_list,                      \\\n",
    "        order_weights_list,                 \\\n",
    "        loss_col,                           \\\n",
    "        enable_backprop_activation_nesting, \\\n",
    "        enable_moad_forward_pass,           \\\n",
    "        enable_strict_derivative_order,     \\\n",
    "        enable_derivative_normalization = moad_mlp_train(\n",
    "\n",
    "            input_data,\n",
    "            output_data,\n",
    "            synapse_list,\n",
    "            bias_val_list,\n",
    "            activation_fn_list,\n",
    "            learning_rate,\n",
    "            epsilon,\n",
    "            epsilon_regularization_factor,\n",
    "            math.ceil(num_epochs / (g_idx + 1)),\n",
    "            order=order,\n",
    "            order_weights_list=order_weights_list,\n",
    "            loss_plateau_patience=math.ceil(num_epochs * 0.01),\n",
    "            num_auto_diff_steps=4,\n",
    "            remanan_stochastic_expansion=0.1,\n",
    "            return_best_synapse=True,\n",
    "            enable_backprop_activation_nesting=enable_backprop_activation_nesting,\n",
    "            enable_moad_forward_pass=enable_moad_forward_pass,\n",
    "            enable_strict_derivative_order=enable_strict_derivative_order,\n",
    "            enable_derivative_normalization=enable_derivative_normalization,\n",
    "            enable_min_max_scaled_output_layer=enable_min_max_scaled_output_layer\n",
    "\n",
    "        )\n",
    "\n",
    "        if best_loss is None or best_loss > np.min(loss_col):\n",
    "\n",
    "            best_loss = np.min(loss_col)\n",
    "\n",
    "            best_synapse_list = synapse_list\n",
    "\n",
    "            best_bias_val_list = bias_val_list\n",
    "\n",
    "            best_order_weights_list = order_weights_list\n",
    "\n",
    "            best_enable_backprop_activation_nesting = enable_backprop_activation_nesting\n",
    "\n",
    "            best_enable_moad_forward_pass = enable_moad_forward_pass\n",
    "\n",
    "            best_enable_strict_derivative_order = enable_strict_derivative_order\n",
    "\n",
    "            best_enable_derivative_normalization = enable_derivative_normalization\n",
    "\n",
    "        all_loss_col += loss_col\n",
    "\n",
    "        print(\n",
    "\n",
    "            np.sum(\n",
    "\n",
    "                loss_col\n",
    "\n",
    "            )\n",
    "\n",
    "        )\n",
    "\n",
    "    if best_loss is None:\n",
    "\n",
    "        synapse_list = best_synapse_list\n",
    "\n",
    "        bias_val_list = best_bias_val_list\n",
    "\n",
    "        order_weights_list = best_order_weights_list\n",
    "\n",
    "        enable_backprop_activation_nesting = best_enable_backprop_activation_nesting\n",
    "\n",
    "        enable_moad_forward_pass = best_enable_moad_forward_pass\n",
    "\n",
    "        enable_strict_derivative_order = best_enable_strict_derivative_order\n",
    "\n",
    "        enable_derivative_normalization = best_enable_derivative_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "\n",
    "    all_loss_col\n",
    "\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_moad_preds = xor_moad_predict(\n",
    "\n",
    "    x,\n",
    "    best_synapse_list,\n",
    "    best_bias_val_list,\n",
    "    activation_fn_list,\n",
    "    epsilon=epsilon,\n",
    "    order=order,\n",
    "    order_weights_list=best_order_weights_list,\n",
    "    enable_moad_forward_pass=best_enable_moad_forward_pass,\n",
    "    enable_strict_derivative_order=best_enable_strict_derivative_order,\n",
    "    enable_min_max_scaled_output_layer=enable_min_max_scaled_output_layer\n",
    "\n",
    ")\n",
    "\n",
    "print(\n",
    "\n",
    "    '\\n', \n",
    "\n",
    "    x, \n",
    "\n",
    "    '\\n', \n",
    "\n",
    "    '\\n', \n",
    "\n",
    "    xor_moad_preds\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synapse_list_crypto_sign = hash_generator(\n",
    "\n",
    "    str(\n",
    "\n",
    "        [\n",
    "\n",
    "            best_synapse_list,\n",
    "\n",
    "            best_bias_val_list,\n",
    "\n",
    "            best_order_weights_list\n",
    "\n",
    "        ]\n",
    "\n",
    "    ),\n",
    "\n",
    "    digest_size=16,\n",
    "\n",
    "    hash_algorithm='blake2b',\n",
    "\n",
    "    verbose=True\n",
    "\n",
    ")\n",
    "\n",
    "save_pickle(\n",
    "\n",
    "    [\n",
    "        \n",
    "        best_synapse_list, \n",
    "        \n",
    "        best_bias_val_list,  \n",
    "        \n",
    "        best_order_weights_list\n",
    "    \n",
    "    ],\n",
    "\n",
    "    f'{saved_synapse_list_file_prefix}{synapse_list_crypto_sign}.pkl',\n",
    "\n",
    "    protocol=-1,\n",
    "\n",
    "    compression=True,\n",
    "\n",
    "    verbose=True\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_file = f'../media/xor_moad_mlp_{synapse_list_crypto_sign}_sigmoid_gml.png'\n",
    "\n",
    "save_plot =  False # True # \n",
    "plot_resolution = 160 # 3200 # \n",
    "enable_scatter_plot_overlay = True # False #\n",
    "\n",
    "fig = plt.figure(\n",
    "\n",
    "    figsize=(6, 6),\n",
    "    layout='constrained'\n",
    "\n",
    ")\n",
    "\n",
    "spec = fig.add_gridspec(\n",
    "\n",
    "    ncols=1, \n",
    "    nrows=1\n",
    "\n",
    ")\n",
    "\n",
    "fig.add_subplot(\n",
    "\n",
    "    spec[0, 0]\n",
    "\n",
    ")\n",
    "\n",
    "plot_xor_gate_decision_boundary(\n",
    "\n",
    "    lambda x: xor_moad_predict(\n",
    "\n",
    "                  x,\n",
    "                  best_synapse_list,\n",
    "                  best_bias_val_list,\n",
    "                  activation_fn_list,\n",
    "                  order=order,\n",
    "                  order_weights_list=best_order_weights_list,\n",
    "                  enable_moad_forward_pass=best_enable_moad_forward_pass,\n",
    "                  enable_strict_derivative_order=best_enable_strict_derivative_order,\n",
    "                  enable_min_max_scaled_output_layer=enable_min_max_scaled_output_layer\n",
    "\n",
    "              ),\n",
    "\n",
    "    x, y, enable_scatter_plot=enable_scatter_plot_overlay\n",
    "\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "\n",
    "    'XOR gate neural network model using multi order automatic derivatives'\n",
    "\n",
    ")\n",
    "\n",
    "if save_plot:\n",
    "\n",
    "    plt.savefig(\n",
    "\n",
    "        plot_file,\n",
    "        bbox_inches='tight',\n",
    "        dpi=plot_resolution\n",
    "\n",
    "    )\n",
    "\n",
    "plt.close()\n",
    "\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualize synapses and bias values using Hinton diagram:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filled_square(x, y, area, colour):\n",
    "     '''\n",
    "         Draws a square-shaped blob with the given area (< 1) at\n",
    "         the given coordinates.\n",
    "     '''\n",
    "     hs = np.sqrt(area) / 2\n",
    "     xcorners = np.array([x - hs, x + hs, x + hs, x - hs])\n",
    "     ycorners = np.array([y - hs, y - hs, y + hs, y + hs])\n",
    "     plt.fill(xcorners, ycorners, colour, edgecolor=colour)\n",
    "\n",
    "def hinton_diagram(W, max_weight=None, plot_file=None, plot_resolution=160, save_plot=False):\n",
    "    '''\n",
    "        Draws a Hinton diagram for visualizing a weight matrix.\n",
    "        Temporarily disables matplotlib interactive mode if it is on,\n",
    "        otherwise this takes forever.\n",
    "    '''\n",
    "    enable_interactive_mode=False\n",
    "    if plt.isinteractive():\n",
    "        plt.ioff()\n",
    "        enable_interactive_mode=True\n",
    "    plt.clf()\n",
    "    height, width = W.shape\n",
    "    if not max_weight:\n",
    "        max_weight = 2 ** np.ceil(np.log(np.max(np.abs(W)))/np.log(2))\n",
    "\n",
    "    plt.fill(np.array([0, width,width, 0]), np.array([0, 0, height,height]), 'gray')\n",
    "    plt.axis('off')\n",
    "    plt.axis('equal')\n",
    "\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            tmp_x, tmp_y = x + 1, y + 1\n",
    "            w = W[y, x]\n",
    "            if w > 0:\n",
    "                create_filled_square(tmp_x - 0.5, height - tmp_y + 0.5, min(1,  w/max_weight), 'white')\n",
    "            elif w < 0:\n",
    "                create_filled_square(tmp_x - 0.5, height - tmp_y + 0.5, min(1, -w/max_weight), 'black')\n",
    "\n",
    "    if enable_interactive_mode:\n",
    "\n",
    "        plt.ion()\n",
    "\n",
    "    if save_plot and plot_file is not None:\n",
    "\n",
    "        assert isinstance(plot_file, str), f'Expected the argument passed using plot_file to be a string, instead received: {plot_file} ...'\n",
    "\n",
    "        plt.savefig(\n",
    "\n",
    "            plot_file,\n",
    "            bbox_inches='tight',\n",
    "            dpi=plot_resolution\n",
    "\n",
    "        )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s_idx, synapse in enumerate(synapse_list):\n",
    "    print(synapse.shape)\n",
    "    hinton_diagram(\n",
    "        synapse,\n",
    "        plot_file=f'../media/xor_moad_mlp_{synapse_list_crypto_sign}_synapse_{s_idx}_sigmoid_gml_hd.png',\n",
    "        plot_resolution=160,#3200,#\n",
    "        save_plot=False#True#\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b_idx, bias_val in enumerate(bias_val_list):\n",
    "    print(bias_val.shape)\n",
    "    hinton_diagram(\n",
    "        bias_val, \n",
    "        plot_file=f'../media/xor_moad_mlp_{synapse_list_crypto_sign}_bias_{b_idx}_sigmoid_gml_hd.png',\n",
    "        plot_resolution=160,#3200,#\n",
    "        save_plot=False,#True#\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del fig, synapse_list, bias_val_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1NHH75GvL58"
   },
   "source": [
    "## **Part 02 -- [Build a more complex neural network classifier using numpy](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/)**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvrXHvqr0Gge"
   },
   "source": [
    "### Display plots inline and change default figure size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCunSkU5zdlc"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdKwQYtJ0aFM"
   },
   "source": [
    "### Generate a dataset and create a plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X, y = sklearn.datasets.make_moons(200, noise=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8lZ9n910Slo"
   },
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    X[:,0],\n",
    "    X[:,1],\n",
    "    s=40,\n",
    "    c=y,\n",
    "    cmap=plt.cm.Spectral\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuc3DSeG4dcI"
   },
   "source": [
    "### Train the logistic regression classifier:\n",
    "\n",
    "The classification problem can be summarized as creating a boundary between the red and the blue dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ez4hJ_9S33j6"
   },
   "outputs": [],
   "source": [
    "linear_classifier = sklearn.linear_model.LogisticRegressionCV()\n",
    "linear_classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_gMrYdu5Ikg"
   },
   "source": [
    "### Visualize the logistic regression classifier output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUPuKKEB5Acv"
   },
   "outputs": [],
   "source": [
    "def plot_decision_boundary(prediction_function, X, y):\n",
    "    # Setting minimum and maximum values for giving the plot function some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, \\\n",
    "                   X[:, 0].max() + .5\n",
    "\n",
    "    y_min, y_max = X[:, 1].min() - .5, \\\n",
    "                   X[:, 1].max() + .5\n",
    "\n",
    "    h = 0.01\n",
    "\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(x_min, x_max, h), \\\n",
    "        np.arange(y_min, y_max, h)\n",
    "    )\n",
    "\n",
    "    # Predict the function value for the whole grid\n",
    "    Z = prediction_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plotting the contour and training examples\n",
    "    plt.contourf(\n",
    "        xx,\n",
    "        yy,\n",
    "        Z,\n",
    "        cmap=plt.colormaps.get_cmap('Spectral')\n",
    "    )\n",
    "    \n",
    "    plt.scatter(\n",
    "        X[:, 0],\n",
    "        X[:, 1],\n",
    "        c=y,\n",
    "        cmap=plt.colormaps.get_cmap('Spectral')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgC6Vm5072hj"
   },
   "source": [
    "### Plotting the decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pH_l5suh7w1f"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(\n",
    "    figsize=(6, 6), \n",
    "    layout='constrained'\n",
    ")\n",
    "\n",
    "spec = fig.add_gridspec(\n",
    "    ncols=1, \n",
    "    nrows=1\n",
    ")\n",
    "\n",
    "fig.add_subplot(\n",
    "    spec[0, 0]\n",
    ")\n",
    "\n",
    "plot_decision_boundary(\n",
    "    lambda x: linear_classifier.predict(x), \n",
    "    X, y\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "    'Logistic Regression'\n",
    ")\n",
    "\n",
    "plt.close()\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzS_LhQYCXnW"
   },
   "source": [
    "### Create a neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWz8z03d8LPg"
   },
   "outputs": [],
   "source": [
    "num_examples  = len(X) # training set size\n",
    "nn_input_dim  = 2      # input layer dimensionality\n",
    "nn_hdim       = 8      # hidden layer dimensionality\n",
    "nn_output_dim = 2      # output layer dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVpwJSH7Cub9"
   },
   "source": [
    "### Gradient descent parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2s7Ap7_FCsub"
   },
   "outputs": [],
   "source": [
    "epsilon = 0.01\n",
    "reg_lambda = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u36QZJmtGZhu"
   },
   "source": [
    "### Function that predicts the output of either 0 or 1:\n",
    "\n",
    "Forward pass across the model layers for creating the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOKPt6UIC5lm"
   },
   "outputs": [],
   "source": [
    "def forward_numpy_pass_tanh(\n",
    "        model,\n",
    "        x,\n",
    "        enable_training=False\n",
    "    ):\n",
    "    W1, b1 = model['W1'], model['b1']\n",
    "    W2, b2 = model['W2'], model['b2']\n",
    "\n",
    "    # Design a network with forward propagation\n",
    "    _, hidden_layer = get_activation_layer_output(\n",
    "        x,\n",
    "        W1,\n",
    "        b1,\n",
    "        np.tanh\n",
    "    )\n",
    "    \n",
    "    _, output_layer = get_activation_layer_output(\n",
    "        hidden_layer,\n",
    "        W2,\n",
    "        b2,\n",
    "        exp\n",
    "    )\n",
    "\n",
    "    if enable_training:\n",
    "        return [x, hidden_layer, output_layer]\n",
    "\n",
    "    return np.argmax(output_layer, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_output_layer_derivative(output_layer, labels):\n",
    "    output_layer[range(len(labels)), labels] -= 1\n",
    "    return output_layer\n",
    "\n",
    "def tanh_activation_layer_derivative(inp):\n",
    "    return (1 - np.power(inp, 2))\n",
    "\n",
    "def backprop_tanh(\n",
    "        x,\n",
    "        y,\n",
    "        model, \n",
    "        model_layers,\n",
    "        reg_lambda=None,\n",
    "        learning_rate=None,\n",
    "    ):\n",
    "    if reg_lambda is None:\n",
    "        reg_lambda = 1\n",
    "    if learning_rate is None:\n",
    "        learning_rate=1\n",
    "    W1, b1 = model['W1'], model['b1']\n",
    "    W2, b2 = model['W2'], model['b2']\n",
    "\n",
    "    probs = forward_numpy_pass_tanh(model, x, enable_training=True)[-1]\n",
    "    a1 = forward_numpy_pass_tanh(model, x, enable_training=True)[1]\n",
    "\n",
    "    # Backpropagation\n",
    "    delta3 = exp_output_layer_derivative(probs, y) # output layer derivative\n",
    "    dW2 = get_synapse_update(a1, delta3) # update values of 2nd synapse\n",
    "    db2 = get_bias_update(delta3) # update bias value for 2nd synapse\n",
    "\n",
    "    a1_derivative = tanh_activation_layer_derivative(a1)\n",
    "    a1_error = get_layer_error(delta3, W2)\n",
    "    delta2 = get_layer_gradient(a1_error, a1_derivative)\n",
    "    dW1 = get_synapse_update(X, delta2)\n",
    "    db1 = get_bias_update(delta2)\n",
    "\n",
    "    # Add regularization terms (b1 and b2 don't have regularization terms)\n",
    "    dW2 += reg_lambda * W2\n",
    "    dW1 += reg_lambda * W1\n",
    "\n",
    "    # Gradient descent parameter update\n",
    "    W1 += -learning_rate * dW1\n",
    "    b1 += -learning_rate * db1\n",
    "    W2 += -learning_rate * dW2\n",
    "    b2 += -learning_rate * db2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktuTAoe_C8xd"
   },
   "source": [
    "### Compute loss function on the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3PNCNomC65a"
   },
   "outputs": [],
   "source": [
    "def loss_function(model, x, y, reg_lambda=0.01):\n",
    "    W1, W2 = model['W1'],  model['W2']\n",
    "    probabilities = forward_numpy_pass_tanh(model, X)\n",
    "    num_examples = len(x)\n",
    "\n",
    "    # Calculating the loss function\n",
    "    corect_logprobs = - np.log(np.where(probabilities == y, 2, 1))\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "\n",
    "    # Adding the regulatization term to the loss function\n",
    "    data_loss += reg_lambda / 2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "\n",
    "    return 1. / num_examples * data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pzWRW7PHDRl"
   },
   "source": [
    "\n",
    "### Training function that learns the parameters for the neural network and returns the model:\n",
    "- nn_hdim: Number of nodes in the hidden layer\n",
    "- num_passes: Number of passes through the training data for gradient descent\n",
    "- print_loss: If True, print the loss every 1000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjZsvXVMG86T"
   },
   "outputs": [],
   "source": [
    "def build_model(nn_hdim):\n",
    "\n",
    "    # Initialize the parameters to random values. We need to learn these.\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)\n",
    "    b1 = np.zeros((1, nn_hdim))\n",
    "    W2 = np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim)\n",
    "    b2 = np.zeros((1, nn_output_dim))\n",
    "\n",
    "    # Assign new parameters to the model\n",
    "    model = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model(\n",
    "        X, \n",
    "        y, \n",
    "        model,\n",
    "        reg_lambda=None,\n",
    "        learning_rate=None,\n",
    "        num_passes=20000, \n",
    "        print_loss=False\n",
    "    ):\n",
    "    W1, b1 = model['W1'], model['b1']\n",
    "    W2, b2 = model['W2'], model['b2']\n",
    "\n",
    "    # Gradient descent. For each batch...\n",
    "    for i in range(0, num_passes):\n",
    "\n",
    "        # Forward propagation\n",
    "        model_layers = forward_numpy_pass_tanh(\n",
    "            model, \n",
    "            X, \n",
    "            enable_training=True\n",
    "        )\n",
    "\n",
    "        backprop_tanh(\n",
    "            X, \n",
    "            y,\n",
    "            model, \n",
    "            model_layers,\n",
    "            reg_lambda=reg_lambda,\n",
    "            learning_rate=learning_rate,\n",
    "        )\n",
    "\n",
    "        model['W1'], model['b1'] = W1, b1\n",
    "        model['W2'], model['b2'] = W2, b2\n",
    "\n",
    "        # Optionally print the loss.\n",
    "        # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n",
    "        if print_loss and (i + 1) % 1000 == 0:\n",
    "            print('Loss after iteration %i: %f ...' %(i + 1, loss_function(model, X, y)), end='')\n",
    "            print('', end='\\r')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qsFUIRzHwFF"
   },
   "source": [
    "### Build a model with 50-dimensional hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KsH3JKtVHni_"
   },
   "outputs": [],
   "source": [
    "model = build_model(50)\n",
    "\n",
    "model = train_model(\n",
    "\n",
    "    X, \n",
    "    \n",
    "    y, \n",
    "    \n",
    "    model, \n",
    "    \n",
    "    learning_rate=0.001, \n",
    "    \n",
    "    reg_lambda=0.01, \n",
    "    \n",
    "    print_loss=True\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faX_dAZlPFLb"
   },
   "source": [
    "### Plot the decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0psfV-xdPAqi"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(\n",
    "\n",
    "    figsize=(6, 6),\n",
    "\n",
    "    layout='constrained'\n",
    "\n",
    ")\n",
    "\n",
    "spec = fig.add_gridspec(\n",
    "\n",
    "    ncols=1,\n",
    "\n",
    "    nrows=1\n",
    "\n",
    ")\n",
    "\n",
    "fig.add_subplot(\n",
    "\n",
    "    spec[0, 0]\n",
    "\n",
    ")\n",
    "\n",
    "plot_decision_boundary(\n",
    "\n",
    "    lambda x: forward_numpy_pass_tanh(\n",
    "\n",
    "                  model,\n",
    "\n",
    "                  x\n",
    "\n",
    "              ),\n",
    "\n",
    "    X, \n",
    "    \n",
    "    y\n",
    "\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "\n",
    "    'Decision boundary for hidden layer size  50'\n",
    "\n",
    ")\n",
    "\n",
    "plt.close()\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del fig, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74-myoOdIQnE"
   },
   "source": [
    "### Visualizing the hidden layers with varying sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-cqJVZTHytt"
   },
   "outputs": [],
   "source": [
    "hidden_layer_dimensions = [1, 2, 3, 4, 5, 20, 50]\n",
    "\n",
    "fig = plt.figure(\n",
    "    figsize=(6, 36),\n",
    "    layout='constrained'\n",
    ")\n",
    "\n",
    "spec = fig.add_gridspec(\n",
    "    ncols=1,\n",
    "    nrows=len(hidden_layer_dimensions)\n",
    ")\n",
    "\n",
    "for i, nn_hdim in enumerate(hidden_layer_dimensions):\n",
    "    fig.add_subplot(\n",
    "        spec[i, 0]\n",
    "    )\n",
    "\n",
    "    plt.title(\n",
    "        'Hidden Layer size %d' % nn_hdim\n",
    "    )\n",
    "\n",
    "    model = build_model(nn_hdim)\n",
    "\n",
    "    plot_decision_boundary(\n",
    "        lambda x: forward_numpy_pass_tanh(\n",
    "                      model, x\n",
    "                  ),\n",
    "        X, y\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "del fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsC5K-7ugEKx"
   },
   "source": [
    "## **Part 03 -- Example illustrating the importance of[ learning rate](http://users.ics.aalto.fi/jhollmen/dippa/node22.html) in hyper-parameter tuning**:\n",
    "\n",
    "- Learning rate is a decreasing function of time.\n",
    "- Two forms that are commonly used are:\n",
    "    * 1) a linear function of time\n",
    "    * 2) a function that is inversely proportional to the time t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLCy6P16h8sa"
   },
   "source": [
    "### Create a very noisy synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBV41LHWIebT"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X, y = sklearn.datasets.make_moons(20000, noise=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "\n",
    "    X[:,0],\n",
    "\n",
    "    X[:,1],\n",
    "\n",
    "    s=40,\n",
    "\n",
    "    c=y,\n",
    "\n",
    "    cmap=plt.cm.Spectral\n",
    "\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEn2RWkgiGOp"
   },
   "outputs": [],
   "source": [
    "linear_classifier = sklearn.linear_model.LogisticRegressionCV()\n",
    "linear_classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3siSKYtLiPhg"
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary(lambda x: linear_classifier.predict(x), X, y)\n",
    "plt.title('Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del linear_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "itPMnFcLjhqb"
   },
   "outputs": [],
   "source": [
    "num_examples = len(X) # training set size\n",
    "nn_input_dim = 2      # input layer dimensionality\n",
    "nn_output_dim = 2     # output layer dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0kG4EwVOjnTy"
   },
   "outputs": [],
   "source": [
    "epsilon = 0.01        # learning rate for gradient descent\n",
    "reg_lambda = 0.01     # regularization strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yb3E1Z-2iUOU"
   },
   "outputs": [],
   "source": [
    "model = build_model(5)\n",
    "\n",
    "model = train_model(\n",
    "\n",
    "    X,\n",
    "\n",
    "    y,\n",
    "\n",
    "    model,\n",
    "\n",
    "    learning_rate=epsilon,\n",
    "\n",
    "    reg_lambda=reg_lambda,\n",
    "\n",
    "    print_loss=True\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqDy-MxBhPv_"
   },
   "source": [
    "### Plotting output of the model that learned features to seperate the noisy data, given a learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jI6ZFYrmg66w"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(\n",
    "\n",
    "    figsize=(6, 6),\n",
    "\n",
    "    layout='constrained'\n",
    "\n",
    ")\n",
    "\n",
    "spec = fig.add_gridspec(\n",
    "\n",
    "    ncols=1,\n",
    "\n",
    "    nrows=1\n",
    "\n",
    ")\n",
    "\n",
    "fig.add_subplot(\n",
    "\n",
    "    spec[0, 0]\n",
    "\n",
    ")\n",
    "\n",
    "plot_decision_boundary(\n",
    "\n",
    "    lambda x: forward_numpy_pass_tanh(\n",
    "\n",
    "                  model,\n",
    "\n",
    "                  x\n",
    "\n",
    "              ),\n",
    "\n",
    "    X, y\n",
    "\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "\n",
    "    'Decision boundary for hidden layer size  50'\n",
    "\n",
    ")\n",
    "\n",
    "plt.close()\n",
    "\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xakIXy_qhZzc"
   },
   "source": [
    "### Training using a lower learning rate such that the neural network re-starts learning different set of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SdxhhV16i4R_"
   },
   "outputs": [],
   "source": [
    "epsilon = 1e-6\n",
    "reg_lambda = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NEvPH7E8l-lI"
   },
   "outputs": [],
   "source": [
    "model = build_model(5)\n",
    "\n",
    "model = train_model(\n",
    "\n",
    "    X,\n",
    "\n",
    "    y,\n",
    "\n",
    "    model,\n",
    "\n",
    "    learning_rate=epsilon,\n",
    "\n",
    "    reg_lambda=reg_lambda,\n",
    "\n",
    "    print_loss=True\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r98-w9LWhjj0"
   },
   "source": [
    "### Plotting the decision boundary layer generated by the same neural network architecture, trained with a lower learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YfkTEZaFnlwY"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(\n",
    "\n",
    "    figsize=(6, 6),\n",
    "\n",
    "    layout='constrained'\n",
    "\n",
    ")\n",
    "\n",
    "spec = fig.add_gridspec(\n",
    "\n",
    "    ncols=1,\n",
    "    nrows=1\n",
    "\n",
    ")\n",
    "\n",
    "fig.add_subplot(\n",
    "\n",
    "    spec[0, 0]\n",
    "\n",
    ")\n",
    "\n",
    "plot_decision_boundary(\n",
    "\n",
    "    lambda x: forward_numpy_pass_tanh(\n",
    "    \n",
    "                  model,\n",
    "                  x\n",
    "\n",
    "              ),\n",
    "\n",
    "    X, y\n",
    "\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "\n",
    "    'Decision boundary for hidden layer size  50'\n",
    "\n",
    ")\n",
    "\n",
    "plt.close()\n",
    "\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del fig, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "plt.plot(); plt.show(); plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 04 -- Multi-layered perceptron (MLP) using MOAD computation to classify noisy data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "\n",
    "    X.T.shape,\n",
    "\n",
    "    y.T.shape\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim, output_dim = X.shape[-1], 1\n",
    "\n",
    "print(\n",
    "\n",
    "    '\\n',\n",
    "\n",
    "    'Input dimension: ',\n",
    "\n",
    "    input_dim,\n",
    "\n",
    "    '\\n',\n",
    "\n",
    "    'Output dimension: ',\n",
    "\n",
    "    output_dim\n",
    "\n",
    ")\n",
    "\n",
    "dense_units_list = [input_dim, 50, output_dim]\n",
    "\n",
    "bias_val_list = [0.1, 0.1]\n",
    "\n",
    "input_data, output_data = X, np.asarray([y]).T\n",
    "\n",
    "print(\n",
    "\n",
    "    '\\n',\n",
    "\n",
    "    'Shape of the input data: ',\n",
    "\n",
    "    input_data.shape,\n",
    "\n",
    "    '\\n',\n",
    "\n",
    "    'Shape of the output data: ',\n",
    "\n",
    "    output_data.shape\n",
    "\n",
    ")\n",
    "\n",
    "epsilon = 1e-8\n",
    "epsilon_regularization_factor = 100\n",
    "num_epochs = 10\n",
    "activation_fn = sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the effects of learning rate changes on the model training performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_hi = 0.5\n",
    "\n",
    "synapse_list_hi_lr = create_synapse(\n",
    "\n",
    "    dense_units_list,\n",
    "\n",
    "    bias_val_list\n",
    "\n",
    ")\n",
    "\n",
    "activation_fn_list = [\n",
    "    \n",
    "    activation_fn for _ in synapse_list_hi_lr\n",
    "\n",
    "]\n",
    "\n",
    "synapse_list_hi_lr,                 \\\n",
    "bias_val_list_hi_lr,                \\\n",
    "order_weights_list_hi,              \\\n",
    "loss_col_hi_lr,                     \\\n",
    "enable_backprop_activation_nesting, \\\n",
    "enable_moad_forward_pass,           \\\n",
    "enable_strict_derivative_order,     \\\n",
    "enable_derivative_normalization = moad_mlp_train(\n",
    "\n",
    "    input_data,\n",
    "    output_data,\n",
    "    synapse_list_hi_lr,\n",
    "    bias_val_list,\n",
    "    activation_fn_list,\n",
    "    learning_rate_hi,\n",
    "    epsilon,\n",
    "    epsilon_regularization_factor,\n",
    "    num_epochs,\n",
    "    order=4,\n",
    "    order_weights_list=None,\n",
    "    loss_plateau_patience=None,\n",
    "    num_auto_diff_steps=1,\n",
    "    remanan_stochastic_expansion=0.1,\n",
    "    return_best_synapse=True,\n",
    "    enable_backprop_activation_nesting=False,\n",
    "    enable_moad_forward_pass=True,\n",
    "    enable_strict_derivative_order=False,\n",
    "    verbose=True\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_lo = 0.1\n",
    "\n",
    "synapse_list_lo_lr = create_synapse(\n",
    "\n",
    "    dense_units_list,\n",
    "\n",
    "    bias_val_list\n",
    "\n",
    ")\n",
    "\n",
    "activation_fn_list = [\n",
    "\n",
    "    activation_fn for _ in synapse_list_lo_lr\n",
    "\n",
    "]\n",
    "\n",
    "synapse_list_lo_lr,                 \\\n",
    "bias_val_list_lo_lr,                \\\n",
    "order_weights_list_lo,              \\\n",
    "loss_col_lo_lr,                     \\\n",
    "enable_backprop_activation_nesting, \\\n",
    "enable_moad_forward_pass,           \\\n",
    "enable_strict_derivative_order,     \\\n",
    "enable_derivative_normalization = moad_mlp_train(\n",
    "\n",
    "    input_data,\n",
    "    output_data,\n",
    "    synapse_list_lo_lr,\n",
    "    bias_val_list,\n",
    "    activation_fn_list,\n",
    "    learning_rate_lo,\n",
    "    epsilon,\n",
    "    epsilon_regularization_factor,\n",
    "    num_epochs,\n",
    "    order=4,\n",
    "    order_weights_list=None,\n",
    "    loss_plateau_patience=None,\n",
    "    num_auto_diff_steps=1,\n",
    "    remanan_stochastic_expansion=0.1,\n",
    "    return_best_synapse=True,\n",
    "    enable_backprop_activation_nesting=False,\n",
    "    enable_moad_forward_pass=True,\n",
    "    enable_strict_derivative_order=False,\n",
    "    verbose=True\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del input_data, output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "\n",
    "    loss_col_hi_lr\n",
    "\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "\n",
    "    'Learning rate: %f '%(learning_rate_hi)\n",
    "\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "\n",
    "    loss_col_lo_lr\n",
    "\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "\n",
    "    'Learning rate: %f '%(learning_rate_lo)\n",
    "\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the decision boundary layer generated by the MOAD MLP model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(\n",
    "\n",
    "    figsize=(6, 6),\n",
    "    layout='constrained'\n",
    "\n",
    ")\n",
    "\n",
    "spec = fig.add_gridspec(\n",
    "\n",
    "    ncols=1,\n",
    "    nrows=1\n",
    "\n",
    ")\n",
    "\n",
    "fig.add_subplot(\n",
    "\n",
    "    spec[0, 0]\n",
    "\n",
    ")\n",
    "\n",
    "plot_decision_boundary(\n",
    "\n",
    "    lambda x: moad_mlp_predict(\n",
    "\n",
    "                  x,\n",
    "                  synapse_list_hi_lr,\n",
    "                  bias_val_list_hi_lr,\n",
    "                  activation_fn_list,\n",
    "                  order=4,\n",
    "                  order_weights_list=order_weights_list_hi, \n",
    "                  enable_moad_forward_pass=True,\n",
    "                  enable_strict_derivative_order=False\n",
    "\n",
    "              ),\n",
    "\n",
    "    X, y\n",
    "\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "\n",
    "    'Decision boundary for MOAD MLP trained with learning rate: %f '%(learning_rate_hi)\n",
    "\n",
    ")\n",
    "\n",
    "plt.close()\n",
    "\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del fig, synapse_list_hi_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bChn1I8SKvia"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(\n",
    "\n",
    "    figsize=(6, 6),\n",
    "    layout='constrained'\n",
    "\n",
    ")\n",
    "\n",
    "spec = fig.add_gridspec(\n",
    "\n",
    "    ncols=1,\n",
    "    nrows=1\n",
    "\n",
    ")\n",
    "\n",
    "fig.add_subplot(\n",
    "\n",
    "    spec[0, 0]\n",
    "\n",
    ")\n",
    "\n",
    "plot_decision_boundary(\n",
    "\n",
    "    lambda x: moad_mlp_predict(\n",
    "\n",
    "                  x,\n",
    "                  synapse_list_lo_lr,\n",
    "                  bias_val_list_lo_lr,\n",
    "                  activation_fn_list,\n",
    "                  order=4,\n",
    "                  order_weights_list=order_weights_list_lo,  \n",
    "                  enable_moad_forward_pass=True,\n",
    "                  enable_strict_derivative_order=False\n",
    "\n",
    "              ),\n",
    "\n",
    "    X, y\n",
    "\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "\n",
    "    f'Decision boundary for MOAD MLP trained with learning rate: {learning_rate_lo:.2e} '\n",
    "\n",
    ")\n",
    "\n",
    "plt.close()\n",
    "\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del fig, synapse_list_lo_lr\n",
    "del X, y, bias_val_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 05 -- Time-series MOAD MLP model for SARS-CoV2 case forecasting:**\n",
    "\n",
    "A time-series data forecasting model using the case statistics of SARS-CoV2 over a multi-year time period across a very large population base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Configuration:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG(object):\n",
    "    GIT_PROVIDER = 'GitHub'\n",
    "    \n",
    "    GIT_REPO_DIR = os.path.realpath(f'{ROOT_DIR}/{GIT_PROVIDER}/MoadComputer/covid19-visualization/')\n",
    "    SARS_COV2_STATS_CSV_DIR = os.path.realpath(f'{GIT_REPO_DIR}/data/Coronavirus_stats/India/archives/')\n",
    "\n",
    "    GIT_SOURCE = 'rahulremanan'\n",
    "    GIT_DIR = 'python_tutorial'\n",
    "    GIT_REPO = f'{GIT_SOURCE}/{GIT_DIR}/'\n",
    "    GIT_DATA_SOURCE = 'MoadComputer'\n",
    "    GIT_DATA_DIR = 'covid19-visualization'\n",
    "    GIT_DATA_REPO = f'{GIT_DATA_SOURCE}/{GIT_DATA_DIR}/'\n",
    "\n",
    "    GIT_CONTENT_URL, GIT_FILE_URL_PREFIX, GIT_DATA_URL = None, None, None \n",
    "    if GIT_PROVIDER.lower() == 'github':\n",
    "        GIT_CONTENT_URL = f'https://raw.{GIT_PROVIDER.lower()}usercontent.com/{GIT_REPO}'\n",
    "        GIT_FILE_URL_POINTER = '/refs/heads/master/Fundamentals_of_deep-learning/weights/'\n",
    "        GIT_FILE_URL_PREFIX = f'{GIT_CONTENT_URL}{GIT_FILE_URL_POINTER}'\n",
    "        GIT_DATA_URL = f'https://{GIT_PROVIDER.lower()}.com/{GIT_DATA_REPO}'\n",
    "\n",
    "    CSV_DATA_HEADER = ['ID', 'state', 'total_cases', 'discharged', 'deaths', 'active_cases']\n",
    "\n",
    "    CRYPTO_DIGEST_SIZE = 64\n",
    "    CRYPTO_HASH_ALGORITHM = 'blake2b'\n",
    "    SYNAPSE_LIST_CRYPTO_SIGN = '6a639eb7ba061902da15d272a17acfb52f917b9f849319f7650217640c9374e5cb78cb2b5f42fd199f93775ef951a7cc2f7d0a866ac7975ae2000fcd1b5d1b7e'\n",
    "    SYNAPSE_LIST_FILE_PREFIX = 'synapse_list_sarscov2_mlp'\n",
    "    SYNAPSE_LIST_FILENAME = f'{SYNAPSE_LIST_FILE_PREFIX}_{SYNAPSE_LIST_CRYPTO_SIGN}.pkl'\n",
    "    SYNAPSE_LIST_FILE = os.path.realpath(f'{ROOT_DIR}/{SYNAPSE_LIST_FILENAME}')\n",
    "    SYNAPSE_FILE_URL = f'{GIT_FILE_URL_PREFIX}{SYNAPSE_LIST_FILENAME}'\n",
    "\n",
    "    LOAD_SAVED_WEIGHTS = True\n",
    "\n",
    "    LEARNING_RATE = 0.7 #1e-3\n",
    "    EPSILON = 1e-8\n",
    "    EPSILON_REGULARIZATION_FACTOR = 100\n",
    "    EPOCHS = 16\n",
    "    ACTIVATION_FUNCTION = sigmoid\n",
    "    MOAD_MLP_DENSE_UNITS_LIST = [24, 16, 8]\n",
    "    MOAD_MLP_BIAS_VAL_LIST = [0.1, 0.1, 0.1]\n",
    "    MOAD_MLP_ORDER = 4\n",
    "    NUM_AUTO_DIFF_STEPS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_syn_file = str(os.path.realpath(f'../weights/{CONFIG.SYNAPSE_LIST_FILENAME}'))\n",
    "if os.path.exists(saved_syn_file) and CONFIG.SYNAPSE_LIST_CRYPTO_SIGN is not None:\n",
    "    !sudo cp {saved_syn_file} ./\n",
    "elif (CONFIG.LOAD_SAVED_WEIGHTS and not os.path.exists(os.path.join('./', CONFIG.SYNAPSE_LIST_FILENAME))) and CONFIG.SYNAPSE_LIST_CRYPTO_SIGN is not None:\n",
    "    !curl -L {CONFIG.SYNAPSE_FILE_URL} --output {CONFIG.SYNAPSE_LIST_FILENAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_repo_dir_name = str(os.path.realpath(f'../../../../{CONFIG.GIT_DATA_REPO}'))\n",
    "target_dir = os.path.realpath(f'./{CONFIG.GIT_PROVIDER}/{CONFIG.GIT_DATA_SOURCE}')\n",
    "if os.path.exists(local_repo_dir_name) and not(os.path.exists(target_dir)):\n",
    "    os.makedirs(target_dir)\n",
    "    !sudo cp -r {local_repo_dir_name} {target_dir}\n",
    "elif not os.path.exists(\n",
    "\n",
    "    os.path.join(\n",
    "\n",
    "        CONFIG.GIT_REPO_DIR,\n",
    "\n",
    "        'app/'\n",
    "\n",
    "    )\n",
    "\n",
    "):\n",
    "\n",
    "    !git clone {CONFIG.GIT_DATA_URL} {CONFIG.GIT_REPO_DIR}\n",
    "# else:\n",
    "#     !git pull {CONFIG.GIT_REPO_DIR} origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "\n",
    "    'Reading csv files from: ',\n",
    "\n",
    "    os.path.realpath(\n",
    "\n",
    "        f'{CONFIG.SARS_COV2_STATS_CSV_DIR}/*.csv'\n",
    "\n",
    "    )\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = os.path.realpath(\n",
    "\n",
    "    f'{CONFIG.SARS_COV2_STATS_CSV_DIR}/*.csv'\n",
    "\n",
    ")\n",
    "\n",
    "sorted_csv_file_list = sorted(\n",
    "\n",
    "    glob.glob(\n",
    "\n",
    "        csv_dir\n",
    "\n",
    "    )\n",
    "\n",
    ")\n",
    "\n",
    "assert len(sorted_csv_file_list) > 0, f'No *.csv files detected in: {CONFIG.SARS_COV2_STATS_CSV_DIR} ...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create training data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_csv_file = sorted_csv_file_list[0]\n",
    "\n",
    "print(\n",
    "    f'Reading csv file header information from: {info_csv_file} ...'\n",
    ")\n",
    "\n",
    "with open(info_csv_file, 'r') as f:\n",
    "   data = (f.read())\n",
    "\n",
    "header_info = data.split('\\n')[0].split(',')\n",
    "\n",
    "assert header_info == CONFIG.CSV_DATA_HEADER, f'Headers of the csv file: {info_csv_file} does not match those provided in the configuration ...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'Last updated case statistics file in the repo: {sorted_csv_file_list[-1]} ...'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_idx_dict, data_labels_dict = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_csv_data = np.genfromtxt(\n",
    "\n",
    "    info_csv_file,\n",
    "    dtype='str',\n",
    "    delimiter=',',\n",
    "    usemask=False,\n",
    "    skip_header=True\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data handling functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_stats(data, idx_list=None):\n",
    "    if idx_list is not None:\n",
    "        return [\n",
    "            \n",
    "            int(data[idx]) for idx in idx_list\n",
    "        \n",
    "        ]\n",
    "\n",
    "    return [\n",
    "        \n",
    "        int(data[2]), \n",
    "        int(data[3]), \n",
    "        int(data[4]), \n",
    "        int(data[5])\n",
    "\n",
    "    ]\n",
    "\n",
    "def get_empty_csv_stats(idx_list=None):\n",
    "    if idx_list is not None:\n",
    "        return [\n",
    "            \n",
    "            int(0) for idx in idx_list\n",
    "\n",
    "        ]\n",
    "\n",
    "    return [\n",
    "        \n",
    "        int(0), \n",
    "        int(0), \n",
    "        int(0), \n",
    "        int(0)\n",
    "    \n",
    "    ]\n",
    "\n",
    "def state_name_correction(inp_str):\n",
    "    if inp_str == 'Telengana':\n",
    "        return 'Telangana'\n",
    "\n",
    "    if inp_str == 'Himanchal Pradesh':\n",
    "        return 'Himachal Pradesh'\n",
    "\n",
    "    if inp_str == 'Karanataka':\n",
    "        return 'Karnataka'\n",
    "\n",
    "    return inp_str\n",
    "\n",
    "def get_state_wide_data_from_file(\n",
    "        fname, \n",
    "        state_idx_dict,\n",
    "        data_labels_dict,\n",
    "        data_type_list=None\n",
    "    ):\n",
    "\n",
    "    idx_list = None\n",
    "\n",
    "    if data_type_list is not None:\n",
    "\n",
    "        assert isinstance(data_type_list, list), f'Expected the data type list to be a list, instead received: {type(data_type_list)} ...'\n",
    "        \n",
    "        idx_list = []\n",
    "\n",
    "        for data_type in data_type_list:\n",
    "\n",
    "            assert data_type in CONFIG.CSV_DATA_HEADER, f'No {data_type} is available. Available options are: total_cases, discharged, deaths, active_cases ...'\n",
    "            \n",
    "            idx_list.append(\n",
    "\n",
    "                data_labels_dict[data_type]\n",
    "\n",
    "            )\n",
    "\n",
    "    state_data = [\n",
    "\n",
    "        [] for _ in state_idx_dict.keys()\n",
    "\n",
    "    ]\n",
    "\n",
    "    csv_data = np.genfromtxt(\n",
    "\n",
    "        fname,\n",
    "        dtype='str',\n",
    "        delimiter=',',\n",
    "        usemask=False, \n",
    "        skip_header=True\n",
    "\n",
    "    )\n",
    "\n",
    "    state_name_list = []\n",
    "\n",
    "    for data in csv_data:\n",
    "\n",
    "        state_name = state_name_correction(str(data[1]))\n",
    "\n",
    "        state_name_list.append(state_name)\n",
    "\n",
    "        state_idx = state_idx_dict[state_name]\n",
    "\n",
    "        state_data[state_idx - 1].append(get_csv_stats(data, idx_list))\n",
    "\n",
    "    missing_states = set(list(state_idx_dict.keys())) - set(state_name_list)\n",
    "\n",
    "    for missing_state in missing_states:\n",
    "\n",
    "        state_idx = state_idx_dict[missing_state]\n",
    "\n",
    "        state_data[state_idx - 1].append(get_empty_csv_stats(idx_list))\n",
    "\n",
    "    return np.array(state_data)\n",
    "\n",
    "def get_state_wide_data_array(\n",
    "        csv_file_list,\n",
    "        data_labels_dict,\n",
    "        data_type_list=None\n",
    "    ):\n",
    "    idx_list = None\n",
    "    if data_type_list is not None:\n",
    "        assert isinstance(data_type_list, list), f'Expected the data type list to be a list, instead received: {type(data_type_list)} ...'\n",
    "        idx_list = []\n",
    "        for data_type in data_type_list:\n",
    "            assert data_type in CONFIG.CSV_DATA_HEADER, f'No {data_type} is available. Available options are: total_cases, discharged, deaths, active_cases ...'\n",
    "            idx_list.append(\n",
    "                data_labels_dict[data_type]\n",
    "            )\n",
    "    \n",
    "    state_data = [[] for _ in state_idx_dict.keys()]\n",
    "\n",
    "    for fname in csv_file_list:\n",
    "        csv_data = np.genfromtxt(\n",
    "\n",
    "            fname,\n",
    "            dtype='str',\n",
    "            delimiter=',',\n",
    "            usemask=False,\n",
    "            skip_header=True\n",
    "\n",
    "        )\n",
    "\n",
    "        state_name_list = []\n",
    "        for data in csv_data:\n",
    "            state_name = state_name_correction(str(data[1]))\n",
    "            state_name_list.append(state_name)\n",
    "            state_idx = state_idx_dict[state_name]\n",
    "            state_data[state_idx - 1].append(get_csv_stats(data, idx_list))\n",
    "    \n",
    "        missing_states = set(list(state_idx_dict.keys())) - set(state_name_list)\n",
    "    \n",
    "        for missing_state in missing_states:\n",
    "            state_idx = state_idx_dict[missing_state]\n",
    "            state_data[state_idx - 1].append(get_empty_csv_stats(idx_list))\n",
    "\n",
    "    return np.array(state_data)\n",
    "\n",
    "def get_rolled_data(\n",
    "        inp_arr,\n",
    "        window_size=1,\n",
    "        axis=1,\n",
    "        epsilon=1e-16,\n",
    "        enable_zero_division_with_epsilon=True\n",
    "    ):\n",
    "\n",
    "    assert (type(epsilon) == float or type(epsilon) == int) and epsilon != 0, f'Expected epsilon value to be a non-zero real number float, instead received: {epsilon} ...'\n",
    "\n",
    "    if enable_zero_division_with_epsilon: inp_arr = np.where(inp_arr == 0, epsilon, inp_arr) \n",
    "\n",
    "    rolled_arr = inp_arr / np.roll(inp_arr, -(window_size + 1), axis=axis)\n",
    "\n",
    "    return rolled_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in info_csv_data:\n",
    "\n",
    "    state_idx  = int(data[0])\n",
    "\n",
    "    state_name = str(data[1])\n",
    "\n",
    "    state_idx_dict.update(\n",
    "\n",
    "        {\n",
    "\n",
    "            state_name : state_idx\n",
    "\n",
    "        }\n",
    "\n",
    "    )\n",
    "\n",
    "for idx, header in enumerate(CONFIG.CSV_DATA_HEADER):\n",
    "\n",
    "    data_labels_dict.update({header : idx})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_name_cosine_encoder(state_idx_dict): \n",
    "    return min_max_scaler(\n",
    "\n",
    "        np.array(\n",
    "\n",
    "            [\n",
    "                \n",
    "                np.cos(\n",
    "\n",
    "                    (state_idx_dict[s] / len(state_idx_dict)) * math.pi\n",
    "\n",
    "                ) for s in state_idx_dict\n",
    "            \n",
    "            ]\n",
    "\n",
    "        )\n",
    "\n",
    "    )\n",
    "\n",
    "def reverse_cosine_encoder(n):\n",
    "    return min_max_scaler(\n",
    "\n",
    "        np.array(\n",
    "\n",
    "            [\n",
    "\n",
    "                np.cos(\n",
    "                    \n",
    "                    math.pi * ((n - i) / n)\n",
    "                \n",
    "                ) for i in range(n)\n",
    "\n",
    "            ]\n",
    "\n",
    "        )\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_encoded_state_data(\n",
    "        state_data,\n",
    "        state_idx_dict,\n",
    "        state_name_cos_enc_list,\n",
    "        date_cos_enc_list\n",
    "    ):\n",
    "\n",
    "    cos_enc_state_data_list = []\n",
    "\n",
    "    for i in range(state_data.shape[1]):\n",
    "\n",
    "        cos_enc_state_data = []\n",
    "\n",
    "        for state in state_idx_dict:\n",
    "\n",
    "            idx = state_idx_dict[state] - 1\n",
    "\n",
    "            cos_enc_state_data.append(\n",
    "\n",
    "                np.concat(\n",
    "\n",
    "                    [\n",
    "\n",
    "                        state_data[ : , i, : ][idx],\n",
    "\n",
    "                        np.array(\n",
    "\n",
    "                               [\n",
    "\n",
    "                                   state_name_cos_enc_list[idx],\n",
    "\n",
    "                                   date_cos_enc_list[i]\n",
    "\n",
    "                               ]\n",
    "\n",
    "                           )\n",
    "\n",
    "                    ],\n",
    "\n",
    "                    axis=0\n",
    "\n",
    "                )\n",
    "\n",
    "            )\n",
    "\n",
    "        cos_enc_state_data_list.append(cos_enc_state_data)\n",
    "\n",
    "    output_shape = (*state_data.shape[ : 2], state_data.shape[-1] + 2)\n",
    "\n",
    "    cos_enc_out  = np.array(\n",
    "\n",
    "        cos_enc_state_data_list\n",
    "\n",
    "    ).flatten().reshape(\n",
    "\n",
    "        output_shape\n",
    "\n",
    "    )\n",
    "\n",
    "    return cos_enc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_data = get_state_wide_data_array(\n",
    "\n",
    "    sorted_csv_file_list,\n",
    "\n",
    "    data_labels_dict\n",
    "\n",
    ")\n",
    "\n",
    "state_total_cases_data = get_state_wide_data_array(\n",
    "\n",
    "    sorted_csv_file_list,\n",
    "\n",
    "    data_labels_dict,\n",
    "\n",
    "    data_type_list=['total_cases']\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cos_enc_list = reverse_cosine_encoder(\n",
    "    \n",
    "    state_data.shape[1]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_name_cos_enc_list = state_name_cosine_encoder(\n",
    "\n",
    "    state_idx_dict\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "\n",
    "    *state_data.shape[ : 2], \n",
    "\n",
    "    state_data.shape[-1] + 2\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_enc_state_data_list = get_cosine_encoded_state_data(\n",
    "\n",
    "    state_data,\n",
    "\n",
    "    state_idx_dict,\n",
    "\n",
    "    state_name_cos_enc_list,\n",
    "\n",
    "    date_cos_enc_list\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \n",
    "    state_data.shape,\n",
    "\n",
    "    cos_enc_state_data_list.shape\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolled_data_list = [\n",
    "\n",
    "    get_rolled_data(\n",
    "\n",
    "        cos_enc_state_data_list,\n",
    "\n",
    "        window_size=i\n",
    "\n",
    "    ) for i in range(1, window_size + 1)\n",
    "\n",
    "]\n",
    "\n",
    "rolled_data_labels = get_rolled_data(\n",
    "\n",
    "    state_total_cases_data,\n",
    "\n",
    "    window_size=window_size + 1\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = rolled_data_labels[ : , : -4, : ].shape[1]\n",
    "print(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(\n",
    "\n",
    "    [\n",
    "\n",
    "        np.nan_to_num(\n",
    "\n",
    "            np.concatenate(\n",
    "\n",
    "                rolled_data_list,\n",
    "\n",
    "                axis=-1\n",
    "\n",
    "            )[ : , : -4, : ][ : , i, : ].flatten(),\n",
    "\n",
    "            copy=True,\n",
    "\n",
    "            nan=-1.0,\n",
    "\n",
    "            posinf=12.0,\n",
    "\n",
    "            neginf=-12.0\n",
    "\n",
    "        ) for i in range(num_samples)\n",
    "\n",
    "    ]\n",
    "\n",
    ")\n",
    "\n",
    "Y = np.array(\n",
    "\n",
    "    [\n",
    "\n",
    "        np.nan_to_num(\n",
    "\n",
    "            rolled_data_labels[ : , : -4, : ][ : , i, : ].flatten(),\n",
    "\n",
    "            copy=True,\n",
    "\n",
    "            nan=-1.0,\n",
    "\n",
    "            posinf=12.0,\n",
    "\n",
    "            neginf=-12.0\n",
    "\n",
    "        ) for i in range(num_samples)\n",
    "\n",
    "    ]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "\n",
    "    X.shape,\n",
    "\n",
    "    Y.shape\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Define the MOAD MLP model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim, output_dim = X.shape[-1], Y.shape[-1]\n",
    "output_dense_bias_val = 0.1\n",
    "\n",
    "CONFIG.MOAD_MLP_DENSE_UNITS_LIST.insert(0, input_dim)\n",
    "CONFIG.MOAD_MLP_DENSE_UNITS_LIST.append(output_dim)\n",
    "CONFIG.MOAD_MLP_BIAS_VAL_LIST.append(output_dense_bias_val)\n",
    "\n",
    "print(\n",
    "\n",
    "    '\\n',\n",
    "\n",
    "    'Input dimension: ',\n",
    "\n",
    "    input_dim,\n",
    "\n",
    "    '\\n',\n",
    "\n",
    "    'Output dimension: ',\n",
    "\n",
    "    output_dim\n",
    "\n",
    ")\n",
    "\n",
    "input_data, output_data = X, Y\n",
    "\n",
    "print(\n",
    "\n",
    "    '\\n',\n",
    "\n",
    "    'Shape of the input data: ',\n",
    "\n",
    "    input_data.shape,\n",
    "\n",
    "    '\\n',\n",
    "\n",
    "    'Shape of the output data: ',\n",
    "\n",
    "    output_data.shape\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synapse_list = create_synapse(\n",
    "\n",
    "    CONFIG.MOAD_MLP_DENSE_UNITS_LIST,\n",
    "    CONFIG.MOAD_MLP_BIAS_VAL_LIST\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved synapses:\n",
    "\n",
    "Load the saved synapses and perform a basic cryptographic signature check before using these saved files for training of the forecasting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG.SYNAPSE_LIST_CRYPTO_SIGN is not None and os.path.exists(CONFIG.SYNAPSE_LIST_FILE):\n",
    "\n",
    "    [synapse_list_saved, bias_val_list_saved,  order_weights_list_saved] = load_pickle(\n",
    "\n",
    "        CONFIG.SYNAPSE_LIST_FILE,\n",
    "        compression=True\n",
    "\n",
    "    )\n",
    "\n",
    "    synapse_list_saved_crypto_sign = hash_generator(\n",
    "\n",
    "        str([synapse_list_saved, bias_val_list_saved,  order_weights_list_saved] ),\n",
    "        digest_size=int(CONFIG.CRYPTO_DIGEST_SIZE),\n",
    "        hash_algorithm=CONFIG.CRYPTO_HASH_ALGORITHM,\n",
    "        verbose=False\n",
    "\n",
    "    )\n",
    "\n",
    "    assert synapse_list_saved_crypto_sign == CONFIG.SYNAPSE_LIST_FILENAME.replace(CONFIG.SYNAPSE_LIST_FILE_PREFIX + '_', '').replace('.pkl', ''), f'Cryptographic integrity check failed on the loaded synapse file: {CONFIG.SYNAPSE_LIST_FILE} ...'\n",
    "\n",
    "    [synapse_list, bias_val_list,  order_weights_list] = copy.deepcopy(\n",
    "        \n",
    "        [\n",
    "\n",
    "            synapse_list_saved, \n",
    "\n",
    "            bias_val_list_saved,  \n",
    "\n",
    "            order_weights_list_saved\n",
    "\n",
    "        ] \n",
    "\n",
    "    )\n",
    "\n",
    "    print(\n",
    "\n",
    "        f'Loaded saved synapse list from: {CONFIG.SYNAPSE_LIST_FILE} ...'\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load prediction data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_data = get_state_wide_data_from_file(\n",
    "\n",
    "    sorted_csv_file_list[-1],\n",
    "\n",
    "    state_idx_dict,\n",
    "\n",
    "    data_labels_dict,\n",
    "\n",
    "    data_type_list=['total_cases']\n",
    "\n",
    ")\n",
    "\n",
    "st_data = st_data.reshape(len(state_idx_dict), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Curriculum learning strategy for training the forecasting model:**\n",
    "\n",
    "**Curriculum learning** is a training strategy of exposing the large corpus of data in chunks that get progressively bigger as the model training progresses, eventually encompassing the entire corpus; such that the likelihood of overfitting is reduced.\n",
    "\n",
    "This strategy is successfully utilized in handling complex numerical datasets, such as for the training of **physics informed neural networks (PINNs)** to model numerical physical models using neural networks. \n",
    "\n",
    "This training method is particularly effective in learning complex features of time-series datasets, such as public health datasets used in the modeling of infectious disease dynamics; including the SARS-CoV2 case load forecasting at a very large population level, that is used in this notebook.\n",
    "\n",
    "The key motivation for implementing the curriculum learning strategy in this SARS-CoV2 case forecasting model is to help the model learn complex features with the dataset, such as: the exponential growth phase of the spread of infections during the early phases of the population spread and the subsequent slow down in the spread, characterized by the plateau phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_loss_col = []\n",
    "\n",
    "bool_opt_combinations = [\n",
    "\n",
    "    [True, False],\n",
    "\n",
    "    #[False, True],\n",
    "\n",
    "    #[True,  True]\n",
    "\n",
    "]\n",
    "\n",
    "train_data_idx_list_00 = [i for i in range(36, 1095)]\n",
    "\n",
    "train_data_idx_list_01 = [i for i in range(num_samples - 101, num_samples - 1)]\n",
    "\n",
    "train_data_idx_list_02 = [i for i in range(num_samples - 36, num_samples - 1)]\n",
    "\n",
    "train_data_idx_list = train_data_idx_list_00 + train_data_idx_list_01 + train_data_idx_list_02\n",
    "                         \n",
    "random.SystemRandom().shuffle(train_data_idx_list_01)\n",
    "\n",
    "random.SystemRandom().shuffle(train_data_idx_list_02)\n",
    "\n",
    "random.SystemRandom().shuffle(train_data_idx_list)\n",
    "\n",
    "train_data_range_list = [\n",
    "\n",
    "    train_data_idx_list,\n",
    "\n",
    "    train_data_idx_list_01,\n",
    "\n",
    "    train_data_idx_list_02\n",
    "\n",
    "]\n",
    "\n",
    "for tr in train_data_range_list:\n",
    "\n",
    "    for idx, i in enumerate(tr):\n",
    "\n",
    "        for j in range(2):\n",
    "\n",
    "            for bool_opt in bool_opt_combinations:\n",
    "\n",
    "                synapse_list,                       \\\n",
    "                bias_val_list,                      \\\n",
    "                order_weights_list,                 \\\n",
    "                loss_col,                           \\\n",
    "                enable_backprop_activation_nesting, \\\n",
    "                enable_moad_forward_pass,           \\\n",
    "                enable_strict_derivative_order,     \\\n",
    "                enable_derivative_normalization = moad_mlp_train(\n",
    "\n",
    "                    input_data[ : i],\n",
    "                    output_data[ : i],\n",
    "                    synapse_list,\n",
    "                    CONFIG.MOAD_MLP_BIAS_VAL_LIST,\n",
    "                    [CONFIG.ACTIVATION_FUNCTION for _ in synapse_list[1:]] + [min_max_scaler],\n",
    "                    CONFIG.LEARNING_RATE,\n",
    "                    CONFIG.EPSILON,\n",
    "                    epsilon_regularization_factor=CONFIG.EPSILON_REGULARIZATION_FACTOR,\n",
    "                    num_epochs=CONFIG.EPOCHS // (2 * 2) if bool_opt[0] and bool_opt[1] else CONFIG.EPOCHS // (4 * 2),\n",
    "                    order=CONFIG.MOAD_MLP_ORDER,\n",
    "                    order_weights_list=None,\n",
    "                    loss_plateau_patience=None,\n",
    "                    num_auto_diff_steps=CONFIG.NUM_AUTO_DIFF_STEPS,\n",
    "                    remanan_stochastic_expansion=0.1,\n",
    "                    return_best_synapse=True,\n",
    "                    enable_backprop_activation_nesting=bool_opt[0],\n",
    "                    enable_moad_forward_pass=bool_opt[1],\n",
    "                    enable_strict_derivative_order=True,\n",
    "                    verbose=True if ((idx + 1) % 1000 == 0) or (i == (num_samples - 2)) else False\n",
    "\n",
    "                )\n",
    "\n",
    "                all_loss_col.append(loss_col)\n",
    "\n",
    "                print(\n",
    "\n",
    "                    f'Training step: {idx + 1} →',\n",
    "\n",
    "                    'Mean loss: %f,'%(np.mean(loss_col)),\n",
    "\n",
    "                    'Min loss: %f,'%(np.min(loss_col)),\n",
    "\n",
    "                    'Max loss: %f ...'%(np.max(loss_col)),\n",
    "\n",
    "                    ' ' * 512,\n",
    "\n",
    "                    end=''\n",
    "\n",
    "                )\n",
    "\n",
    "                print('', end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "\n",
    "    list(\n",
    "\n",
    "        all_loss_col[-1]\n",
    "\n",
    "    )\n",
    "\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generate predictions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_layer = compute_forward_pass(\n",
    "\n",
    "    X[-1],\n",
    "\n",
    "    synapse_list,\n",
    "\n",
    "    CONFIG.MOAD_MLP_BIAS_VAL_LIST,\n",
    "\n",
    "    [CONFIG.ACTIVATION_FUNCTION for _ in synapse_list],\n",
    "\n",
    "    epsilon=CONFIG.EPSILON,\n",
    "\n",
    "    order=CONFIG.MOAD_MLP_ORDER,\n",
    "\n",
    "    enable_training=False,\n",
    "\n",
    "    enable_strict_derivative_order=False,\n",
    "\n",
    "    enable_normalization=False,\n",
    "\n",
    "    enable_min_max_scaled_output_layer=False\n",
    "\n",
    ")\n",
    "\n",
    "out_layer = out_layer[1]\n",
    "\n",
    "out_layer = min_max_scaler(out_layer)\n",
    "\n",
    "out_layer = np.where(out_layer == 0, 1, out_layer)\n",
    "\n",
    "out_layer = np.where(out_layer > 1, 1, out_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_preds =  np.where(st_data == 0, 1, st_data).reshape(len(state_idx_dict)) / out_layer.reshape(len(state_idx_dict))\n",
    "\n",
    "for key in state_idx_dict.keys():\n",
    "\n",
    "    st_idx = state_idx_dict[key]\n",
    "\n",
    "    state_preds = out_preds[st_idx - 1]\n",
    "\n",
    "    print(state_preds, st_idx)\n",
    "\n",
    "    num_preds_headers = 1\n",
    "\n",
    "    if out_preds.shape == (len(state_idx_dict), 1):\n",
    "\n",
    "        num_preds_headers = 1\n",
    "\n",
    "    elif len(out_preds.shape) == 2:\n",
    "\n",
    "        num_preds_headers = out_preds.shape[-1] + 2\n",
    "\n",
    "    elif len(out_preds.shape) == 1:\n",
    "\n",
    "        num_preds_headers = 1\n",
    "\n",
    "    for idx, info in enumerate(CONFIG.CSV_DATA_HEADER[2 : num_preds_headers + 2]):\n",
    "\n",
    "        if out_preds.shape == (len(state_idx_dict), 1):\n",
    "\n",
    "            st_pred = state_preds\n",
    "\n",
    "        elif len(out_preds.shape) == 2:\n",
    "\n",
    "            st_pred = state_preds[idx]\n",
    "\n",
    "        elif len(out_preds.shape) == 1:\n",
    "\n",
    "            st_pred = state_preds\n",
    "\n",
    "        print(\n",
    "\n",
    "            f'{key} {info}: {math.ceil(st_pred)} ...'\n",
    "\n",
    "        )\n",
    "\n",
    "        pred_prob = (1 / out_layer[st_idx - 1] if out_layer[st_idx - 1] > 0 else 1)\n",
    "\n",
    "        pred_prob = np.where(pred_prob >= 1, pred_prob, 1)\n",
    "\n",
    "        print(f'Case rate of increase prediction: {pred_prob}')\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cryptographically sign and save synapses:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synapse_list_crypto_sign = hash_generator(\n",
    "\n",
    "    str([synapse_list, bias_val_list,  order_weights_list]),\n",
    "    digest_size=int(CONFIG.CRYPTO_DIGEST_SIZE),\n",
    "    hash_algorithm=CONFIG.CRYPTO_HASH_ALGORITHM,\n",
    "    verbose=True\n",
    "\n",
    ")\n",
    "\n",
    "print(\n",
    "\n",
    "    '\\n',\n",
    "    'Generated a cryptographic signature of the synapse list: ',\n",
    "    '\\n',\n",
    "    synapse_list_crypto_sign\n",
    "\n",
    ")\n",
    "\n",
    "save_pickle(\n",
    "\n",
    "    [synapse_list, bias_val_list,  order_weights_list],\n",
    "    f'{CONFIG.SYNAPSE_LIST_FILE_PREFIX}_{synapse_list_crypto_sign}.pkl',\n",
    "    protocol=-1,\n",
    "    compression=True,\n",
    "    verbose=True\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Housekeeping:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "safety_doc_url = str('https://docs.python.org/3/library/os.html#os.walk')\n",
    "assert CONFIG.GIT_REPO_DIR != os.path.realpath('/') or CONFIG.GIT_REPO_DIR != '/' or CONFIG.GIT_REPO_DIR != '//', f'Folder delete operation will not be performed. Please read more here: {safety_doc_url} ...'\n",
    "for root, dirs, files in os.walk(CONFIG.GIT_REPO_DIR, topdown=False):\n",
    "    for name in files:\n",
    "        try:\n",
    "            filename = os.path.join(root, name)\n",
    "            os.remove(filename)\n",
    "        except Exception as e:\n",
    "            print(f'File: {filename} removal failed due to: {e} ...')\n",
    "    for name in dirs:\n",
    "        try:\n",
    "            dir_name = os.path.join(root, name)\n",
    "            os.rmdir(dir_name)\n",
    "            if os.path.exists(dir_name):\n",
    "                print(f'Attempting to delete the directory: {dir_name} using elevated OS user privileges ...')\n",
    "                try:\n",
    "                    !sudo rm -r {dir_name}\n",
    "                except Exception as e:\n",
    "                    if os.path.exists(dir_name):\n",
    "                        print(f'Failed to delete the directory: {dir_name} using elevated OS user privileges ...')\n",
    "                    print(f'Failed due to error: {e} ...')\n",
    "        except Exception as e:\n",
    "            print(f'Folder: {dir_name} removal failed due to: {e} ...')\n",
    "try:\n",
    "    os.rmdir(CONFIG.GIT_REPO_DIR)\n",
    "    if os.path.exists(CONFIG.GIT_REPO_DIR):\n",
    "        print(f'Attempting to delete the directory: {CONFIG.GIT_REPO_DIR} using elevated OS user privileges ...')\n",
    "        try:\n",
    "            !sudo rm -r {CONFIG.GIT_REPO_DIR}\n",
    "        except Exception as e:\n",
    "            if os.path.exists(CONFIG.GIT_REPO_DIR):\n",
    "                print(f'Failed to delete the directory: {CONFIG.GIT_REPO_DIR} using elevated OS user privileges ...')\n",
    "            print(f'Failed due to error: {e} ...')\n",
    "except Exception as e:\n",
    "    print(f'Folder: {CONFIG.GIT_REPO_DIR} removal failed due to: {e} ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEltQunn_eIO"
   },
   "source": [
    "## **Concluding notes -- Frank Rosenblatt, connectionism and the Perceptron**:\n",
    "\n",
    "![Frank Rosenblatt and the Perceptron](https://github.com/rahulremanan/python_tutorial/raw/master/Fundamentals_of_deep-learning/media/Frank_Rosenblatt's_Mark_I_Perceptron__Cornell_Aeronautical_Laboratory__Buffalo__New%20York.jpg)\n",
    "\n",
    "#### Image 1: Frank Rosenblatt working on his Mark 1 Perceptron at Cornell Aeronautical Laboratory in Buffalo, New York, circa 1960.\n",
    "\n",
    "This notebook is created to coincide the 90th birth anniversary of pioneering psychologist and artificial intelligence researcher, Frank Rosenblatt, born July 11, 1928 – died July 11, 1971. He is known for his work on connectionism, the incredible Mark 1 Perceptron. This notebook aims to remember the promise, the controversy and the resurgence of connectionism and neural networks as a tool in artificial intelligence.\n",
    "\n",
    "[Here is a brief biography of Frank Rosenblatt](https://en.wikipedia.org/wiki/Frank_Rosenblatt) (Via Wikipedia):\n",
    "\n",
    "Frank Rosenblatt was born in New Rochelle, New York as son of Dr. Frank and Katherine Rosenblatt. After graduating from The Bronx High School of Science in 1946, he attended Cornell University, where he obtained his A.B. in 1950 and his Ph.D. in 1956.\n",
    "\n",
    "He then went to Cornell Aeronautical Laboratory in Buffalo, New York, where he was successively a research psychologist, senior psychologist, and head of the cognitive systems section. This is also where he conducted the early work on perceptrons, which culminated in the development and hardware construction of the Mark I Perceptron in 1960. This was essentially the first computer that could learn new skills by trial and error, using a type of neural network that simulates human thought processes.\n",
    "\n",
    "Rosenblatt’s research interests were exceptionally broad. In 1959 he went to Cornell’s Ithaca campus as director of the Cognitive Systems Research Program and also as a lecturer in the Psychology Department. In 1966 he joined the Section of Neurobiology and Behavior within the newly formed Division of Biological Sciences, as associate professor. Also in 1966, he became fascinated with the transfer of learned behavior from trained to naive rats by the injection of brain extracts, a subject on which he would publish extensively in later years.\n",
    "\n",
    "In 1970 he became field representative for the Graduate Field of Neurobiology and Behavior, and in 1971 he shared the acting chairmanship of the Section of Neurobiology and Behavior. Frank Rosenblatt died in July 1971 on his 43rd birthday, in a boating accident in Chesapeake Bay.\n",
    "\n",
    "![Mark 1 Perceptron](https://github.com/rahulremanan/python_tutorial/raw/master/Fundamentals_of_deep-learning/media/Smithsonian_Perceptron.jpg)\n",
    "\n",
    "#### Image 2: Mark 1 Perceptron at Smithsonian Institute, Washington DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
