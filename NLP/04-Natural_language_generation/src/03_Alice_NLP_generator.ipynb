{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hello World! Python Workshops @ Think Coffee**\n",
    "\n",
    "**3-5pm, 7/30/17**\n",
    "\n",
    "**Day 3, Alice NLP generator**\n",
    "\n",
    "**@python script author (original content): Rahul**\n",
    "\n",
    "**@jupyter notebook converted tutorial author: Nick Giangreco**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example script to generate text from Lewis Carroll's Alice in Wonderland. At least 20 epochs are required before the generated text starts sounding coherent. It is recommended to run this script on GPU, as recurrent networks are quite computationally intensive. If you try this script on new data, make sure your corpus has at least ~100k characters. ~1M is better.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading and reading Alice.txt corpus, saving characters (unique alphabet and punctuation characters in corpus) in array, and making dictionary associating each character with it's position in the character array (making two dictionaries where the key and position are either the key or value)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 847580\n",
      "total chars: 65\n"
     ]
    }
   ],
   "source": [
    "path = get_file('Alice.txt', origin='https://github.com/rahulremanan/python_tutorial/blob/master/Alice.txt')\n",
    "text = open(path).read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cutting the document into semi-redundant sentences, where each element in the sentences list contain 40 sentences that overlap with the previous element's sentences. Also, storing character in each next_chars array's elements, where the current element is the 40th character after the previous character.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 282514\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Making X boolean (false) array with a shape of the length of the sentences by the step (40) by the length of the unique characters/punctuation in the document.**\n",
    "\n",
    "**Making y boolean (false) array with a shape of the length of the sentences by the length of the unique characters/punctuation in the document.**\n",
    "\n",
    "**Then, going through each sentence and character in the sentence, storing a 1 (converting false to true) in the respective sentence and characters in X and y.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the LSTM model:**\n",
    "\n",
    "**Adding empty model,**\n",
    "\n",
    "**Adding first layer --> an LSTM unit with 128 nodes and shape of 40 by length of characters/punctuations in the document,**\n",
    "\n",
    "**Adding another layer --> a regular densely-connected NN layer with the number of nodes being the number of unique characters/punctuation in the document,**\n",
    "\n",
    "**Adding another layer --> an activation function to the output, with a softmax activation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiating a RMSProp optimizer and compiling our LSTM model with the optimizer. This optimizer is usually a good choice for RNNs. 'lr' is the learning rate.**\n",
    "\n",
    "**model.compile means to start the learning process with the optimizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining sample function, which is a helper function to sample an index from a probability array.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model training:**\n",
    "\n",
    "**Fitting predictor data onto the response. On a CPU, this will run very slow, so decrease the number of iterations to have it run though the performance will be very poor.**\n",
    "\n",
    "**Then in the for loop, we are grabbing a random set of sentences,**\n",
    "\n",
    "**outputting them (with our randomly generated seed for grabbing random sentences),**\n",
    "\n",
    "**then we're randomly predicting from a random multinomial model.**\n",
    "- **making a subsample matrix from our larger X data, and outputting sentences indexed by a random, multinomial-distribution sampled transformation of our data indices.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Epoch 1/1\n",
      "282514/282514 [==============================] - 720s - loss: 0.5980   \n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"5\" class=\"blob-num js-line-number\" data-\"\n",
      "5\" class=\"blob-num js-line-number\" data-line-number=\"1646\"></td>\n",
      "        <td id=\"lc2444\" class=\"blob-code blob-code-inner js-file-line\">\n",
      "</td>\n",
      "      </tr>\n",
      "      <tr>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:4: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        <td id=\"l1669\" class=\"blob-num js-line-number\" data-line-number=\"1444\"></td>\n",
      "        <td id=\"lc1644\" class=\"blob-code blob-code-inner js-file-line\">\n",
      "</td>\n",
      "      </tr>\n",
      "      <tr>\n",
      "        <td id=\"l1447\" class=\"blob-num js-line-number\" data-line-number=\"1147\"></td>\n",
      "   \n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"5\" class=\"blob-num js-line-number\" data-\"\n",
      "5\" class=\"blob-num js-line-number\" data-line-number=\"3107\"></td>\n",
      "        <td id=\"lc3440\" class=\"blob-code blob-code-inner js-file-line\">\n",
      "</td>\n",
      "      </tr>\n",
      "      <tr>\n",
      "        <td id=\"l3304\" class=\"blob-num js-line-number\" data-line-number=\"1434\"></td>\n",
      "        <td id=\"lc1266\" class=\"blob-code blob-code-inner js-file-line\">\n",
      "</td>\n",
      "      </tr>\n",
      "      <tr>\n",
      "        <td id=\"l1444\" class=\"blob-num js-line-number\" data-line-number=\"3447\"></td>\n",
      "   \n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"5\" class=\"blob-num js-line-number\" data-\"\n",
      "5\" class=\"blob-num js-line-number\" data-line-number=\"1503\"></td>\n",
      "        <td id=\"lc3237\" class=\"blob-code blob-code-inner js-file-line\">\n",
      "</td>\n",
      "      </tr>\n",
      "      <tr>\n",
      "        <td id=\"l1156\" class=\"blob-num js-line-number\" data-line-number=\"467\"></td>\n",
      "        <td id=\"lc1534\" class=\"blob-code blob-code-inner js-file-line\">on thiagn were and surben,</td>\n",
      "      </tr>\n",
      "      <tr>\n",
      "        <td id=\"l03415\" class=\"blob-code blob-code-inner js-file\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"5\" class=\"blob-num js-line-number\" data-\"\n",
      "5\" class=\"blob-num js-line-number\" data-line-number=\"3420\" </td>\n",
      "        <td id=\"lc1361\" class=\"blob-code blob-code-inner js-lile-line\">snbust.?&y\">\n",
      "        hres,&#39; sd tigh of though</td>\n",
      "      </tr>\n",
      "      <tr>\n",
      "        <td id=\"l2417\" class=\"blob-num js-line-number\" data-line-number=\"244\"></td>\n",
      "        <td id=\"lc2667\" class=\"blob-code blob-code-inner js-file-line\">sust aryesdlo it</td>\n",
      "      </tr>\n",
      "      <tr>\n",
      "        <td id=\"l3091\" cla\n"
     ]
    }
   ],
   "source": [
    "# train the model, output generated text after each iteration\n",
    "for iteration in range(1, 2):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(X, y,\n",
    "              batch_size=128,\n",
    "              epochs=1)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
