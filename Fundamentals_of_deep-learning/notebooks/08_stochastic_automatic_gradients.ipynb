{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Ultimate Guide to Stochastic Automatic Differentiation**\n\nThis notebook outlines the concept of automatic differentiation using **stochastic automatic gradient (SAG)** computations. \nThe implementation of **SAG** models are a pre-cursor to the development of an end-to-end quantum artificial intelligence model. \nIt is also an important component of **software defined quantum networks (SDQN)**, where the traditional flexibility and scalability of general purpose digital computing infrastructure is combined with the power of quantum processes.","metadata":{}},{"cell_type":"code","source":"import os, math, random, tempfile, matplotlib, numpy as np,         \\\n       pandas as pd, seaborn as sns, sklearn.metrics as sk_metrics, \\\n       tensorflow as tf, tensorflow_datasets as tfds\nfrom matplotlib import pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"matplotlib.rcParams['figure.figsize'] = [9, 6]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(tf.__version__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Set random seed for reproducible results ","metadata":{}},{"cell_type":"code","source":"# tf.random.set_seed(22)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Computing first derivative of a function","metadata":{}},{"cell_type":"markdown","source":"## Compute stochastic automatic differentation\nA naive python function for calculating the local derivative using stochastic automatic gradient (SAG) computation.","metadata":{}},{"cell_type":"code","source":"def compute_stochastic_auto_grad(x, fn, epsilon, num_steps=100, dtype='float32'):\n    x = np.array(x, dtype=dtype)\n    delta = np.zeros_like(x, dtype=dtype)\n    rnd_steps = int(random.SystemRandom().uniform(int(0.9 * num_steps), int(1.1 * num_steps)))\n    for i in range(rnd_steps):\n        fwd_epsilon = np.zeros_like(x, dtype=dtype) + random.SystemRandom().uniform(epsilon * (1 - epsilon), epsilon * (1 + epsilon))\n        rwd_epsilon = np.zeros_like(x, dtype=dtype) + random.SystemRandom().uniform(epsilon * (1 - epsilon), epsilon * (1 + epsilon))\n        sum_epsilon = fwd_epsilon + rwd_epsilon\n        delta += (fn(x + (x * fwd_epsilon)) - fn(x - (x * rwd_epsilon))) / (x * sum_epsilon)\n    delta /=  np.array(rnd_steps, dtype=dtype)\n    delta = np.array(delta, dtype=dtype)\n    return delta","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Automatic differentiation examples","metadata":{}},{"cell_type":"code","source":"x = 1 / math.pi\ntf_x = tf.convert_to_tensor(float(x))\nprint(tf_x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Native Python function to exponentiate an input","metadata":{}},{"cell_type":"code","source":"def exponentiate_fn(x, y):\n    return math.pow(x, y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### TensorFlow function to exponentiate an input","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef tf_exponentiate_fn(x, y):\n    return tf.math.pow(x , y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Auto differentiation using TensorFlow","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef tf_input_fn(x):\n    return tf_exponentiate_fn(x, tf.constant(2.0))\n\nwith tf.GradientTape() as g:\n  g.watch(tf_x)\n  tf_y = tf_input_fn(tf_x)\ntf_dy_dx = g.gradient(tf_y, tf_x)\nprint(tf_dy_dx)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Differentiation using simple stochastic auto gradient","metadata":{}},{"cell_type":"code","source":"def input_fn(x):\n    return exponentiate_fn(x, 2)\n\nsag_dy_dx = compute_stochastic_auto_grad(x, input_fn, math.pow(10, -5.4444))\nprint(sag_dy_dx)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dy_dx = 2 * x\nprint(dy_dx)\nprint(f'TensorFlow automatic gradient calculation error: {tf_dy_dx.numpy() - dy_dx}')\nprint(f'Simple stochastic automatic gradient calculation error: {sag_dy_dx - dy_dx}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"$\n{f(x) = {x}^{\\pi}}\n$","metadata":{}},{"cell_type":"markdown","source":"${\\therefore \\frac{d(f(x))}{dx} = {\\pi}{x}^{(\\pi- 1)}}$","metadata":{}},{"cell_type":"markdown","source":"### Auto differentiation using TensorFlow","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef tf_input_fn(x):\n    return tf_exponentiate_fn(x, tf.constant(math.pi, tf.float32))\n\nwith tf.GradientTape() as g:\n  g.watch(tf_x)\n  tf_y = tf_input_fn(tf_x)\ntf_dy_dx = g.gradient(tf_y, tf_x)\nprint(tf_dy_dx)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Differentiation using simple stochastic auto gradient","metadata":{}},{"cell_type":"code","source":"def input_fn(x):\n    return exponentiate_fn(x, math.pi)\n\nsag_dy_dx = compute_stochastic_auto_grad(x, input_fn, math.pow(10, -5.4444))\nprint(sag_dy_dx)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dy_dx = math.pi * math.pow(x, math.pi - 1)\nprint(dy_dx)\nprint(f'TensorFlow automatic gradient calculation error: {tf_dy_dx.numpy() - dy_dx}')\nprint(f'Simple stochastic automatic gradient calculation error: {sag_dy_dx - dy_dx}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Auto differentiation using TensorFlow","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef tf_input_fn(x):\n    return tf_exponentiate_fn(x, x)\n\nwith tf.GradientTape() as g:\n  g.watch(tf_x)\n  tf_y = tf_input_fn(tf_x)\ntf_dy_dx = g.gradient(tf_y, tf_x)\nprint(tf_dy_dx)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Differentiation using simple stochastic auto gradient","metadata":{}},{"cell_type":"code","source":"def input_fn(x):\n    return exponentiate_fn(x, x)\n\nsag_dy_dx = compute_stochastic_auto_grad(x, input_fn, math.pow(10, -5.4444))\nprint(sag_dy_dx)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dy_dx = (1 + math.log(x)) * math.pow(x, x)\nprint(dy_dx)\nprint(f'TensorFlow automatic gradient calculation error: {tf_dy_dx.numpy() - dy_dx}')\nprint(f'Simple stochastic automatic gradient calculation error: {sag_dy_dx - dy_dx}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def input_fn(x):\n    return exponentiate_fn(np.exp(x), np.power(x,2))\n\nsag_dy_dx = compute_stochastic_auto_grad(x, input_fn, math.pow(10, -5.4444))\nprint(sag_dy_dx)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@tf.function\ndef tf_input_fn(x):\n    return tf_exponentiate_fn(tf.math.exp(x), tf.math.pow(x,2))\n\nwith tf.GradientTape() as g:\n  g.watch(tf_x)\n  tf_y = tf_input_fn(tf_x)\ntf_dy_dx = g.gradient(tf_y, tf_x)\nprint(tf_dy_dx)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# [Building a digit recongnizer using TensorFlow auto grad](https://www.tensorflow.org/guide/core/mlp_core)","metadata":{}},{"cell_type":"markdown","source":"## Load MNIST training and validation data","metadata":{}},{"cell_type":"code","source":"train_data, val_data, test_data = tfds.load('mnist', \n                                            split=['train[10000:]', 'train[0:10000]', 'test'],\n                                            batch_size=128, as_supervised=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_viz, y_viz = tfds.load('mnist', split=['train[:1500]'], batch_size=-1, as_supervised=True)[0]\nx_viz = tf.squeeze(x_viz, axis=3)\n\nfor i in range(9):\n    plt.subplot(3, 3, 1 + i)\n    plt.axis('off')\n    plt.imshow(x_viz[i], cmap='gray')\n    plt.title(f'True Label: {y_viz[i]}')\n    plt.subplots_adjust(hspace=.5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.countplot(x=y_viz.numpy());\nplt.xlabel('Digits')\nplt.title('MNIST digit distribution');","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess(x, y):\n  # Reshaping the data\n  x = tf.reshape(x, shape=[-1, 784])\n  # Rescaling the data\n  x = x / 255\n  return x, y\n\ntrain_data, val_data = train_data.map(preprocess), val_data.map(preprocess)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Activation function","metadata":{}},{"cell_type":"code","source":"x = tf.linspace(-2, 2, 201)\nx = tf.cast(x, tf.float32)\nplt.plot(x, tf.nn.relu(x));\nplt.xlabel('x')\nplt.ylabel('ReLU(x)')\nplt.title('ReLU activation function');","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x = tf.linspace(-4, 4, 201)\nx = tf.cast(x, tf.float32)\nplt.plot(x, tf.nn.softmax(x, axis=0));\nplt.xlabel('x')\nplt.ylabel('Softmax(x)')\nplt.title('Softmax activation function');","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def xavier_init(shape):\n  # Computes the xavier initialization values for a weight matrix\n  in_dim, out_dim = shape\n  xavier_lim = tf.sqrt(6.)/tf.sqrt(tf.cast(in_dim + out_dim, tf.float32))\n  weight_vals = tf.random.uniform(shape=(in_dim, out_dim), \n                                  minval=-xavier_lim, maxval=xavier_lim, seed=22)\n  return weight_vals","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DenseLayer(tf.Module):\n\n  def __init__(self, out_dim, weight_init=xavier_init, activation=tf.identity):\n    # Initialize the dimensions and activation functions\n    self.out_dim = out_dim\n    self.weight_init = weight_init\n    self.activation = activation\n    self.built = False\n\n  def __call__(self, x):\n    if not self.built:\n      # Infer the input dimension based on first call\n      self.in_dim = x.shape[1]\n      # Initialize the weights and biases\n      self.w = tf.Variable(self.weight_init(shape=(self.in_dim, self.out_dim)))\n      self.b = tf.Variable(tf.zeros(shape=(self.out_dim,)))\n      self.built = True\n    # Compute the forward pass\n    z = tf.add(tf.matmul(x, self.w), self.b)\n    return self.activation(z)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MLP(tf.Module):\n\n  def __init__(self, layers):\n    self.layers = layers\n\n  @tf.function\n  def __call__(self, x, preds=False): \n    # Execute the model's layers sequentially\n    for layer in self.layers:\n      x = layer(x)\n    return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hidden_layer_1_size = 700\nhidden_layer_2_size = 500\noutput_size = 10\n\nmlp_model = MLP([\n    DenseLayer(out_dim=hidden_layer_1_size, activation=tf.nn.relu),\n    DenseLayer(out_dim=hidden_layer_2_size, activation=tf.nn.relu),\n    DenseLayer(out_dim=output_size)])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def cross_entropy_loss(y_pred, y):\n  # Compute cross entropy loss with a sparse operation\n  sparse_ce = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=y_pred)\n  return tf.reduce_mean(sparse_ce)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def accuracy(y_pred, y):\n  # Compute accuracy after extracting class predictions\n  class_preds = tf.argmax(tf.nn.softmax(y_pred), axis=1)\n  is_equal = tf.equal(y, class_preds)\n  return tf.reduce_mean(tf.cast(is_equal, tf.float32))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Adam:\n\n    def __init__(self, learning_rate=1e-3, beta_1=0.9, beta_2=0.999, ep=1e-7):\n      # Initialize optimizer parameters and variable slots\n      self.beta_1 = beta_1\n      self.beta_2 = beta_2\n      self.learning_rate = learning_rate\n      self.ep = ep\n      self.t = 1.\n      self.v_dvar, self.s_dvar = [], []\n      self.built = False\n\n    def apply_gradients(self, grads, vars):\n      # Initialize variables on the first call\n      if not self.built:\n        for var in vars:\n          v = tf.Variable(tf.zeros(shape=var.shape))\n          s = tf.Variable(tf.zeros(shape=var.shape))\n          self.v_dvar.append(v)\n          self.s_dvar.append(s)\n        self.built = True\n      # Update the model variables given their gradients\n      for i, (d_var, var) in enumerate(zip(grads, vars)):\n        self.v_dvar[i].assign(self.beta_1*self.v_dvar[i] + (1-self.beta_1)*d_var)\n        self.s_dvar[i].assign(self.beta_2*self.s_dvar[i] + (1-self.beta_2)*tf.square(d_var))\n        v_dvar_bc = self.v_dvar[i]/(1-(self.beta_1**self.t))\n        s_dvar_bc = self.s_dvar[i]/(1-(self.beta_2**self.t))\n        var.assign_sub(self.learning_rate*(v_dvar_bc/(tf.sqrt(s_dvar_bc) + self.ep)))\n      self.t += 1.\n      return","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_step(x_batch, y_batch, loss, acc, model, optimizer):\n  # Update the model state given a batch of data\n  with tf.GradientTape() as tape:\n    y_pred = model(x_batch)\n    batch_loss = loss(y_pred, y_batch)\n  batch_acc = acc(y_pred, y_batch)\n  grads = tape.gradient(batch_loss, model.variables)\n  optimizer.apply_gradients(grads, model.variables)\n  return batch_loss, batch_acc\n\ndef val_step(x_batch, y_batch, loss, acc, model):\n  # Evaluate the model on given a batch of validation data\n  y_pred = model(x_batch)\n  batch_loss = loss(y_pred, y_batch)\n  batch_acc = acc(y_pred, y_batch)\n  return batch_loss, batch_acc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(mlp, train_data, val_data, loss, acc, optimizer, epochs):\n  # Initialize data structures\n  train_losses, train_accs = [], []\n  val_losses, val_accs = [], []\n\n  # Format training loop and begin training\n  for epoch in range(epochs):\n    batch_losses_train, batch_accs_train = [], []\n    batch_losses_val, batch_accs_val = [], []\n\n    # Iterate over the training data\n    for x_batch, y_batch in train_data:\n      # Compute gradients and update the model's parameters\n      batch_loss, batch_acc = train_step(x_batch, y_batch, loss, acc, mlp, optimizer)\n      # Keep track of batch-level training performance\n      batch_losses_train.append(batch_loss)\n      batch_accs_train.append(batch_acc)\n\n    # Iterate over the validation data\n    for x_batch, y_batch in val_data:\n      batch_loss, batch_acc = val_step(x_batch, y_batch, loss, acc, mlp)\n      batch_losses_val.append(batch_loss)\n      batch_accs_val.append(batch_acc)\n\n    # Keep track of epoch-level model performance\n    train_loss, train_acc = tf.reduce_mean(batch_losses_train), tf.reduce_mean(batch_accs_train)\n    val_loss, val_acc = tf.reduce_mean(batch_losses_val), tf.reduce_mean(batch_accs_val)\n    train_losses.append(train_loss)\n    train_accs.append(train_acc)\n    val_losses.append(val_loss)\n    val_accs.append(val_acc)\n    print(f'Epoch: {epoch}')\n    print(f'Training loss: {train_loss:.3f}, Training accuracy: {train_acc:.3f}')\n    print(f'Validation loss: {val_loss:.3f}, Validation accuracy: {val_acc:.3f}')\n  return train_losses, train_accs, val_losses, val_accs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_losses, train_accs, val_losses, val_accs = train_model(\n                                                     mlp_model, \n                                                     train_data, \n                                                     val_data, \n                                                     loss=cross_entropy_loss, acc=accuracy,\n                                                     optimizer=Adam(), \n                                                     epochs=10\n                                                 )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_metrics(train_metric, val_metric, metric_type):\n  # Visualize metrics vs training Epochs\n  plt.figure()\n  plt.plot(range(len(train_metric)), train_metric, label = f'Training {metric_type}')\n  plt.plot(range(len(val_metric)), val_metric, label = f'Validation {metric_type}')\n  plt.xlabel('Epochs')\n  plt.ylabel(metric_type)\n  plt.legend()\n  plt.title(f'{metric_type} vs Training epochs');","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_metrics(train_losses, val_losses, 'cross entropy loss')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_metrics(train_accs, val_accs, 'accuracy')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ExportModule(tf.Module):\n  def __init__(self, model, preprocess, class_pred):\n    # Initialize pre and postprocessing functions\n    self.model = model\n    self.preprocess = preprocess\n    self.class_pred = class_pred\n\n  @tf.function(input_signature=[tf.TensorSpec(shape=[None, None, None, None], dtype=tf.uint8)]) \n  def __call__(self, x):\n    # Run the ExportModule for new data points\n    x = self.preprocess(x)\n    y = self.model(x)\n    y = self.class_pred(y)\n    return y","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_test(x):\n  # The export module takes in unprocessed and unlabeled data\n  x = tf.reshape(x, shape=[-1, 784])\n  x = x / 255\n  return x\n\ndef class_pred_test(y):\n  # Generate class predictions from MLP output\n  return tf.argmax(tf.nn.softmax(y), axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mlp_model_export = ExportModule(\n                       model=mlp_model,\n                       preprocess=preprocess_test,\n                       class_pred=class_pred_test\n                   )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"models = tempfile.mkdtemp()\nsave_path = os.path.join(models, 'mlp_model_export')\ntf.saved_model.save(mlp_model_export, save_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mlp_loaded = tf.saved_model.load(save_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def accuracy_score(y_pred, y):\n  # Generic accuracy function\n  is_equal = tf.equal(y_pred, y)\n  return tf.reduce_mean(tf.cast(is_equal, tf.float32))\n\nx_test, y_test = tfds.load('mnist', split=['test'], batch_size=-1, as_supervised=True)[0]\ntest_classes = mlp_loaded(x_test)\ntest_acc = accuracy_score(test_classes, y_test)\nprint(f'Test Accuracy: {test_acc:.3f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Accuracy breakdown by digit:')\nprint('---------------------------')\nlabel_accs = {}\nfor label in range(10):\n  label_ind = (y_test == label)\n  # extract predictions for specific true label\n  pred_label = test_classes[label_ind]\n  labels = y_test[label_ind]\n  # compute class-wise accuracy\n  label_accs[accuracy_score(pred_label, labels).numpy()] = label\nfor key in sorted(label_accs):\n  print(f'Digit {label_accs[key]}: {key:.3f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_confusion_matrix(test_labels, test_classes):\n  # Compute confusion matrix and normalize\n  plt.figure(figsize=(10,10))\n  confusion = sk_metrics.confusion_matrix(test_labels.numpy(), \n                                          test_classes.numpy())\n  confusion_normalized = confusion / confusion.sum(axis=1, keepdims=True)\n  axis_labels = range(10)\n  ax = sns.heatmap(\n      confusion_normalized, xticklabels=axis_labels, yticklabels=axis_labels,\n      cmap='Blues', annot=True, fmt='.4f', square=True)\n  plt.title('Confusion matrix')\n  plt.ylabel('True label')\n  plt.xlabel('Predicted label')\n\nshow_confusion_matrix(y_test, test_classes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Building a trainable multi layer perceptron (MLP) using SAG ","metadata":{}},{"cell_type":"code","source":"# train_data, val_data, test_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train XOR gate solver","metadata":{}},{"cell_type":"code","source":"def sigmoid(x, derivative=False):\n    '''\n    Parameters:\n      x: input\n      derivative: boolean to specify if the derivative of the function should be computed\n    '''\n    if derivative:\n        return (x*(1-x))\n    return (1/(1+np.exp(-x)))\n\ndef ReLU(x, derivative=False):\n  if derivative:\n      return np.where(x < 0, 0, 1)\n  x_relu = np.maximum(x, 0)\n  return x_relu\n\ndef relu(x, derivative=False):\n  return ReLU(x, derivative=derivative)\n\ndef tanh(x, derivative=False):\n    if (derivative == True):\n        return (1 - (x ** 2))\n    return np.tanh(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x = np.asarray([[0, 0],\n                [1, 1],\n                [1, 0],\n                [0, 1]])\ny = np.asarray([[0],\n                [0],\n                [1],\n                [1]])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(x.shape, y.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_dim = x.shape[1]\noutput_dim = y.shape[1]\nhidden_units_list = [input_dim, 8, output_dim]\nbias_val_list = [1, 1, 1, 1]\nsynapse_list = []\nfor i, unit_size in enumerate(hidden_units_list):\n    if i + 1 < len(hidden_units_list):\n        synapse_list.append(2 * np.random.random((unit_size, hidden_units_list[i + 1])) - bias_val_list[i])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"synapse_list[0].shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_steps = 10000\n\nloss_col = []\nlearning_rate = 1\nactivation_fn = sigmoid # tanh # relu # ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def one_hot_encoded_labels(labels):\n    out_labels = []\n    max_val = max(labels) + 1\n    for label in labels:\n        out_labels.append([1 if label == i else 0 for i in range(max_val)])\n    return np.array(out_labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def stochastic_auto_differentiation(x, input_fn, epsilon, num_eval_steps=10):\n    auto_diff_out = 0\n    for i in range(num_eval_steps):\n        fn_x = input_fn(x)\n\n        rand_epsilon_a = random.SystemRandom().uniform(abs(epsilon), (abs(epsilon) / 100))\n        rand_epsilon_b = random.SystemRandom().uniform(abs(epsilon), (abs(epsilon) / 100))\n\n        x_a = x + rand_epsilon_a\n        x_b = x + rand_epsilon_b\n\n        x_delta = x_a - x_b\n        \n        fn_auto_diff_a = input_fn(x_a)\n        fn_auto_diff_b = input_fn(x_b)\n\n        auto_diff_out += (fn_auto_diff_a - fn_auto_diff_b) / x_delta\n        auto_diff_out += (fn_auto_diff_a - fn_x)           / rand_epsilon_a\n        auto_diff_out += (fn_auto_diff_b - fn_x)           / rand_epsilon_b\n\n    auto_diff_out = auto_diff_out / (3 * num_eval_steps)\n\n    return auto_diff_out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def stochastic_auto_gradient(layer, layer_error, activation_fn, epsilon):\n    return layer_error * stochastic_auto_differentiation(layer, activation_fn, epsilon)\n\ndef forward_pass(input_data, synapse_list, bias_val_list, activation_fn):\n    layer_list = [input_data]\n    for i, synapse in enumerate(synapse_list):\n        layer_list.append(activation_fn(np.dot(layer_list[i], synapse) + bias_val_list[i]))\n    return layer_list\n\ndef back_propagation(output_data, layer_list, synapse_list, learning_rate, verbose=False):\n    output_loss_derivative = output_data - layer_list[-1]\n    loss_col.append(np.mean(np.abs(output_loss_derivative)))\n    if verbose:\n        print ('Prediction error during training : ' + str(np.mean(np.abs(output_loss_derivative))))\n\n    layer_error = learning_rate * output_loss_derivative\n\n    synapse_list = list(reversed(synapse_list))\n    layer_list = list(reversed(layer_list))\n\n    new_synapse_list = []\n    for i, layer in enumerate(layer_list):\n        if i + 1 < len(layer_list):\n            layer_delta = stochastic_auto_gradient(layer, layer_error, activation_fn, 1e-8)\n            layer_error = layer_delta.dot(synapse_list[i].T)\n            new_synapse_list.append(synapse_list[i] + layer_list[i + 1].T.dot(layer_delta))\n\n    synapse_list = list(reversed(new_synapse_list))\n    layer_list = list(reversed(layer_list))\n\n    return synapse_list, layer_list","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for t in range(training_steps):\n    input_data, output_data = x, y\n    layer_list = forward_pass(input_data, synapse_list, bias_val_list, activation_fn)\n    synapse_list, layer_list = back_propagation(output_data, layer_list, synapse_list, learning_rate, verbose=(t + 1) % (0.1 * training_steps) == 0)\nprint ('Training completed ...')\nprint (layer_list[-1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train MNIST classifier","metadata":{}},{"cell_type":"code","source":"input_dim = 28 * 28\noutput_dim = 10\nhidden_units_list = [input_dim, 8, 8, output_dim]\nbias_val_list = [1, 1, 1, 1]\nsynapse_list = []\nfor i, unit_size in enumerate(hidden_units_list):\n    if i + 1 < len(hidden_units_list):\n        synapse_list.append(2 * np.random.random((unit_size, hidden_units_list[i + 1])) - bias_val_list[i])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_steps = 100\n\nloss_col = []\nlearning_rate = 1e-5\nactivation_fn = relu # sigmoid # tanh # ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for t in range(training_steps):\n    for x_batch, y_batch in train_data:\n        x = x_batch.numpy()\n        y = y_batch.numpy()\n        input_data, output_data = x, y\n        layer_list = forward_pass(input_data, synapse_list, bias_val_list, activation_fn)\n        output_data = one_hot_encoded_labels(output_data)\n        synapse_list, layer_list = back_propagation(output_data, layer_list, synapse_list, learning_rate, verbose=(t + 1) % (0.1 * training_steps) == 0)\nprint ('Training completed ...')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(10):\n    print(np.argmax(forward_pass(x[i], synapse_list, bias_val_list, activation_fn)[-1]), y[i])\n    plt.imshow(x[i].reshape(28,28))\n    plt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-18T18:34:04.457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}