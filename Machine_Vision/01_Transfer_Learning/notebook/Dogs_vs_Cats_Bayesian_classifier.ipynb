{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HBaX5wAhDDYi"
   },
   "source": [
    "# Deep Bayesian networks using Monte Carlo dropouts\n",
    "\n",
    "## Dogs vs Cats Image Classifier using deep Bayesian network\n",
    "\n",
    "This is a demonstration notebook on: deep Bayesian networks using Monte Carlo dropouts. In this notebook, an Inception-ResNet version 2 base network is combined with a multi-layer Bayesian perceptron architecture. The Bayesian network was built using dropout, simulating tiny changes to the architecture, to compute the model uncertainty associated with each predictions.\n",
    "\n",
    "Monte Carlo estimations using dropout, is a Bayesain approach to quantify the model uncertainty. By quantifying uncertainty, harder questions such as trust worthiness of a decision made by a neural network, can later be validated against out-of-sample, real world examples.\n",
    "\n",
    "The idea of using dropout layers as a scalable, automated mechanism to construct a deep Bayesian network, was first put forth by Yarin Gal; in his PhD thesis paper at Cambridge University. \n",
    "\n",
    "Read the original work [Dropout as a Bayesian Approximation by Yarin Gal, University of Cambridge (Via arxiv)](https://arxiv.org/pdf/1506.02142.pdf) \n",
    "\n",
    "## [Launch this notebook in Google CoLab](https://colab.research.google.com/github/rahulremanan/python_tutorial/blob/master/Machine_Vision/01_Transfer_Learning/notebook/Dogs_vs_Cats_Bayesian_classifier.ipynb)\n",
    "\n",
    "## Summary of the data labels:\n",
    "\n",
    "0.  Cat  \n",
    "1.  Dog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tLZoE3tpX_Hw"
   },
   "outputs": [],
   "source": [
    "setup=True\n",
    "download_data=True\n",
    "enable_bayesian_optimizer=False\n",
    "enable_random_optimizer=False\n",
    "enable_training=True\n",
    "load_weights=True\n",
    "upload_weights = True\n",
    "\n",
    "dataset_id='dogs_vs_cats'\n",
    "val_fn='val_f1'\n",
    "loss_fn='categorical_crossentropy'\n",
    "\n",
    "train_aug=True\n",
    "val_aug=False\n",
    "\n",
    "gen_predictions=True\n",
    "detection_threshold=0.25\n",
    "\n",
    "colab_mode = True\n",
    "fetch_raw_data = False\n",
    "upload_data = False\n",
    "\n",
    "fine_tune = True\n",
    "verbose = False\n",
    "enable_dropout = True\n",
    "\n",
    "bayesian_cam = True\n",
    "\n",
    "MODEL_NAME='InceptionResNetV2_{}.model'.format(dataset_id)\n",
    "  \n",
    "EPOCHS=1\n",
    "NB_FROZEN_LAYERS = 45\n",
    "LEARNING_RATE = 9.31579590417492e-05\n",
    "INPUT_SHAPE = (299,299,3)\n",
    "BATCH_SIZE = 2\n",
    "STEPS_PER_EPOCHS = 400\n",
    "FC_SIZE = 65\n",
    "WEIGHTS_DECAY = 0.01\n",
    "DROPOUT = 0.5\n",
    "ACTIVATION='tanh'\n",
    "OPTIMIZER='adam'\n",
    "OPTIMIZERS=['adam_amsgrad','adadelta','sgd','adam']\n",
    "ACTIVATIONS=['relu','tanh','sigmoid','elu']\n",
    "LEARNING_RATES=[0.1, 0.01, 1e-3]\n",
    "\n",
    "path_to_train = './{}/train/'.format(dataset_id)\n",
    "train_labels = './train_{}.csv'.format(dataset_id)\n",
    "checkpointer_savepath = './{}/{}'.format(dataset_id, MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XB29XMoWV6jW"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import subprocess\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zini14mmWfTw"
   },
   "outputs": [],
   "source": [
    "def execute_in_shell(command=None, \n",
    "                     verbose = False):\n",
    "    \"\"\" \n",
    "        command -- keyword argument, takes a list as input\n",
    "        verbsoe -- keyword argument, takes a boolean value as input\n",
    "    \n",
    "        This is a function that executes shell scripts from within python.\n",
    "        \n",
    "        Keyword argument 'command', should be a list of shell commands.\n",
    "        Keyword argument 'verbose', should be a boolean value to set verbose level.\n",
    "        \n",
    "        Example usage: execute_in_shell(command = ['ls ./some/folder/',\n",
    "                                                    ls ./some/folder/  -1 | wc -l'],\n",
    "                                        verbose = True ) \n",
    "                                        \n",
    "        This command returns dictionary with elements: Output and Error.\n",
    "        \n",
    "        Output records the console output,\n",
    "        Error records the console error messages.\n",
    "                                        \n",
    "    \"\"\"\n",
    "    error = []\n",
    "    output = []\n",
    "    \n",
    "    if isinstance(command, list):\n",
    "        for i in range(len(command)):\n",
    "            try:\n",
    "                process = subprocess.Popen(command[i], shell=True, stdout=subprocess.PIPE)\n",
    "                process.wait()\n",
    "                out, err = process.communicate()\n",
    "                error.append(err)\n",
    "                output.append(out)\n",
    "                if verbose:\n",
    "                    print ('Success running shell command: {}'.format(command[i]))\n",
    "            except Exception as e:\n",
    "                print ('Failed running shell command: {}'.format(command[i]))\n",
    "                if verbose:\n",
    "                    print(type(e))\n",
    "                    print(e.args)\n",
    "                    print(e)\n",
    "                \n",
    "    else:\n",
    "        print ('The argument command takes a list input ...')\n",
    "    return {'Output': output, 'Error': error }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h2BGOqNBWiXF"
   },
   "outputs": [],
   "source": [
    "command = ['pip3 install -q kaggle PyDrive scikit-optimize >/dev/null 2>&1',\n",
    "           'mkdir /content/',\n",
    "           'mkdir /content/.kaggle/',\n",
    "           'mkdir ./{}/'.format(dataset_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "doBv_rgpW5Q0"
   },
   "outputs": [],
   "source": [
    "if setup and colab_mode:\n",
    "  execute_in_shell(command = command, \n",
    "                   verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lQ4UVW30K5em"
   },
   "outputs": [],
   "source": [
    "if colab_mode:\n",
    "    from pydrive.auth import GoogleAuth\n",
    "    from pydrive.drive import GoogleDrive\n",
    "    from google.colab import auth\n",
    "    from oauth2client.client import GoogleCredentials\n",
    "    from googleapiclient.http import MediaIoBaseDownload\n",
    "    \n",
    "import io\n",
    "import glob\n",
    "import fnmatch\n",
    "import random\n",
    "\n",
    "from multiprocessing import Process\n",
    "\n",
    "import os, sys, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from imgaug import augmenters as iaa\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nruekEqqXFHO"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import glob\n",
    "try:\n",
    "    import h5py\n",
    "except:\n",
    "    print ('Package h5py needed for saving model weights ...')\n",
    "    sys.exit(1)\n",
    "import json\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import tensorflow\n",
    "    import keras\n",
    "except:\n",
    "    print ('This code uses tensorflow deep-learning framework and keras api ...')\n",
    "    print ('Install tensorflow and keras to train the classifier ...')\n",
    "    sys.exit(1)\n",
    "import PIL\n",
    "from collections import defaultdict\n",
    "from keras.applications.inception_v3 import InceptionV3,    \\\n",
    "                                            preprocess_input as preprocess_input_inceptionv3\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2,    \\\n",
    "                                            preprocess_input as preprocess_input_inceptionv4\n",
    "from keras.models import Model,                             \\\n",
    "                         model_from_json,                    \\\n",
    "                         load_model\n",
    "from keras.layers import Dense,                             \\\n",
    "                         GlobalAveragePooling2D,            \\\n",
    "                         Dropout,                           \\\n",
    "                         BatchNormalization\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD,                           \\\n",
    "                             RMSprop,                       \\\n",
    "                             Adagrad,                       \\\n",
    "                             Adadelta,                      \\\n",
    "                             Adam,                          \\\n",
    "                             Adamax,                        \\\n",
    "                             Nadam\n",
    "from keras.callbacks import EarlyStopping,   \\\n",
    "                            ModelCheckpoint, \\\n",
    "                            ReduceLROnPlateau\n",
    "                            \n",
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H1uUkCyOXK3s"
   },
   "outputs": [],
   "source": [
    "if setup and fetch_raw_data and colab_mode:\n",
    "  from google.colab import files\n",
    "  uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ec_RhyAlXMLE"
   },
   "outputs": [],
   "source": [
    "command = ['mv ./*.json /content/.kaggle/',\n",
    "           'cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json',\n",
    "           'cp /content/.kaggle/kaggle.json /root/.kaggle/kaggle.json',\n",
    "           'chmod 600 ~/.kaggle/kaggle.json',\n",
    "           'kaggle competitions download -c dogs-vs-cats-redux-kernels-edition',\n",
    "           'mv /content/*.csv ./',\n",
    "           'mv /content/*.zip ./',\n",
    "           'mv ./train.zip ./train_{}.zip'.format(dataset_id),\n",
    "           'mv ./test.zip ./test_{}.zip'.format(dataset_id),\n",
    "           'mv ./sample_submission.csv ./sample_submission_{}'.format(dataset_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lvxm7O6cXSDM"
   },
   "outputs": [],
   "source": [
    "if fetch_raw_data:\n",
    "  execute_in_shell(command = command, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cM9K0jBIXZVy"
   },
   "outputs": [],
   "source": [
    "def cloud_authenticate():\n",
    "  auth.authenticate_user()\n",
    "  gauth = GoogleAuth()\n",
    "  gauth.credentials = GoogleCredentials.get_application_default()\n",
    "  drive = GoogleDrive(gauth)\n",
    "  print (\"Sucessfully authenticated to access Google Drive ...\")\n",
    "  return drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tEkwrigGXaWr"
   },
   "outputs": [],
   "source": [
    "if colab_mode:\n",
    "    drive = cloud_authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e2UawP3SXdM1"
   },
   "outputs": [],
   "source": [
    "def googledrive_fetch(file_name = None, \n",
    "                fetch=True, \n",
    "                fetch_by_id = False,\n",
    "                latest = True,\n",
    "                file_id = None,\n",
    "                multi_file = False):\n",
    "  \n",
    "  \"\"\"\n",
    "    A function that fetches files from Google Drive.\n",
    "    \n",
    "    The function takes five keyword arguments:\n",
    "      file_name -- Passes the file name string\n",
    "      fetch -- Specify if a file name should be downloaded\n",
    "      fetch_by_id -- Specify a file to be downloaded by file id\n",
    "      multi_file -- Download all the files with the same file name from Google Drive\n",
    "  \"\"\"\n",
    "  \n",
    "  query = 'title='+\"'\"+file_name+\"'\"\n",
    "  try:\n",
    "    file_list=drive.ListFile({'q': \"{}\".format(query)}).GetList()\n",
    "  except:\n",
    "    return (\"Error finding file with {}\".format(query))\n",
    "  \n",
    "  if len(file_list) >1:\n",
    "    print (\"A total of {} files with the same file name found ...\".format(len(file_list)))\n",
    "    for f in file_list:\n",
    "      title = f['title']\n",
    "      id = f.metadata.get('id')\n",
    "      print (\"Found: {} file, with file id: {}\".format(title, id))\n",
    "    \n",
    "    if multi_file:\n",
    "      print (\"Downloading {} files with file name {}\".format(len(file_list), title))\n",
    "      print (\"Staring download ...\")\n",
    "    elif latest:\n",
    "      print (\"Downloading the most recent {} file ...\".format(title))\n",
    "    elif file_id == None:\n",
    "      print (\"Set keyword argument fetch_by_id = True and specify id using keyword argument file_id = 'id' to download a specific file ...\")\n",
    "      print (\"--OR--\")\n",
    "      print (\"Set keyword argument multi_file = True to automatically download all the files ...\")\n",
    "      return None\n",
    "    else:\n",
    "      print (\"Starting download ...\")\n",
    "    \n",
    "  n = 0\n",
    "  \n",
    "  if latest:\n",
    "    try:\n",
    "      title = file_list[0]['title']\n",
    "    except:\n",
    "      return (\"Error finding file with {}\".format(query))\n",
    "    latest_file_id = file_list[0].metadata.get('id')\n",
    "    print (\"Found most recent version of: {} file with file id: {} ...\".format(title, latest_file_id))    \n",
    "  \n",
    "  for f in file_list:\n",
    "      if fetch and multi_file and n>0:\n",
    "        save_path = os.path.join('./'+str(n)+'_'+file_name)\n",
    "      else:\n",
    "        save_path = os.path.join('./'+file_name)     \n",
    "      \n",
    "      title = f['title']\n",
    "      \n",
    "      if fetch_by_id and file_id !=None:\n",
    "        id = file_id\n",
    "      elif latest:\n",
    "        id = latest_file_id\n",
    "      elif fetch_by_id and file_id == None:\n",
    "        print ('Please specify the file id for downloading using the file_id argument ...')\n",
    "      else:\n",
    "        id = f.metadata.get('id')\n",
    "      \n",
    "      print (\"Downloading {} file, with file id: {} ...\".format(title, id))\n",
    "      \n",
    "      if fetch or fetch_by_id or latest:\n",
    "        local_file = io.FileIO(save_path, mode='wb')\n",
    "        try:\n",
    "          request = drive.auth.service.files().get_media(fileId=id)\n",
    "          downloader = MediaIoBaseDownload(local_file, request, chunksize=2048*102400)\n",
    "\n",
    "          done = False\n",
    "\n",
    "          while done is False:\n",
    "              status, done = downloader.next_chunk()\n",
    "        except:\n",
    "          return 'Downloading failed ...'\n",
    "        \n",
    "        local_file.close()\n",
    "        print (\"Successfully downloaded the file: {} to: {} ...\".format(file_name, save_path))\n",
    "      \n",
    "      if fetch_by_id and file_id !=None:\n",
    "        return None\n",
    "      elif latest:\n",
    "        return None\n",
    "      elif n >= 0:\n",
    "        print (\"Downloaded {} of {} files ...\".format(n+1, len(file_list)))\n",
    "      else:\n",
    "        print (\"Download failed ...\")\n",
    "      \n",
    "      n +=1\n",
    "  \n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rIBlZ28KXggT"
   },
   "outputs": [],
   "source": [
    "def googledrive_save(file_name = None, \n",
    "               file_dir = None, \n",
    "               upload = False,\n",
    "               prefix = None):\n",
    "  if upload == True and file_name != None and file_dir !=None:\n",
    "    try:\n",
    "      if prefix != None:\n",
    "        file = drive.CreateFile({'title': str(prefix) + str(file_name) })\n",
    "      else:\n",
    "        file = drive.CreateFile({'title': str(file_name) })\n",
    "      file.SetContentFile(os.path.join(file_dir + str(file_name)))\n",
    "      file.Upload()\n",
    "      print (str(file_name) + \" successfully uploaded to Google drive ...\")\n",
    "    except:\n",
    "      print (\"Failed to save :\" + str(file_name) + \" to Google drive ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UxugOHGvXqwZ"
   },
   "outputs": [],
   "source": [
    "file_dir = './'\n",
    "file_name = ['train_{}.zip'.format(dataset_id),\n",
    "             'train_{}.csv'.format(dataset_id),\n",
    "             'test_{}.zip'.format(dataset_id),\n",
    "             'sample_submission_{}.csv'.format(dataset_id),\n",
    "             'Transfer_learn_299_299_{}.h5'.format(dataset_id),\n",
    "             '{}'.format(MODEL_NAME)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8c_3R8k1Xrzz"
   },
   "outputs": [],
   "source": [
    "if upload_data and colab_mode:\n",
    "  for f in file_name:\n",
    "    googledrive_save(file_name = f,\n",
    "                     file_dir = file_dir,\n",
    "                     upload = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hx-EaR7wi0Fg"
   },
   "outputs": [],
   "source": [
    "if download_data and colab_mode:\n",
    "  for f in file_name:\n",
    "    googledrive_fetch(file_name = f, \n",
    "                      fetch=True, \n",
    "                      latest = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u2ef8JKZCUZA"
   },
   "outputs": [],
   "source": [
    "command = ['mkdir ./{}/'.format(dataset_id),\n",
    "           'mkdir ./{}/train/'.format(dataset_id),\n",
    "           'sudo apt-get install p7zip-full',\n",
    "           '7z e ./train_{}.zip -o./{}/train/ -r'.format(dataset_id,\n",
    "                                                         dataset_id),\n",
    "           'rm ./train_{}.zip'.format(dataset_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DGAqlsugFX2a"
   },
   "outputs": [],
   "source": [
    "if setup:\n",
    "  execute_in_shell(command = command, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "etkuxdZIbnBz"
   },
   "outputs": [],
   "source": [
    "command = ['mkdir ./{}/'.format(dataset_id),\n",
    "           'mkdir ./{}/test/'.format(dataset_id),\n",
    "           'mkdir ./{}/train/'.format(dataset_id),\n",
    "           'mkdir ./{}/train/cats/'.format(dataset_id),\n",
    "           'mkdir ./{}/train/dogs/'.format(dataset_id),\n",
    "           'unzip -q ./train_{}.zip -d ./{}/train/'.format(dataset_id,\n",
    "                                                           dataset_id),\n",
    "           'unzip -q ./test_{}.zip  -d ./{}/test/'.format(dataset_id,\n",
    "                                                          dataset_id),\n",
    "           'mv ./{}/train/train/cat*.jpg ./{}/train/cats/'.format(dataset_id,\n",
    "                                                             dataset_id),\n",
    "           'mv ./{}/train/train/dog*.jpg ./{}/train/dogs/'.format(dataset_id,\n",
    "                                                             dataset_id),\n",
    "           \"find ./{}/train/ -name 'dog*.jpg' -exec mv --target-directory=./{}/train/dogs/ '{}' +\".format(dataset_id,\n",
    "                                                                                                            dataset_id,\n",
    "                                                                                                            {}),\n",
    "           \"find ./{}/train/ -name 'cat*.jpg' -exec mv --target-directory=./{}/train/cats/ '{}' +\".format(dataset_id,\n",
    "                                                                                                            dataset_id,\n",
    "                                                                                                            {}),\n",
    "           \n",
    "           'mv ./{}/train/dog*.jpg ./{}/train/dogs/'.format(dataset_id,\n",
    "                                                             dataset_id),\n",
    "           'mv ./{} ./{}/{}'.format(MODEL_NAME,\n",
    "                                    dataset_id,\n",
    "                                    MODEL_NAME),\n",
    "           'mkdir ./{}/checkpoint/'.format(dataset_id),\n",
    "           'mv ./Transfer_learn_299_299_{}.h5 ./{}/checkpoint/Transfer_learn_299_299_.h5'.format(dataset_id,\n",
    "                                                                                                 dataset_id),\n",
    "           'rm ./*.zip',\n",
    "           'rm -r ./{}/train/train'.format(dataset_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bweAmV3wdjBz"
   },
   "outputs": [],
   "source": [
    "if setup:\n",
    "  execute_in_shell(command = command, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZzQrHQOVQLnp"
   },
   "outputs": [],
   "source": [
    "def generate_timestamp():\n",
    "    \"\"\" \n",
    "        A function to generate time-stamp information.\n",
    "        Calling the function returns a string formatted current system time.\n",
    "        Eg: 2018_10_10_10_10_10\n",
    "    \n",
    "        Example usage: generate_timestamp() \n",
    "    \"\"\"    \n",
    "    timestring = time.strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "    print (\"Time stamp generated: \" + timestring)\n",
    "    return timestring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oA4PW76mSny8"
   },
   "outputs": [],
   "source": [
    "timestr = generate_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9aRvCWIjQOJS"
   },
   "outputs": [],
   "source": [
    "def is_valid_file(parser, arg):\n",
    "    \"\"\"\n",
    "        A function that checks if a give file path contains a valid file or not.\n",
    "        \n",
    "        The function returns the full file path if there is a valid file persent.\n",
    "        If there is no valid file present at a file path location, it returns a parser error message.\n",
    "        \n",
    "        Takes two positional arguments: parser and arg\n",
    "        \n",
    "        Example usage: \n",
    "            import argsparse\n",
    "            \n",
    "            a = argparse.ArgumentParser()\n",
    "            a.add_argument(\"--file_path\", \n",
    "                              help = \"Check if a file exists in the specified file path ...\", \n",
    "                              dest = \"file_path\", \n",
    "                              required=False,\n",
    "                              type=lambda x: is_valid_file(a, x),\n",
    "                              nargs=1)\n",
    "            \n",
    "            args = a.parse_args()\n",
    "            \n",
    "            args = get_user_options()\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(arg):\n",
    "        try:\n",
    "            parser.error(\"The file %s does not exist ...\" % arg)\n",
    "            return None\n",
    "        except:\n",
    "            if parser != None:\n",
    "                print (\"No valid argument parser found ...\")\n",
    "                print (\"The file %s does not exist ...\" % arg)\n",
    "                return None\n",
    "            else:\n",
    "                print (\"The file %s does not exist ...\" % arg)\n",
    "                return None\n",
    "    else:\n",
    "        return arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UF4q-hlSQQhu"
   },
   "outputs": [],
   "source": [
    "def is_valid_dir(parser, arg):\n",
    "    \"\"\"\n",
    "        This function checks if a directory exists or not.\n",
    "        It can be used inside the argument parser.\n",
    "        \n",
    "        Example usage: \n",
    "            \n",
    "            import argsparse\n",
    "            \n",
    "            a = argparse.ArgumentParser()\n",
    "            a.add_argument(\"--dir_path\", \n",
    "                              help = \"Check if a file exists in the specified file path ...\", \n",
    "                              dest = \"file_path\", \n",
    "                              required=False,\n",
    "                              type=lambda x: is_valid_dir(a, x),\n",
    "                              nargs=1)\n",
    "            \n",
    "            args = a.parse_args()\n",
    "            \n",
    "            args = get_user_options() \n",
    "    \"\"\"\n",
    "    if not os.path.isdir(arg):\n",
    "        try:\n",
    "            return parser.error(\"The folder %s does not exist ...\" % arg)\n",
    "        except:\n",
    "            if parser != None:\n",
    "                print (\"No valid argument parser found\")\n",
    "                print (\"The folder %s does not exist ...\" % arg)\n",
    "                return None\n",
    "            else:\n",
    "                print (\"The folder %s does not exist ...\" % arg)\n",
    "                return None\n",
    "    else:\n",
    "        return arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VDQ3U68yS7MQ"
   },
   "outputs": [],
   "source": [
    "def string_to_bool(val):\n",
    "    \"\"\"\n",
    "        A function that checks if an user argument is boolean or not.\n",
    "        \n",
    "        Example usage:\n",
    "            \n",
    "            \n",
    "                import argsparse\n",
    "            \n",
    "                a = argparse.ArgumentParser()\n",
    "                \n",
    "                a.add_argument(\"--some_bool_arg\", \n",
    "                   help = \"Specify a boolean argument ...\", \n",
    "                   dest = \"some_bool_arg\", \n",
    "                   required=False, \n",
    "                   default=[True], \n",
    "                   nargs=1, \n",
    "                   type = string_to_bool)\n",
    "                \n",
    "            args = a.parse_args()\n",
    "            \n",
    "            args = get_user_options()\n",
    "            \n",
    "    \"\"\"\n",
    "    if val.lower() in ('yes', 'true', 't', 'y', '1', 'yeah', 'yup'):\n",
    "        return True\n",
    "    elif val.lower() in ('no', 'false', 'f', 'n', '0', 'none', 'nope'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_fx7uJrfwjyZ"
   },
   "outputs": [],
   "source": [
    "def activation_val(val):\n",
    "    activation_function_options = ('hard_sigmoid',\n",
    "                                   'elu',\n",
    "                                   'linear',\n",
    "                                   'relu', \n",
    "                                   'selu', \n",
    "                                   'sigmoid',\n",
    "                                   'softmax',\n",
    "                                   'softplus',\n",
    "                                   'sofsign',\n",
    "                                   'tanh')\n",
    "    if val.lower() in activation_function_options:\n",
    "        return val\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Unexpected activation function. \\\n",
    "                                         \\nExpected values are:  {} ...'.format(activation_function_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OjxX4blfTAqc"
   },
   "outputs": [],
   "source": [
    "def loss_val(val):\n",
    "    loss_function_options = ('mean_squared_error',\n",
    "                             'mean_absolute_error',\n",
    "                             'mean_absolute_percentage_error',\n",
    "                             'mean_squared_logarithmic_error', \n",
    "                             'squared_hinge', \n",
    "                             'hinge',\n",
    "                             'categorical_hinge',\n",
    "                             'logcosh',\n",
    "                             'categorical_crossentropy',\n",
    "                             'sparse_categorical_crossentropy',\n",
    "                             'binary_crossentropy',\n",
    "                             'kullback_leibler_divergence',\n",
    "                             'poisson',\n",
    "                             'cosine_proximity')\n",
    "    if val.lower() in loss_function_options:\n",
    "        return val\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Unexpected loss function. \\\n",
    "                                         \\nExpected values are:  {} ...'.format(loss_function_options))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p_6KPfrLTFDb"
   },
   "outputs": [],
   "source": [
    "def get_nb_files(directory):\n",
    "  if not os.path.exists(directory):\n",
    "    return 0\n",
    "  cnt = 0\n",
    "  for r, dirs, files in os.walk(directory):\n",
    "    for dr in dirs:\n",
    "      cnt += len(glob.glob(os.path.join(r, dr + \"/*\")))\n",
    "  return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6_60icE9TLcx"
   },
   "outputs": [],
   "source": [
    "def add_top_layer(args, enable_dropout, base_model, nb_classes):\n",
    "  \"\"\"\n",
    "    This functions adds a fully connected convolutional neural network layer to a base model.\n",
    "    \n",
    "    The required input arguments for this function are: args, base_model and nb_classes.\n",
    "        args: argument inputs the user arguments to be passed to the function,\n",
    "        base_model: argument inputs the base model architecture to be added to the top layer,\n",
    "        nb_classes: argument inputs the total number of classes for the output layer.    \n",
    "  \"\"\"\n",
    "  try:\n",
    "      dropout = float(args.dropout[0])\n",
    "      weight_decay = float(args.decay[0])\n",
    "  except:\n",
    "      dropout = DEFAULT_DROPOUT\n",
    "      print ('Invalid input for dropout ...')\n",
    "      \n",
    "  try:\n",
    "      activation = str(args.activation[0]).lower()\n",
    "      print ('Building model using activation function: ' + str(activation))\n",
    "  except:\n",
    "      activation = 'relu'\n",
    "      print ('Invalid input for activation function ...')\n",
    "      print ('Choice of activation functions: hard_sigmoid, elu, linear, relu, selu, sigmoid, softmax, softplus, sofsign, tanh ...')\n",
    "      print ('Building model using default activation function: relu')\n",
    "      \n",
    "  bm = base_model.output\n",
    "  \n",
    "  x = Dropout(dropout,\n",
    "              name='dropout_fc1')(bm,\n",
    "                       training=enable_dropout)\n",
    "  x = GlobalAveragePooling2D(name='gloablAveragePooling2D_fc1')(x)\n",
    "  x = Dropout(dropout,\n",
    "              name='dropout_fc2')(x,\n",
    "                       training=enable_dropout)\n",
    "  x = BatchNormalization(name='batchNormalization_fc1')(x)\n",
    "  x = Dense(FC_SIZE, \n",
    "            activation=activation,\n",
    "            kernel_regularizer=l2(weight_decay),\n",
    "            name='dense_fc1')(x)\n",
    "  x = Dropout(dropout,\n",
    "              name='dropout_fc3')(x,\n",
    "                       training=enable_dropout)\n",
    "  \n",
    "  x1 = Dense(FC_SIZE, \n",
    "             activation=activation,\n",
    "             kernel_regularizer=l2(weight_decay),\n",
    "             name=\"dense_fc2\")(x)\n",
    "  x1 = Dropout(dropout,\n",
    "               name = 'dropout_fc4')(x1, \n",
    "                                  training=enable_dropout)\n",
    "  x1 = BatchNormalization(name=\"batchNormalization_fc2\")(x1)\n",
    "  x1 = Dense(FC_SIZE, \n",
    "             activation=activation, \n",
    "             kernel_regularizer=l2(weight_decay),\n",
    "             name=\"dense_fc3\")(x1)\n",
    "  x1 = Dropout(dropout,\n",
    "               name = 'dropout_fc5')(x1, \n",
    "                                  training=enable_dropout)\n",
    "\n",
    "  x2 = Dense(FC_SIZE, \n",
    "             activation=activation, \n",
    "             kernel_regularizer=l2(weight_decay),\n",
    "             name=\"dense_fc4\")(x)\n",
    "  x2 = Dropout(dropout,\n",
    "               name = 'dropout_fc6')(x2, \n",
    "                                  training=enable_dropout)\n",
    "  x2 = BatchNormalization(name=\"batchNormalization_fc3\")(x2)\n",
    "  x2 = Dense(FC_SIZE, \n",
    "             activation=activation, \n",
    "             kernel_regularizer=l2(weight_decay),\n",
    "             name=\"dense_fc5\")(x2)\n",
    "  x2 = Dropout(dropout,\n",
    "               name = 'dropout_fc7')(x2, \n",
    "                                  training=enable_dropout)\n",
    "\n",
    "  x12 = concatenate([x1, x2], name = 'mixed11')\n",
    "  x12 = Dropout(dropout,\n",
    "                name = 'dropout_fc8')(x12, \n",
    "                                   training=enable_dropout)\n",
    "  x12 = Dense(FC_SIZE//16, \n",
    "              activation=activation, \n",
    "              kernel_regularizer=l2(weight_decay),\n",
    "              name = 'dense_fc6')(x12)\n",
    "  x12 = Dropout(dropout,\n",
    "                name = 'dropout_fc9')(x12, \n",
    "                                   training=enable_dropout)\n",
    "  x12 = BatchNormalization(name=\"batchNormalization_fc4\")(x12)\n",
    "  x12 = Dense(FC_SIZE//32, \n",
    "              activation=activation, \n",
    "              kernel_regularizer=l2(weight_decay),\n",
    "              name = 'dense_fc7')(x12)\n",
    "  x12 = Dropout(dropout,\n",
    "                name = 'dropout_fc10')(x12, \n",
    "                                   training=enable_dropout)\n",
    "  \n",
    "  x3 = Dropout(dropout,\n",
    "              name='dropout_fc11')(bm,\n",
    "                       training=enable_dropout)\n",
    "  x3 = GlobalAveragePooling2D( name = 'globalAveragePooling2D_fc2')(x3)\n",
    "  x3 = Dense(FC_SIZE//2, \n",
    "             activation=activation, \n",
    "             kernel_regularizer=l2(weight_decay),\n",
    "             name = 'dense_fc8')(x3)\n",
    "  x3 = Dropout(dropout,\n",
    "               name = 'dropout_fc12')(x3, \n",
    "                                  training=enable_dropout)\n",
    "  x3 = BatchNormalization(name=\"batchNormalization_fc5\")(x3)\n",
    "  x3 = Dense(FC_SIZE//2, \n",
    "             activation=activation, \n",
    "             kernel_regularizer=l2(weight_decay),\n",
    "             name = 'dense_fc9')(x3)\n",
    "  x3 = Dropout(dropout,\n",
    "               name = 'dropout_fc13')(x3, \n",
    "                                  training=enable_dropout)\n",
    "  \n",
    "  xout = concatenate([x12, x3], name ='mixed12')\n",
    "  xout = Dense(FC_SIZE//32, \n",
    "               activation= activation, \n",
    "               kernel_regularizer=l2(weight_decay),\n",
    "               name = 'dense_fc10')(xout)\n",
    "  xout = Dropout(dropout,\n",
    "                 name = 'dropout_fc14')(xout, \n",
    "                                     training=enable_dropout)\n",
    "  \n",
    "  predictions = Dense(nb_classes,           \\\n",
    "                      activation='softmax', \\\n",
    "                      kernel_regularizer=l2(weight_decay),\n",
    "                      name='prediction')(xout) # Softmax output layer\n",
    "  \n",
    "  model = Model(inputs=base_model.input, \n",
    "                outputs=predictions)\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZcRpb31gTSps"
   },
   "outputs": [],
   "source": [
    "def finetune_model(model, optimizer, loss, NB_FROZEN_LAYERS):\n",
    "  \"\"\"\n",
    "      A function that freezes the bottom NB_LAYERS and retrain the remaining top layers.\n",
    "      \n",
    "      The required input arguments for this function are: model, optimizer and NB_FROZEN_LAYERS.\n",
    "          model: inputs a model architecture with base layers to be frozen during training,\n",
    "          optimizer: inputs a choice of optimizer value for compiling the model,\n",
    "          loss: inputs a choice for loss function used for compiling the model,\n",
    "          NB_FROZEN_LAYERS: inputs a number that selects the total number of base layers to be frozen during training.\n",
    "      \n",
    "  \"\"\"\n",
    "                     \n",
    "  for layer in model.layers[:NB_FROZEN_LAYERS]:\n",
    "     layer.trainable = False\n",
    "  for layer in model.layers[NB_FROZEN_LAYERS:]:\n",
    "     layer.trainable = True\n",
    "  model.compile(optimizer=optimizer, \n",
    "                loss=loss, \n",
    "                metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QJ4Xj1IMTXWB"
   },
   "outputs": [],
   "source": [
    "def transferlearn_model(model, base_model, optimizer, loss):\n",
    "  \"\"\"\n",
    "     Function that freezes the base layers to train just the top layer.\n",
    "     \n",
    "     This function takes three positional arguments:\n",
    "         model: specifies the input model,\n",
    "         base_model: specifies the base model architecture,\n",
    "         optimizer: optimizer function for training the model,\n",
    "         loss: loss function for compiling the model\n",
    "     \n",
    "     Example usage:\n",
    "         transferlearn_model(model, base_model, optimizer)\n",
    "  \"\"\"\n",
    "  for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "  model.compile(optimizer=optimizer, \n",
    "                loss=loss, \n",
    "                metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "907hvyFWTXvH"
   },
   "outputs": [],
   "source": [
    "def save_model(args, name, model):\n",
    "    file_loc = args.output_dir[0]\n",
    "    file_pointer = os.path.join(file_loc+\"//trained_\"+ timestr)\n",
    "    model.save_weights(os.path.join(file_pointer + \"_weights\"+str(name)+\".model\"))\n",
    "    \n",
    "    model_json = model.to_json()                                                # Serialize model to JSON\n",
    "    with open(os.path.join(file_pointer+\"_config\"+str(name)+\".json\"), \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    print (\"Saved the trained model weights to: \" + \n",
    "           str(os.path.join(file_pointer + \"_weights\"+str(name)+\".model\")))\n",
    "    print (\"Saved the trained model configuration as a json file to: \" + \n",
    "           str(os.path.join(file_pointer+\"_config\"+str(name)+\".json\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3O0Jz7gCTk9P"
   },
   "outputs": [],
   "source": [
    "def generate_labels(args):\n",
    "    file_loc = args.output_dir[0]\n",
    "    file_pointer = os.path.join(file_loc+\"//trained_labels\")\n",
    "    \n",
    "    data_dir = args.train_dir[0]\n",
    "    val_dir_ = args.val_dir[0]\n",
    "    \n",
    "    dt = defaultdict(list)\n",
    "    dv = defaultdict(list)\n",
    "    \n",
    "    for root, subdirs, files in os.walk(data_dir):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            assert file_path.startswith(data_dir)\n",
    "            suffix = file_path[len(data_dir):]\n",
    "            suffix = suffix.lstrip(\"/\")\n",
    "            label = suffix.split(\"/\")[0]\n",
    "            dt[label].append(file_path)\n",
    "            \n",
    "    for root, subdirs, files in os.walk(val_dir_):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            assert file_path.startswith(val_dir_)\n",
    "            suffix = file_path[len(val_dir_):]\n",
    "            suffix = suffix.lstrip(\"/\")\n",
    "            label = suffix.split(\"/\")[0]\n",
    "            dv[label].append(file_path)\n",
    "\n",
    "    labels = sorted(dt.keys())\n",
    "    val_labels = sorted(dv.keys())\n",
    "    \n",
    "    if set(labels) == set (val_labels):\n",
    "        print(\"\\nTraining labels: \" + str(labels))\n",
    "        print(\"\\nValidation labels: \" + str(val_labels))\n",
    "        with open(os.path.join(file_pointer+\".json\"), \"w\") as json_file:\n",
    "            json.dump(labels, json_file)\n",
    "    else:\n",
    "      print(\"\\nTraining labels: \" + str(labels))\n",
    "      print(\"\\nValidation labels: \" + str(val_labels))\n",
    "      print (\"Mismatched training and validation data labels ...\")\n",
    "      print (\"Sub-folder names do not match between training and validation directories ...\")\n",
    "      sys.exit(1)\n",
    "\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IL_cSYVmTpBc"
   },
   "outputs": [],
   "source": [
    "def normalize(args, \n",
    "              labels, \n",
    "              move = False, \n",
    "              sub_sample = False):\n",
    "    if args.normalize[0] and os.path.exists(args.root_dir[0]):      \n",
    "        commands = [\"rm -r {}/.tmp_train/\".format(args.root_dir[0]),\n",
    "                    \"rm -r {}/.tmp_validation/\".format(args.root_dir[0]),\n",
    "                    \"mkdir {}/.tmp_train/\".format(args.root_dir[0]),\n",
    "                    \"mkdir {}/.tmp_validation/\".format(args.root_dir[0])]\n",
    "        execute_in_shell(command=commands,\n",
    "                         verbose=verbose)\n",
    "        del commands\n",
    "        \n",
    "        mk_train_folder = \"mkdir -p {}/.tmp_train/\".format(args.root_dir[0]) + \"{}\"\n",
    "        mk_val_folder = \"mkdir -p {}/.tmp_validation/\".format(args.root_dir[0]) + \"{}\"\n",
    "        \n",
    "        train_class_sizes = []\n",
    "        val_class_sizes = []\n",
    "        \n",
    "        for label in labels:\n",
    "            train_class_sizes.append(len(glob.glob(args.train_dir[0] + \"/{}/*\".format(label))))\n",
    "            val_class_sizes.append(len(glob.glob(args.val_dir[0] + \"/{}/*\".format(label))))\n",
    "        \n",
    "        train_size = min(train_class_sizes)\n",
    "        val_size = min(val_class_sizes)\n",
    "        \n",
    "        if sub_sample and 0 <= args.train_sub_sample[0] <=1 and 0 <= args.val_sub_sample[0] <=1 :\n",
    "            train_size = int(train_size * args.train_sub_sample[0])\n",
    "            val_size = int(val_size * args.val_sub_sample[0])\n",
    "        \n",
    "        print (\"Normalized training class size {}\".format(train_size))\n",
    "        print (\"Normalized validation class size {}\".format(val_size))\n",
    "        \n",
    "        for label in labels:\n",
    "            commands = [mk_train_folder.format(label),\n",
    "                        mk_val_folder.format(label)]\n",
    "        \n",
    "            execute_in_shell(command=commands,\n",
    "                             verbose=verbose)\n",
    "            del commands\n",
    "        \n",
    "        commands = []\n",
    "        \n",
    "        for label in labels:\n",
    "            train_images = (glob.glob('{}/{}/*.*'.format(args.train_dir[0], label), recursive=True))\n",
    "            val_images = (glob.glob('{}/{}/*.*'.format(args.val_dir[0], label), recursive=True))\n",
    "            \n",
    "            sys_rnd = random.SystemRandom()\n",
    "            \n",
    "            if move:\n",
    "              cmd = 'mv'\n",
    "            else:\n",
    "              cmd = 'cp'\n",
    "            \n",
    "            for file in sys_rnd.sample(train_images, train_size):\n",
    "                if os.path.exists(file):\n",
    "                    commands.append('{} {} ./.tmp_train/{}/'.format(cmd, file, label))\n",
    "            \n",
    "            for file in sys_rnd.sample(val_images, val_size):\n",
    "                if os.path.exists(file):\n",
    "                    commands.append('{} {} ./.tmp_validation/{}/'.format(cmd, file, label))\n",
    "                \n",
    "            p = Process(target=execute_in_shell, args=([commands]))\n",
    "            p.start()\n",
    "            p.join()\n",
    "        print (\"\\nData normalization pipeline completed successfully ...\")\n",
    "    else:\n",
    "        print (\"\\nFailed to initiate data normalization pipeline ...\")\n",
    "        return False    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NbCxb0NwTx52"
   },
   "outputs": [],
   "source": [
    "def generate_plot(args, name, model_train):\n",
    "    gen_plot = args.plot[0]\n",
    "    if gen_plot==True:\n",
    "        plot_training(args, name, model_train)\n",
    "    else:\n",
    "        print (\"\\nNo training summary plots generated ...\")\n",
    "        print (\"Set: --plot True for creating training summary plots\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iP8gd9_kT0K9"
   },
   "outputs": [],
   "source": [
    "def plot_training(args, name, history):\n",
    "  output_loc = args.output_dir[0]\n",
    "  \n",
    "  output_file_acc = os.path.join(output_loc+\n",
    "                                 \"//training_plot_acc_\" + \n",
    "                                 timestr+str(name)+\".png\")\n",
    "  output_file_loss = os.path.join(output_loc+\n",
    "                                  \"//training_plot_loss_\" + \n",
    "                                  timestr+str(name)+\".png\")\n",
    "  fig_acc = plt.figure()\n",
    "  plt.plot(history.history['acc'])\n",
    "  plt.plot(history.history['val_acc'])\n",
    "  plt.title('model accuracy')\n",
    "  plt.ylabel('accuracy')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train', 'test'], loc='upper left')\n",
    "  fig_acc.savefig(output_file_acc, dpi=fig_acc.dpi)\n",
    "  print (\"Successfully created the training accuracy plot: \" \n",
    "         + str(output_file_acc))\n",
    "  plt.close()\n",
    "\n",
    "  fig_loss = plt.figure()\n",
    "  plt.plot(history.history['loss'])\n",
    "  plt.plot(history.history['val_loss'])\n",
    "  plt.title('model loss')\n",
    "  plt.ylabel('loss')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train', 'test'], loc='upper left')\n",
    "  fig_loss.savefig(output_file_loss, dpi=fig_loss.dpi)\n",
    "  print (\"Successfully created the loss function plot: \" \n",
    "         + str(output_file_loss))\n",
    "  plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SAW1lgZST4MS"
   },
   "outputs": [],
   "source": [
    "def select_optimizer(args):\n",
    "  optimizer_val = args.optimizer_val[0]\n",
    "  lr = args.learning_rate[0]\n",
    "  decay = args.decay[0]\n",
    "  epsilon = args.epsilon[0]\n",
    "  rho = args.rho[0]\n",
    "  beta_1 = args.beta_1[0]\n",
    "  beta_2 = args.beta_2[0]\n",
    "  \n",
    "  if optimizer_val.lower() == 'sgd' :\n",
    "    optimizer = SGD(lr=lr,       \\\n",
    "                    decay=decay, \\\n",
    "                    momentum=1,  \\\n",
    "                    nesterov=False)\n",
    "    print (\"Using SGD as the optimizer ...\")\n",
    "  elif optimizer_val.lower() == 'nsgd':\n",
    "    optimizer = SGD(lr=lr,      \\\n",
    "                    decay=decay,\\\n",
    "                    momentum=1, \\\n",
    "                    nesterov=True)\n",
    "    print (\"Using SGD as the optimizer with Nesterov momentum ...\")\n",
    "  elif optimizer_val.lower() == 'rms' \\\n",
    "       or \\\n",
    "       optimizer_val.lower() == 'rmsprop':\n",
    "    optimizer = RMSprop(lr=lr,          \\\n",
    "                        rho=rho,        \\\n",
    "                        epsilon=epsilon,\\\n",
    "                        decay=decay)\n",
    "    print (\"Using RMSProp as the optimizer ...\")\n",
    "  elif optimizer_val.lower() == 'ada' \\\n",
    "       or \\\n",
    "       optimizer_val.lower() == 'adagrad':\n",
    "    optimizer = Adagrad(lr=lr,           \\\n",
    "                        epsilon=epsilon, \\\n",
    "                        decay=decay)\n",
    "    print (\"Using Adagrad as the optimizer ...\")\n",
    "  elif optimizer_val.lower() == 'adelta' \\\n",
    "       or \\\n",
    "       optimizer_val.lower() == 'adadelta':\n",
    "    optimizer = Adadelta(lr=lr,           \\\n",
    "                         rho=rho,         \\\n",
    "                         epsilon=epsilon, \\\n",
    "                         decay=decay)\n",
    "    print (\"Using Adadelta as the optimizer ...\")\n",
    "  elif optimizer_val.lower() == 'adam':\n",
    "    optimizer = Adam(lr=lr,           \\\n",
    "                     beta_1=beta_1,   \\\n",
    "                     beta_2=beta_2,    \\\n",
    "                     epsilon=epsilon, \\\n",
    "                     decay=decay,     \\\n",
    "                     amsgrad=False)\n",
    "    print (\"Using Adam as the optimizer ...\")\n",
    "    print (\"Optimizer parameters (recommended default): \")\n",
    "    print (\"\\n lr={} (0.001),     \\\n",
    "            \\n beta_1={} (0.9),   \\\n",
    "            \\n beta_2={} (0.999), \\\n",
    "            \\n epsilon={} (1e-08), \\\n",
    "            \\n decay={} (0.0)\".format(lr, \n",
    "                                      beta_1, \n",
    "                                      beta_2, \n",
    "                                      epsilon, \n",
    "                                      decay))\n",
    "  elif optimizer_val.lower() == 'amsgrad':\n",
    "    optimizer = Adam(lr=lr,           \\\n",
    "                     beta_1=beta_1,   \\\n",
    "                     beta_2=beta_2,    \\\n",
    "                     epsilon=epsilon, \\\n",
    "                     decay=decay,     \\\n",
    "                     amsgrad=True)\n",
    "    print (\"Using AmsGrad variant of Adam as the optimizer ...\")\n",
    "    print (\"Optimizer parameters (recommended default): \")\n",
    "    print (\"\\n lr={} (0.001),     \\\n",
    "            \\n beta_1={} (0.9),   \\\n",
    "            \\n beta_2={} (0.999), \\\n",
    "            \\n epsilon={} (1e-08), \\\n",
    "            \\n decay={} (0.0)\".format(lr, \n",
    "                                      beta_1, \n",
    "                                      beta_2, \n",
    "                                      epsilon, \n",
    "                                      decay))\n",
    "  elif optimizer_val.lower() == 'adamax':  \n",
    "    optimizer = Adamax(lr=lr,           \\\n",
    "                       beta_1=beta_1,   \\\n",
    "                       beta_2=beta_2,    \\\n",
    "                       epsilon=epsilon, \\\n",
    "                       decay=decay)\n",
    "    print (\"Using Adamax variant of Adam as the optimizer ...\")\n",
    "    print (\"Optimizer parameters (recommended default): \")\n",
    "    print (\"\\n lr={} (0.002),     \\\n",
    "            \\n beta_1={} (0.9),   \\\n",
    "            \\n beta_2={} (0.999), \\\n",
    "            \\n epsilon={} (1e-08), \\\n",
    "            \\n schedule_decay={} (0.0)\".format(lr, \n",
    "                                               beta_1, \n",
    "                                               beta_2, \n",
    "                                               epsilon, \n",
    "                                               decay))\n",
    "  elif optimizer_val.lower() == 'nadam':  \n",
    "    optimizer = Nadam(lr=lr,            \\\n",
    "                      beta_1=beta_1,    \\\n",
    "                      beta_2=beta_2,     \\\n",
    "                      epsilon=epsilon,  \\\n",
    "                      schedule_decay=decay)\n",
    "    print (\"Using Nesterov Adam optimizer ...\\\n",
    "           \\n decay arguments is passed on to schedule_decay variable ...\")\n",
    "    print (\"Optimizer parameters (recommended default): \")\n",
    "    print (\"\\n lr={} (0.002),     \\\n",
    "            \\n beta_1={} (0.9),   \\\n",
    "            \\n beta_2={} (0.999), \\\n",
    "            \\n epsilon={} (1e-08), \\\n",
    "            \\n schedule_decay={} (0.004)\".format(lr, \n",
    "                                                 beta_1, \n",
    "                                                 beta_2, \n",
    "                                                 epsilon, \n",
    "                                                 decay))\n",
    "  else:\n",
    "      optimizer = DEFAULT_OPTIMIZER\n",
    "      print (\"Using stochastic gradient descent with Nesterov momentum ('nsgd') as the default optimizer ...\")\n",
    "      print (\"Options for optimizer are: 'sgd',        \\\n",
    "                                         \\n'nsgd',     \\\n",
    "                                         \\n'rmsprop',  \\\n",
    "                                         \\n'adagrad',  \\\n",
    "                                         \\n'adadelta', \\\n",
    "                                         \\n'adam',     \\\n",
    "                                         \\n'nadam',    \\\n",
    "                                         \\n'amsgrad',  \\\n",
    "                                         \\n'adamax' ...\")\n",
    "  return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jeTTS3ydT_kP"
   },
   "outputs": [],
   "source": [
    "def process_model(args, \n",
    "                  model, \n",
    "                  base_model, \n",
    "                  optimizer, \n",
    "                  loss, \n",
    "                  checkpointer_savepath):\n",
    "  load_weights_ = args.load_weights[0]\n",
    "  fine_tune_model = args.fine_tune[0]\n",
    "  load_checkpoint = args.load_checkpoint[0]\n",
    "   \n",
    "  if load_weights_ == True:     \n",
    "      try:\n",
    "          with open(args.config_file[0]) as json_file:\n",
    "              model_json = json_file.read()\n",
    "          model = model_from_json(model_json)\n",
    "      except:\n",
    "          model = model\n",
    "      try:\n",
    "          model.load_weights(args.weights_file[0])\n",
    "          print (\"\\nLoaded model weights from: \" + str(args.weights_file[0]))\n",
    "      except:\n",
    "          print (\"\\nError loading model weights ...\")\n",
    "          print (\"Tabula rasa ...\")\n",
    "          print (\"Loaded default model weights ...\")\n",
    "  elif load_checkpoint == True and os.path.exists(checkpointer_savepath):     \n",
    "      try:\n",
    "          model = load_model(checkpointer_savepath)\n",
    "          print (\"\\nLoaded model from checkpoint: \" + str(checkpointer_savepath))\n",
    "      except:\n",
    "          if os.path.exists(args.saved_chkpnt[0]):\n",
    "            model = load_model(args.saved_chkpnt[0])\n",
    "            print ('\\nLoaded saved checkpoint file ...')\n",
    "          else:\n",
    "            print (\"\\nError loading model checkpoint ...\")\n",
    "            print (\"Tabula rasa ...\")\n",
    "            print (\"Loaded default model weights ...\")\n",
    "  else:\n",
    "      model = model\n",
    "      print (\"\\nTabula rasa ...\")\n",
    "      print (\"Loaded default model weights ...\")\n",
    " \n",
    "  try:\n",
    "      NB_FROZEN_LAYERS = args.frozen_layers[0]\n",
    "  except:\n",
    "      NB_FROZEN_LAYERS = DEFAULT_NB_LAYERS_TO_FREEZE\n",
    "      \n",
    "  if fine_tune_model == True:\n",
    "      print (\"\\nFine tuning Inception architecture ...\")\n",
    "      print (\"Frozen layers: \" + str(NB_FROZEN_LAYERS))\n",
    "      model = finetune_model(model, optimizer, loss, NB_FROZEN_LAYERS)\n",
    "  else:\n",
    "      print (\"\\nTransfer learning using Inception architecture ...\")\n",
    "      model = transferlearn_model(model, base_model, optimizer, loss)\n",
    "      \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gljUeGs8UK1w"
   },
   "outputs": [],
   "source": [
    "def process_images(args):  \n",
    "  train_aug = args.train_aug[0] \n",
    "  test_aug = args.test_aug[0] \n",
    "   \n",
    "  if str((args.base_model[0]).lower()) == 'inceptionv4' or  \\\n",
    "     str((args.base_model[0]).lower()) == 'inception_v4' or \\\n",
    "     str((args.base_model[0]).lower()) == 'inception_resnet':\n",
    "      preprocess_input = preprocess_input_inceptionv4\n",
    "  else:\n",
    "      preprocess_input = preprocess_input_inceptionv3\n",
    "  \n",
    "  if train_aug==True:\n",
    "    try:\n",
    "        train_rotation_range = args.train_rot[0]\n",
    "        train_width_shift_range = args.train_w_shift[0]\n",
    "        train_height_shift_range = args.train_ht_shift[0]\n",
    "        train_shear_range = args.train_shear[0]\n",
    "        train_zoom_range = args.train_zoom[0]\n",
    "        train_vertical_flip = args.train_vflip[0]\n",
    "        train_horizontal_flip = args.train_hflip[0]\n",
    "    except:\n",
    "        train_rotation_range = 30\n",
    "        train_width_shift_range = 0.2\n",
    "        train_height_shift_range = 0.2\n",
    "        train_shear_range = 0.2\n",
    "        train_zoom_range = 0.2\n",
    "        train_vertical_flip = True\n",
    "        train_horizontal_flip = True\n",
    "        print (\"\\nFailed to load custom training image augmentation parameters ...\")\n",
    "        print (\"Loaded pre-set defaults ...\")\n",
    "        print (\"To switch off image augmentation during training, set --train_augmentation flag to False\")\n",
    "        \n",
    "    train_datagen =  ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                        rotation_range=train_rotation_range,\n",
    "                                        width_shift_range=train_width_shift_range,\n",
    "                                        height_shift_range=train_height_shift_range,\n",
    "                                        shear_range=train_shear_range,\n",
    "                                        zoom_range=train_zoom_range,\n",
    "                                        vertical_flip=train_vertical_flip,                                  \n",
    "                                        horizontal_flip=train_horizontal_flip)\n",
    "    print (\"\\nCreated image augmentation pipeline for training images ...\")     \n",
    "    print (\"Image augmentation parameters for training images: \\\n",
    "          \\n image rotation range = {},\\\n",
    "          \\n width shift range = {},\\\n",
    "          \\n height shift range = {}, \\\n",
    "          \\n shear range = {} ,\\\n",
    "          \\n zoom range = {}, \\\n",
    "          \\n enable vertical flip = {}, \\\n",
    "          \\n enable horizontal flip = {}\".format(train_rotation_range,\n",
    "                                                   train_width_shift_range,\n",
    "                                                   train_height_shift_range,\n",
    "                                                   train_shear_range,\n",
    "                                                   train_zoom_range,\n",
    "                                                   train_vertical_flip,\n",
    "                                                   train_horizontal_flip))\n",
    "  else:\n",
    "      train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "  \n",
    "  if test_aug==True:\n",
    "      try:\n",
    "        test_rotation_range = args.test_rot[0]\n",
    "        test_width_shift_range = args.test_w_shift[0]\n",
    "        test_height_shift_range = args.test_ht_shift[0]\n",
    "        test_shear_range = args.test_shear[0]\n",
    "        test_zoom_range = args.test_zoom[0]\n",
    "        test_vertical_flip = args.test_vflip[0]\n",
    "        test_horizontal_flip = args.test_hflip[0]\n",
    "      except:\n",
    "        test_rotation_range = 30\n",
    "        test_width_shift_range = 0.2\n",
    "        test_height_shift_range = 0.2\n",
    "        test_shear_range = 0.2\n",
    "        test_zoom_range = 0.2\n",
    "        test_vertical_flip = True\n",
    "        test_horizontal_flip = True\n",
    "        print (\"\\nFailed to load custom training image augmentation parameters ...\")\n",
    "        print (\"Loaded pre-set defaults ...\")\n",
    "        print (\"To switch off image augmentation during training, set --train_augmentation flag to False\")\n",
    "      test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                        rotation_range=test_rotation_range,\n",
    "                                        width_shift_range=test_width_shift_range,\n",
    "                                        height_shift_range=test_height_shift_range,\n",
    "                                        shear_range=test_shear_range,\n",
    "                                        zoom_range=test_zoom_range,\n",
    "                                        vertical_flip=test_vertical_flip,\n",
    "                                        horizontal_flip=test_horizontal_flip)\n",
    "      print (\"\\nCreated image augmentation pipeline for training images ...\")     \n",
    "      print (\"\\nImage augmentation parameters for training images:\")\n",
    "      print( \"\\n image rotation range = {},\\\n",
    "              \\n width shift range = {},\\\n",
    "              \\n height shift range = {}, \\\n",
    "              \\n shear range = {} ,\\\n",
    "              \\n zoom range = {}, \\\n",
    "              \\n enable vertical flip = {}, \\\n",
    "              \\n enable horizontal flip = {}\".format(test_rotation_range,\n",
    "                                                     test_width_shift_range,\n",
    "                                                     test_height_shift_range,\n",
    "                                                     test_shear_range,\n",
    "                                                     test_zoom_range,\n",
    "                                                     test_vertical_flip,\n",
    "                                                     test_horizontal_flip))\n",
    "  else:\n",
    "      test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "  return [train_datagen, test_datagen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s67CJ9KRwC7I"
   },
   "outputs": [],
   "source": [
    "def gen_model(args, enable_dropout):\n",
    "  if str((args.base_model[0]).lower()) == 'inceptionv4' or  \\\n",
    "     str((args.base_model[0]).lower()) == 'inception_v4' or \\\n",
    "     str((args.base_model[0]).lower()) == 'inception_resnet':\n",
    "      base_model = InceptionResNetV2(weights='imagenet', \\\n",
    "                                     include_top=False)\n",
    "      base_model_name = 'Inception version 4'\n",
    "  else:\n",
    "      base_model = InceptionV3(weights='imagenet', \n",
    "                               include_top=False)\n",
    "      base_model_name = 'Inception version 3'\n",
    "  print ('\\nBase model: ' + str(base_model_name))\n",
    "  nb_classes = len(glob.glob(args.train_dir[0] + \"/*\"))\n",
    "  model = add_top_layer(args, \n",
    "                        enable_dropout,\n",
    "                        base_model, \n",
    "                        nb_classes)\n",
    "  print (\"New top layer added to: \" + str(base_model_name))\n",
    "  return [model, base_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qyj2ocsgUVRP"
   },
   "outputs": [],
   "source": [
    "def train(args): \n",
    "  \"\"\"\n",
    "    A function that takes the user arguments and initiates a training session of the neural network.\n",
    "    \n",
    "    This function takes only one input: args\n",
    "    \n",
    "    Example usage:\n",
    "            \n",
    "        if train_model == True:\n",
    "            print (\"Training sesssion initiated ...\")\n",
    "            train(args)\n",
    "  \"\"\"    \n",
    "  \n",
    "  if not os.path.exists(args.output_dir[0]):\n",
    "    os.makedirs(args.output_dir[0])\n",
    "    \n",
    "  optimizer  = select_optimizer(args)\n",
    "  loss = args.loss[0]\n",
    "  checkpointer_savepath = os.path.join(args.output_dir[0]     +       \n",
    "                                       '/checkpoint/Transfer_learn_' +       \n",
    "                                       str(IM_WIDTH)  + '_'  + \n",
    "                                       str(IM_HEIGHT) + '_'  + '.h5')\n",
    "  \n",
    "  nb_train_samples = get_nb_files(args.train_dir[0])\n",
    "  nb_classes = len(glob.glob(args.train_dir[0] + \"/*\"))\n",
    "  \n",
    "  print (\"\\nTotal number of training samples = \" + str(nb_train_samples))\n",
    "  print (\"Number of training classes = \" + str(nb_classes))\n",
    "  \n",
    "  nb_val_samples = get_nb_files(args.val_dir[0])\n",
    "  nb_val_classes = len(glob.glob(args.val_dir[0] + \"/*\"))\n",
    "  \n",
    "  print (\"\\nTotal number of validation samples = \" + str(nb_val_samples))\n",
    "  print (\"Number of validation classes = \" + str(nb_val_classes))\n",
    "  \n",
    "  if nb_val_classes == nb_classes:\n",
    "      print (\"\\nInitiating training session ...\")\n",
    "  else:\n",
    "      print (\"\\nMismatched number of training and validation data classes ...\")\n",
    "      print (\"Unequal number of sub-folders found between train and validation directories ...\")\n",
    "      print (\"Each sub-folder in train and validation directroies are treated as a separate class ...\")\n",
    "      print (\"Correct this mismatch and re-run ...\")\n",
    "      print (\"\\nNow exiting ...\")\n",
    "      sys.exit(1)\n",
    "      \n",
    "  nb_epoch = int(args.epoch[0])\n",
    "  batch_size = int(args.batch[0])    \n",
    "  \n",
    "  [train_datagen, validation_datagen] = process_images(args)\n",
    "  \n",
    "  labels = generate_labels(args)\n",
    "  \n",
    "  train_dir = args.train_dir[0]\n",
    "  val_dir = args.val_dir[0]\n",
    "  \n",
    "  if args.normalize[0] and os.path.exists(args.root_dir[0]):\n",
    "      normalize(args, \n",
    "                labels, \n",
    "                move = False,\n",
    "                sub_sample = args.sub_sample[0])\n",
    "      train_dir = os.path.join(args.root_dir[0] + \n",
    "                               str ('/.tmp_train/'))\n",
    "      val_dir = os.path.join(args.root_dir[0] + \n",
    "                             str ('/.tmp_validation/'))\n",
    "      \n",
    "  print (\"\\nGenerating training data: ... \")\n",
    "  train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                      target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      class_mode='categorical')\n",
    "  \n",
    "  print (\"\\nGenerating validation data: ... \")\n",
    "  validation_generator = validation_datagen.flow_from_directory(val_dir,\n",
    "                                                          target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "                                                          batch_size=batch_size,\n",
    "                                                          class_mode='categorical')\n",
    "  \n",
    "  \n",
    "  [model, base_model] = gen_model(args, enable_dropout)\n",
    "    \n",
    "  model = process_model(args, \n",
    "                        model, \n",
    "                        base_model, \n",
    "                        optimizer, \n",
    "                        loss, \n",
    "                        checkpointer_savepath)\n",
    "            \n",
    "  print (\"\\nInitializing training with  class labels: \" + \n",
    "         str(labels))\n",
    "  \n",
    "  model_summary_ = args.model_summary[0]\n",
    "  \n",
    "  if model_summary_ == True:\n",
    "      print (model.summary())\n",
    "  else:\n",
    "      print (\"\\nSuccessfully loaded deep neural network classifier for training ...\")\n",
    "      print (\"\\nReady, Steady, Go ...\")\n",
    "      print (\"\\n\")\n",
    "        \n",
    "  if not os.path.exists(os.path.join(args.output_dir[0] + '/checkpoint/')):\n",
    "    os.makedirs(os.path.join(args.output_dir[0] + '/checkpoint/'))\n",
    "    \n",
    "  lr = args.learning_rate[0]\n",
    "    \n",
    "  earlystopper = EarlyStopping(patience=6, \n",
    "                               verbose=1)\n",
    "  checkpointer = ModelCheckpoint(checkpointer_savepath, \n",
    "                                 verbose=1,  \n",
    "                                 save_best_only=True)\n",
    "  learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                              patience=2,\n",
    "                                              mode = 'max',\n",
    "                                              epsilon=1e-4, \n",
    "                                              cooldown=1,\n",
    "                                              verbose=1, \n",
    "                                              factor=0.5, \n",
    "                                              min_lr=lr*1e-2)\n",
    "  \n",
    "  model_train = model.fit_generator(train_generator,\n",
    "                                    epochs=nb_epoch,\n",
    "                                    steps_per_epoch=nb_train_samples//20,\n",
    "                                    validation_data=validation_generator,\n",
    "                                    validation_steps=nb_val_samples//20,\n",
    "                                    class_weight='auto', \n",
    "                                    callbacks=[earlystopper, \n",
    "                                               learning_rate_reduction, \n",
    "                                               checkpointer])\n",
    "  \n",
    "  if args.fine_tune[0] == True:\n",
    "      save_model(args, \"_ft_\", model)\n",
    "      generate_plot(args, \"_ft_\", model_train)\n",
    "  else:\n",
    "      save_model(args, \"_tl_\", model)\n",
    "      generate_plot(args, \"_tl_\", model_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SdcTAv0qUcmz"
   },
   "outputs": [],
   "source": [
    "import types\n",
    "args=types.SimpleNamespace()\n",
    "args.base_model=['Inception_V4']\n",
    "args.frozen_layers=[NB_FROZEN_LAYERS]\n",
    "args.optimizer_val=['amsgrad']\n",
    "args.decay=[0.0]\n",
    "args.beta_2=[0.999]\n",
    "args.beta_1=[0.9]\n",
    "args.rho=[0.9]\n",
    "args.learning_rate=[1e-3]\n",
    "args.loss=['categorical_crossentropy']\n",
    "args.activation=['sigmoid']\n",
    "args.epsilon=[1e-8]\n",
    "args.dropout=[0.4]\n",
    "args.test_hflip=[True]\n",
    "args.test_vflip=[True]\n",
    "args.test_zoom=[True]\n",
    "args.test_shear=[True]\n",
    "args.train_model=[enable_training]\n",
    "args.output_dir=['./{}/'.format(dataset_id)]\n",
    "args.root_dir=['./']\n",
    "args.val_dir=['./{}/validation/'.format(dataset_id)]\n",
    "args.train_dir=['./{}/train/'.format(dataset_id)]\n",
    "args.epoch=[20]\n",
    "args.batch=[10]\n",
    "args.train_aug=[True]\n",
    "args.test_aug=[True]\n",
    "args.normalize=[False]\n",
    "args.sub_sample=[False]\n",
    "args.load_weights=[False]\n",
    "args.fine_tune=[True]\n",
    "args.load_checkpoint=[True]\n",
    "args.model_summary=[False]\n",
    "args.plot=[True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pTMzW3dnJWyq"
   },
   "outputs": [],
   "source": [
    "command = ['mkdir ./{}/validation/'.format(dataset_id),\n",
    "           'mkdir ./{}/validation/cats/'.format(dataset_id),\n",
    "           'mkdir ./{}/validation/dogs/'.format(dataset_id),\n",
    "           'cd ./{}/train/cats/ ; shuf -n 1000 -e * | xargs -i mv {} ../../../{}/validation/cats/'.format(dataset_id,\n",
    "                                                                                                          '{}',\n",
    "                                                                                                          dataset_id),\n",
    "           'cd ./{}/train/dogs/ ; shuf -n 1000 -e * | xargs -i mv {} ../../../{}/validation/dogs/'.format(dataset_id,\n",
    "                                                                                                          '{}',\n",
    "                                                                                                          dataset_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vLEBVMIehObI"
   },
   "outputs": [],
   "source": [
    "if setup:\n",
    "  execute_in_shell(command = command, \n",
    "                   verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s353hzKhX-Pk"
   },
   "outputs": [],
   "source": [
    "IM_WIDTH, IM_HEIGHT = 299, 299                                                  # Default input image size for Inception v3 and v4 architecture\n",
    "DEFAULT_EPOCHS = 100\n",
    "DEFAULT_BATCHES = 20\n",
    "FC_SIZE = 4096\n",
    "DEFAULT_DROPOUT = 0.1\n",
    "DEFAULT_NB_LAYERS_TO_FREEZE = 169\n",
    "\n",
    "verbose = False\n",
    "\n",
    "sgd = SGD(lr=1e-7, decay=0.5, momentum=1, nesterov=True)\n",
    "rms = RMSprop(lr=1e-7, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "ada = Adagrad(lr=1e-3, epsilon=1e-08, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-FJ2Q9bsXrs5"
   },
   "outputs": [],
   "source": [
    "if enable_training:\n",
    "  train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4iBnWYPPCH2"
   },
   "outputs": [],
   "source": [
    "command = ['rm ./Transfer_learn_299_299_{}.h5'.format(dataset_id),\n",
    "           'cp ./{}/checkpoint/Transfer_learn_299_299_.h5 ./Transfer_learn_299_299_{}.h5'.format(dataset_id,\n",
    "                                                                                                 dataset_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7hMNYvd0TWGd"
   },
   "outputs": [],
   "source": [
    "execute_in_shell(command = command, \n",
    "                   verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UtA6EKajNGh6"
   },
   "outputs": [],
   "source": [
    "file_dir = './'\n",
    "file_name = 'Transfer_learn_299_299_{}.h5'.format(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6o8ODppKNG-p"
   },
   "outputs": [],
   "source": [
    "if colab_mode:\n",
    "    drive = cloud_authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "88MVBatANE05"
   },
   "outputs": [],
   "source": [
    "if upload_weights and colab_mode:\n",
    "  googledrive_save(file_name = file_name,\n",
    "                   file_dir = file_dir,\n",
    "                   upload = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UtQJJLkY0y3U"
   },
   "outputs": [],
   "source": [
    "model = gen_model(args, \n",
    "                  enable_dropout)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XAX0OmDbQbc1"
   },
   "outputs": [],
   "source": [
    "model.load_weights('./Transfer_learn_299_299_{}.h5'.format(dataset_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRPMI4TluEaH"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MfYOQD6xN5xE"
   },
   "outputs": [],
   "source": [
    "command = ['apt-get install -y graphviz libgraphviz-dev && pip3 install pydot graphviz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NSUerE_cjPnt"
   },
   "outputs": [],
   "source": [
    "if setup:\n",
    "  execute_in_shell(command = command, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ruodvGGCN6hy"
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model \n",
    "import pydot \n",
    "import graphviz # apt-get install -y graphviz libgraphviz-dev && pip3 install pydot graphviz \n",
    "from IPython.display import SVG \n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V9IUCopJN68c"
   },
   "outputs": [],
   "source": [
    "output_dir = './'\n",
    "plot_model(model, to_file= output_dir + '/model_summary_plot.png') \n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2tjhL6-u0slD"
   },
   "outputs": [],
   "source": [
    "! wget https://cdn-images-1.medium.com/max/1600/1*mONNI1lG9VuiqovpnYqicA.jpeg -O cats_01.jpeg\n",
    "! wget https://www.petspyjamas.com/uploads/2013/07/can-cats-and-dogs-be-friends-6.jpg -O cat_and_dog_01.jpg\n",
    "! wget https://github.com/rahulremanan/python_tutorial/raw/master/Machine_Vision/01_Transfer_Learning/media/goose_the_cat.png -O goose_the_cat.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xu4SO7S76oUI"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import keras.applications.inception_resnet_v2 as InceptionResNetV2\n",
    "import tqdm\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import UpSampling2D, Conv2D\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u0TfN5OnjIRM"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  labels_json='./dogs_vs_cats/trained_labels.json'\n",
    "  with open(labels_json) as json_file:\n",
    "     labels = json.load(json_file)\n",
    "  print (labels)\n",
    "except:\n",
    "  labels = [\"cats\", \"dogs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_cEh5hRGFxX8"
   },
   "outputs": [],
   "source": [
    "PRE_PROCESSOR = preprocess_input_inceptionv4\n",
    "MODEL = model\n",
    "INPUT_IMG_FILE = './goose_the_cat.png'\n",
    "LABELS= labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2TL9EOqcGBox"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XvoyBm_7Fz2G"
   },
   "outputs": [],
   "source": [
    "img=mpimg.imread(INPUT_IMG_FILE)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aBhcUGnRMtgI"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IUzPpfHJGFgv"
   },
   "outputs": [],
   "source": [
    "def class_activation_map(INPUT_IMG_FILE=None,\n",
    "                         PRE_PROCESSOR=None,\n",
    "                         LABEL_DECODER=None,\n",
    "                         MODEL=None,\n",
    "                         LABELS=None,\n",
    "                         IM_WIDTH=299,\n",
    "                         IM_HEIGHT=299,\n",
    "                         CONV_LAYER='conv_7b',\n",
    "                         URL_MODE=False,\n",
    "                         FILE_MODE=False,\n",
    "                         EVAL_STEPS=10,\n",
    "                         HEATMAP_SHAPE=[8,8],\n",
    "                         BENCHMARK=True):\n",
    "  \"\"\"\n",
    "     A function to visualize class activation maps.\n",
    "     \n",
    "     Also generate a Bayesian class activation map, that outputs a list of \n",
    "     heatmaps summarizing the model uncertainty.\n",
    "     \n",
    "     Currently has performance scalability issues for number of evaluation steps, \n",
    "     due to the nature in which Tensorflow computes gradeints.\n",
    "     \n",
    "     See the description of the problem here: https://stackoverflow.com/questions/36245481/tensorflow-slow-performance-when-getting-gradients-at-inputs\n",
    "          \n",
    "  \"\"\"\n",
    "  #K.clear_session()\n",
    "  if INPUT_IMG_FILE == None:\n",
    "    print ('No input file specified to generate predictions ...')\n",
    "    return\n",
    "  \n",
    "  if URL_MODE:\n",
    "    response = requests.get(INPUT_IMG_FILE)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    img = img.resize((IM_WIDTH, IM_HEIGHT))\n",
    "  elif FILE_MODE:\n",
    "    img = INPUT_IMG_FILE\n",
    "  else:\n",
    "    img = image.load_img(INPUT_IMG_FILE, target_size=(IM_WIDTH, IM_HEIGHT))\n",
    "    \n",
    "  x = img\n",
    "  \n",
    "  if not FILE_MODE:\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    if PRE_PROCESSOR !=None:\n",
    "      preprocess_input = PRE_PROCESSOR\n",
    "      x = preprocess_input(x)\n",
    "  \n",
    "  model = MODEL\n",
    "  if model == None:\n",
    "    print ('No input model specified to generate predictions ...')\n",
    "    return\n",
    "  labels = LABELS\n",
    "  \n",
    "  heatmaps = []\n",
    "  \n",
    "  last_conv_layer = model.get_layer(CONV_LAYER)  \n",
    "  feature_size = tensor_featureSizeExtractor(last_conv_layer)\n",
    "  \n",
    "  model_input = model.input\n",
    "  model_output = model.output\n",
    "  last_conv_layer_out = last_conv_layer.output\n",
    "  \n",
    "  iterate_input = []\n",
    "  \n",
    "  pred_labels = []\n",
    "  out_labels = []\n",
    "  \n",
    "  probabilities = np.empty((0,len(labels)), float)\n",
    "  \n",
    "  for step in (range(EVAL_STEPS)):\n",
    "    input_img = x\n",
    "    \n",
    "    startTime = time.time()\n",
    "    \n",
    "    preds = model.predict(x, batch_size=1)    \n",
    "    preds_endTime = time.time()    \n",
    "    probability = preds.flatten()\n",
    "    probabilities = np.append(probabilities, \n",
    "                              np.array([probability]), \n",
    "                              axis=0)\n",
    "    \n",
    "    if labels !=None:\n",
    "      pred_label = labels[np.argmax(probability)]\n",
    "      pred_labels.append(pred_label)\n",
    "      out_labels.append(pred_label)\n",
    "      print('PREDICTION: {}'.format(pred_label))\n",
    "      print('ACCURACY: {}'.format(preds[0]))\n",
    "      del pred_label\n",
    "    elif LABEL_DECODER !=None:\n",
    "      pred_label = pd.DataFrame(LABEL_DECODER(preds, top=3)[0],\n",
    "                                columns=['col1',\n",
    "                                         'category',\n",
    "                                         'probability']).iloc[:,1:]\n",
    "      pred_labels.append(pred_label.loc[0,'category'])\n",
    "      out_labels.append(pred_label.loc[0,'category'])\n",
    "      print('PREDICTION:',pred_label.loc[0,'category'])\n",
    "      del pred_label\n",
    "    else:\n",
    "      print ('No labels will be generated ...')\n",
    "      \n",
    "    pred_labels = set(pred_labels)\n",
    "    pred_labels = list(pred_labels)  \n",
    "    argmax = np.argmax(probability)\n",
    "    \n",
    "    heatmap_startTime = time.time() \n",
    "    \n",
    "    output = model_output[:, argmax] \n",
    "    \n",
    "    model_endTime = time.time() \n",
    "    \n",
    "    grads = K.gradients(output, \n",
    "                        last_conv_layer_out)[0]\n",
    "    pooled_grads = K.mean(grads, \n",
    "                          axis=(0, 1, 2))      \n",
    "    iterate = K.function([model_input], [pooled_grads,\n",
    "                                         last_conv_layer_out[0]])    \n",
    "    pooled_grads_value, conv_layer_output_value = iterate([input_img])\n",
    "    \n",
    "    grad_endTime = time.time()\n",
    "    \n",
    "    for i in range(feature_size):\n",
    "      conv_layer_output_value[:,:,i] *= pooled_grads_value[i]\n",
    "      \n",
    "    iter_endTime = time.time()\n",
    "    \n",
    "    heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= np.max(heatmap)\n",
    "    \n",
    "    heatmap_endTime = time.time()  \n",
    "    \n",
    "    try:\n",
    "      heatmaps.append(heatmap)\n",
    "      if EVAL_STEPS >1:\n",
    "        del probability\n",
    "        del heatmap\n",
    "        del output\n",
    "        del grads\n",
    "        del pooled_grads\n",
    "        del iterate\n",
    "        del pooled_grads_value\n",
    "        del conv_layer_output_value\n",
    "        del input_img\n",
    "        gc.collect()\n",
    "    except:\n",
    "      print ('Failed updating heatmaps ...')\n",
    "    \n",
    "    endTime = time.time()\n",
    "    \n",
    "    predsTime = preds_endTime - startTime\n",
    "    gradsTime = grad_endTime - model_endTime\n",
    "    iterTime = iter_endTime - grad_endTime\n",
    "    heatmapTime = heatmap_endTime - heatmap_startTime\n",
    "    executionTime = endTime - startTime\n",
    "    model_outputTime = model_endTime - heatmap_startTime\n",
    "    \n",
    "    if BENCHMARK:\n",
    "      print ('Heatmap generation time: {} seconds ...'. format(heatmapTime))\n",
    "      print ('Gradient generation time: {} seconds ...'.format(gradsTime))\n",
    "      print ('Iteration loop execution time: {} seconds ...'.format(iterTime))\n",
    "      print ('Model output generation time: {} seconds'.format(model_outputTime))\n",
    "      print ('Prediction generation time: {} seconds ...'.format(predsTime))\n",
    "      print ('Completed processing {} out of {} steps in {} seconds ...'.format(int(step+1), int(EVAL_STEPS), float(executionTime)))\n",
    "      print ('\\n')\n",
    "      print ('Percentage time spent generating heatmap: {}'.format((heatmapTime/executionTime)*100))\n",
    "      print ('Percentage time spent generating gradients: {}'.format((gradsTime/executionTime)*100))\n",
    "      print ('Percentage time spent generating iteration loop: {}'.format((iterTime/executionTime)*100))\n",
    "      print ('Percentage time spent generating model outputs: {}'.format((model_outputTime/executionTime)*100))\n",
    "      print ('Percentage time spent generating predictions: {}'.format((predsTime/executionTime)*100))\n",
    "      print ('\\n')\n",
    "  if EVAL_STEPS >1:\n",
    "    heatmap_sum = heatmaps[0]\n",
    "    for i in range(len(heatmaps)-1):\n",
    "      if i<= len(heatmaps):\n",
    "        heatmap_sum = np.nan_to_num(heatmaps[i+1])+np.nan_to_num(heatmap_sum)\n",
    "    print (heatmap_sum)\n",
    "    mean_heatmap = heatmap_sum/len(heatmaps)\n",
    "  else:\n",
    "    mean_heatmap = heatmap\n",
    "    \n",
    "  mean = np.matrix.mean(np.asmatrix(probabilities), axis=0)\n",
    "  stdev = np.matrix.std(np.asmatrix(probabilities), axis=0)\n",
    "  \n",
    "  accuracy = np.matrix.tolist(mean)[0][np.argmax(mean)]\n",
    "  uncertainty = np.matrix.tolist(stdev)[0][np.argmax(mean)]\n",
    "  \n",
    "  return [mean_heatmap, accuracy, uncertainty, pred_labels, heatmaps, out_labels, probabilities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6hqvX0S1I0eR"
   },
   "outputs": [],
   "source": [
    "def tensor_featureSizeExtractor(last_conv_layer):\n",
    "  if len(last_conv_layer.output.get_shape().as_list()) == 4:\n",
    "    feature_size = last_conv_layer.output.get_shape().as_list()[3]\n",
    "    return feature_size\n",
    "  else:\n",
    "    print ('Received tensor shape: {} instead of expected shape: 4'.format(len(last_conv_layer.output.get_shape().as_list())))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ygms0EOhH3ZV"
   },
   "outputs": [],
   "source": [
    "def heatmap_overlay(INPUT_IMG_FILE,\n",
    "                    HEATMAP,\n",
    "                    THRESHOLD=0.8):\n",
    "  img = cv2.imread(INPUT_IMG_FILE)\n",
    "  \n",
    "  heatmap = cv2.resize(HEATMAP, (img.shape[1], img.shape[0]))\n",
    "  heatmap = np.uint8(255 * heatmap)\n",
    "  heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "  hif = THRESHOLD\n",
    "  #superimposed_img = heatmap * hif + img\n",
    "  superimposed_img = cv2.addWeighted(img,THRESHOLD,heatmap,1-THRESHOLD,0)\n",
    "  return [superimposed_img, heatmap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GoM8VyP6H3hM"
   },
   "outputs": [],
   "source": [
    "if bayesian_cam:\n",
    "  output = class_activation_map(INPUT_IMG_FILE=INPUT_IMG_FILE,\n",
    "                                PRE_PROCESSOR=PRE_PROCESSOR,\n",
    "                                MODEL=MODEL,\n",
    "                                LABELS=LABELS,\n",
    "                                IM_WIDTH=299,\n",
    "                                IM_HEIGHT=299,\n",
    "                                CONV_LAYER='conv_7b',\n",
    "                                EVAL_STEPS=100)\n",
    "  HEATMAP = output[0]\n",
    "  \n",
    "  plt.matshow(HEATMAP)\n",
    "  plt.show()\n",
    "  print (output[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CtsZU9nSMDX5"
   },
   "outputs": [],
   "source": [
    "if bayesian_cam:\n",
    "  heatmap_output = heatmap_overlay(INPUT_IMG_FILE,\n",
    "                                   HEATMAP,\n",
    "                                   THRESHOLD=0.8)\n",
    "  superimposed_img = heatmap_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k0CZbyuWMDQC"
   },
   "outputs": [],
   "source": [
    "if bayesian_cam:\n",
    "  output_file = './class_activation_map.jpeg'\n",
    "  cv2.imwrite(output_file, superimposed_img)\n",
    "\n",
    "  img=mpimg.imread(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gLNQKACWMDIJ"
   },
   "outputs": [],
   "source": [
    "if bayesian_cam:\n",
    "  plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QfYPYx7s2EkC"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    " \n",
    "  \n",
    "if bayesian_cam:\n",
    "  heatmaps=output[4]\n",
    "  labels=output[5]\n",
    "  img_array = []\n",
    "\n",
    "  for i in range(len(heatmaps)):\n",
    "    HEATMAP = heatmaps[i]\n",
    "    LABEL = labels[i]\n",
    "    heatmap_output = heatmap_overlay(INPUT_IMG_FILE,\n",
    "                                     HEATMAP,\n",
    "                                     THRESHOLD=0.7)\n",
    "    height, width, layers = heatmap_output[0].shape\n",
    "    size = (width,height)\n",
    "    superimposed_img = heatmap_output[0]\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(superimposed_img,'{}'.format(LABEL),(10,100), font, 4,(255,255,255),2)\n",
    "    img_array.append(np.uint8(superimposed_img))\n",
    " \n",
    "  out = cv2.VideoWriter('bayesian_class_activation_maps.avi',cv2.VideoWriter_fourcc(*'DIVX'), 8, size)\n",
    " \n",
    "  for i in range(len(img_array)):\n",
    "      out.write(img_array[i])\n",
    "  out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dqvqg3li9WiK"
   },
   "outputs": [],
   "source": [
    "! apt-get install handbrake handbrake-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mx8eILl-70mz"
   },
   "outputs": [],
   "source": [
    "! HandBrakeCLI -i ./bayesian_class_activation_maps.avi -o ./bayesian_class_activation_maps.mp4 -e x264 -q 22 -r 15 -B 64 -X 480 -O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mYroEljv-Bbf"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "video = io.open('./bayesian_class_activation_maps.mp4', 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''<video alt=\"test\" controls>\n",
    "                  <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "               </video>'''.format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vyy9E2gYMDAf"
   },
   "outputs": [],
   "source": [
    "download_output=True\n",
    "from google.colab import files\n",
    "if download_output and bayesian_cam:\n",
    "  files.download('./bayesian_class_activation_maps.mp4')\n",
    "  files.download('./class_activation_map.jpeg')\n",
    "  files.download('./model_summary_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3e5z_MidkJrx"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hYZ5EJJ4Zc4L"
   },
   "outputs": [],
   "source": [
    "target_size = (IM_WIDTH, IM_HEIGHT)\n",
    "INPUT_IMAGE=\"./dogs_vs_cats/test/test/4106.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "meMZlJuwZGRq"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image as PyImage\n",
    "PyImage(INPUT_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vekR1_QRVA-1"
   },
   "outputs": [],
   "source": [
    "img = Image.open(INPUT_IMAGE)\n",
    "if img.size != target_size:\n",
    "    img = img.resize((299, 299))\n",
    "_x_ = image.img_to_array(img)\n",
    "_x_ = np.expand_dims(_x_, axis=0)\n",
    "_x_ = preprocess_input(_x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fW6kuOmDKjRX"
   },
   "outputs": [],
   "source": [
    "model.predict(_x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_GSSGR4QeGy"
   },
   "outputs": [],
   "source": [
    "def plot_preds(preds, labels, timestr):\n",
    "  output_loc = args.output_dir[0]\n",
    "  output_file_preds = os.path.join(output_loc+\"//preds_out_\"+timestr+\".png\")\n",
    "  fig = plt.figure()\n",
    "  plt.axis('on')\n",
    "  labels = labels\n",
    "  plt.barh([0, 1], preds, alpha=0.5)\n",
    "  plt.yticks([0, 1], labels)\n",
    "  plt.xlabel('Probability')\n",
    "  plt.xlim(0,1.01)\n",
    "  plt.tight_layout()\n",
    "  fig.savefig(output_file_preds, dpi=fig.dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5nFne3I2tIz9"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  labels_json='./dogs_vs_cats/trained_labels.json'\n",
    "  with open(labels_json) as json_file:\n",
    "     labels = json.load(json_file)\n",
    "  print (labels)\n",
    "except:\n",
    "  labels = [\"cats\", \"dogs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BpFNoVDhQfHH"
   },
   "outputs": [],
   "source": [
    "def predict(model=None, \n",
    "            img=None, \n",
    "            labels=None,\n",
    "            target_size=None, \n",
    "            bayesian_inference=True, \n",
    "            eval_steps=2, \n",
    "            detection_threshold=0.5,\n",
    "            prediction_max=True,\n",
    "            verbose=False):\n",
    "  \n",
    "  if verbose:\n",
    "    print (\"Running prediction model on the image file ...\")\n",
    "  if img.size != target_size:\n",
    "    img = img.resize(target_size)\n",
    "\n",
    "  _x_ = image.img_to_array(img)\n",
    "  _x_ = np.expand_dims(_x_, axis=0)\n",
    "  _x_ = preprocess_input(_x_)\n",
    "  \n",
    "  probabilities = np.empty((0,len(labels)), float)\n",
    "  \n",
    "  if bayesian_inference:\n",
    "\n",
    "    for step in (range(eval_steps)):\n",
    "      start = time.time()\n",
    "      preds = model.predict(_x_, batch_size=1)\n",
    "      probability = preds.flatten()\n",
    "      probabilities = np.append(probabilities, np.array([probability]), axis=0)\n",
    "      end = time.time()\n",
    "      execution_time = end - start\n",
    "      if verbose:\n",
    "        print ('Execution time: {} seconds ...'.format(execution_time))\n",
    "    \n",
    "    mean = np.matrix.mean(np.asmatrix(probabilities), axis=0)\n",
    "    max_ = np.matrix.max(np.asmatrix(probabilities), axis=0)\n",
    "      \n",
    "    if prediction_max:\n",
    "      out = max_\n",
    "    else:\n",
    "      out = mean\n",
    "      \n",
    "    predictions = np.matrix.tolist(out)[0]  \n",
    "    uncertainty = np.matrix.std(np.asmatrix(probabilities), axis=0)\n",
    "    uncertainty = np.matrix.tolist(uncertainty)[0][np.argmax(predictions)]\n",
    "    \n",
    "    preds_label = []\n",
    "    for i in range(len(labels)):\n",
    "      if predictions[i] >= detection_threshold:\n",
    "        preds_label.append(labels[i])\n",
    "    \n",
    "    accuracy = predictions[np.argmax(predictions)]\n",
    "    probabilities = np.matrix.tolist(probabilities)[0]\n",
    "    \n",
    "    return predictions, preds_label, accuracy, uncertainty, max_, probabilities\n",
    "  else:\n",
    "    preds = model.predict(_x_, batch_size=1)\n",
    "    probability = preds.flatten()\n",
    "    preds_label = labels[np.argmax(probability)]\n",
    "    accuracy = probability[np.argmax(probability)]\n",
    "    return preds[0], preds_label, accuracy, probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nGQqZffdYRMY"
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    img = Image.open(INPUT_IMAGE)\n",
    "    bayesian_inference=True\n",
    "    preds = predict(model, \n",
    "                    img, \n",
    "                    labels,\n",
    "                    target_size, \n",
    "                    bayesian_inference, \n",
    "                    eval_steps=100,\n",
    "                    detection_threshold=0.5,\n",
    "                    prediction_max=False,\n",
    "                    verbose=False)\n",
    "    print (preds[1])\n",
    "    print (preds[0])\n",
    "    print ('This picture contain: {}'.format(preds[1]))\n",
    "    print ('Predicted with accuracy of: {}'.format(preds[2]))\n",
    "    if bayesian_inference:\n",
    "      try:\n",
    "        print ('Uncertainty in prediction: {}'.format(preds[3]))\n",
    "      except:\n",
    "        print ('Uncertainty in prediction: {}'.format('Unknown'))\n",
    "    timestr = generate_timestamp()\n",
    "    plot_preds(preds[0], labels, timestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_cIDUI-S0bxZ"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from keras.preprocessing import image\n",
    "\n",
    "bayesian_inference=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DaCNL9REChX0"
   },
   "outputs": [],
   "source": [
    "args.image_url = ['https://cdn-images-1.medium.com/max/1600/1*mONNI1lG9VuiqovpnYqicA.jpeg']\n",
    "target_size = (IM_WIDTH, IM_HEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TG1eQucg0bAv"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image as PyImage\n",
    "from IPython.core.display import HTML \n",
    "PyImage(url = args.image_url[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HrkCbwONKrBv"
   },
   "outputs": [],
   "source": [
    "if args.image_url is not None:\n",
    "    response = requests.get(args.image_url[0])\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    preds = predict(model=model, \n",
    "                    img=img, \n",
    "                    labels=labels,\n",
    "                    target_size=target_size, \n",
    "                    bayesian_inference=True, \n",
    "                    eval_steps=50, \n",
    "                    detection_threshold=0.5,\n",
    "                    prediction_max=False)\n",
    "    print (preds[1])\n",
    "    print (preds[0])\n",
    "    print ('This picture contain: {}'.format(preds[1]))\n",
    "    print ('Predicted with accuracy of: {}'.format(preds[2]))\n",
    "    if bayesian_inference:\n",
    "      try:\n",
    "        print ('Uncertainty in prediction: {}'.format(preds[3]))\n",
    "      except:\n",
    "        print ('Uncertainty in prediction: {}'.format('Unknown'))\n",
    "    timestr = generate_timestamp()\n",
    "    plot_preds(preds[0], labels, timestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ABunw9n-hlrG"
   },
   "outputs": [],
   "source": [
    "#from IPython.display import Image as PyImage\n",
    "#PyImage(\"./dogs_vs_cats/preds_out_{}.png\".format(timestr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ULRhyfuhtN1-"
   },
   "outputs": [],
   "source": [
    "args.image_url=['http://static.cdnbridge.com/resources/18/160536/picture/16/85388054.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v1E9Uwcsu5u-"
   },
   "outputs": [],
   "source": [
    "PyImage(url = args.image_url[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EACmb3QQuszG"
   },
   "outputs": [],
   "source": [
    "if args.image_url is not None:\n",
    "    response = requests.get(args.image_url[0])\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    preds = predict(model=model, \n",
    "                    img=img, \n",
    "                    labels=labels,\n",
    "                    target_size=target_size, \n",
    "                    bayesian_inference=True, \n",
    "                    eval_steps=50,\n",
    "                    detection_threshold=0.5,\n",
    "                    prediction_max=False)\n",
    "    print (preds[1])\n",
    "    print (preds[0])\n",
    "    print ('This picture contain: {}'.format(preds[1]))\n",
    "    print ('Predicted with accuracy of: {}'.format(preds[2]))\n",
    "    if bayesian_inference:\n",
    "      try:\n",
    "        print ('Uncertainty in prediction: {}'.format(preds[3]))\n",
    "      except:\n",
    "        print ('Uncertainty in prediction: {}'.format('Unknown'))\n",
    "    timestr = generate_timestamp()\n",
    "    plot_preds(preds[0], labels, timestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rOtwBjJ_hjJJ"
   },
   "outputs": [],
   "source": [
    "#from IPython.display import Image as PyImage\n",
    "#PyImage(\"./dogs_vs_cats/preds_out_{}.png\".format(timestr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-N46x005uvQF"
   },
   "outputs": [],
   "source": [
    "args.image_url=['https://www.shelterluv.com/sites/default/files/animal_pics/3451/2019/02/13/08/20190213083008.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kH95o15Nw-3D"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image as PyImage\n",
    "from IPython.core.display import HTML \n",
    "PyImage(url = args.image_url[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4OYuA6qw_zr"
   },
   "outputs": [],
   "source": [
    "if args.image_url is not None:\n",
    "    response = requests.get(args.image_url[0])\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    preds = predict(model, \n",
    "                    img,\n",
    "                    labels,\n",
    "                    target_size, \n",
    "                    bayesian_inference, \n",
    "                    eval_steps=50,\n",
    "                    detection_threshold=0.5,\n",
    "                    prediction_max=False)\n",
    "    print (preds[1])\n",
    "    print (preds[0])\n",
    "    print ('This picture contain: {}'.format(preds[1]))\n",
    "    print ('Predicted with accuracy of: {}'.format(preds[2]))\n",
    "    if bayesian_inference:\n",
    "      try:\n",
    "        print ('Uncertainty in prediction: {}'.format(preds[3]))\n",
    "      except:\n",
    "        print ('Uncertainty in prediction: {}'.format('Unknown'))\n",
    "    timestr = generate_timestamp()\n",
    "    plot_preds(preds[0], labels, timestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hb_hPEXqhe_C"
   },
   "outputs": [],
   "source": [
    "#from IPython.display import Image as PyImage\n",
    "#PyImage(\"./dogs_vs_cats/preds_out_{}.png\".format(timestr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LvZOZCIvxF0e"
   },
   "outputs": [],
   "source": [
    "args.image_url=['https://www.petspyjamas.com/uploads/2013/07/can-cats-and-dogs-be-friends-6.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wH827FyFL4L9"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image as PyImage\n",
    "from IPython.core.display import HTML \n",
    "PyImage(url = args.image_url[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z7OsIeqtyUVm"
   },
   "outputs": [],
   "source": [
    "if args.image_url is not None:\n",
    "    response = requests.get(args.image_url[0])\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    preds = predict(model, \n",
    "                    img, \n",
    "                    labels,\n",
    "                    target_size, \n",
    "                    bayesian_inference, \n",
    "                    eval_steps=10,\n",
    "                    detection_threshold=0.75,\n",
    "                    prediction_max=False,\n",
    "                    )\n",
    "    print (preds[1])\n",
    "    print (preds[0])\n",
    "    print ('This picture contain: {}'.format(preds[1]))\n",
    "    print ('Predicted with accuracy of: {}'.format(preds[2]))\n",
    "    if bayesian_inference:\n",
    "      try:\n",
    "        print ('Uncertainty in prediction: {}'.format(preds[3]))\n",
    "      except:\n",
    "        print ('Uncertainty in prediction: {}'.format('Unknown'))\n",
    "    timestr = generate_timestamp()\n",
    "    plot_preds(preds[0], labels, timestr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GPU6CTkDf5A-"
   },
   "outputs": [],
   "source": [
    "#from IPython.display import Image as PyImage\n",
    "#PyImage(\"./dogs_vs_cats/preds_out_{}.png\".format(timestr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xz-UIcR8jeyI"
   },
   "source": [
    "## Create Kaggle submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yYAbJtuPkvoY"
   },
   "outputs": [],
   "source": [
    "rotation_range = 30\n",
    "width_shift_range = 0.2\n",
    "height_shift_range = 0.2\n",
    "shear_range = 0.2\n",
    "zoom_range = 0.2\n",
    "vertical_flip = True\n",
    "horizontal_flip = True\n",
    "img_width = 299\n",
    "img_height = 299\n",
    "\n",
    "TEST_DIR = './dogs_vs_cats/test/test/'\n",
    "OUTPUT_FILE = \"./dogs_vs_cats_InceptionResNetV2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HoRt-NUmxwoK"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import gc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KR0sUQBOb1vX"
   },
   "outputs": [],
   "source": [
    "id_ = []\n",
    "pred_labels = []\n",
    "verbose = False\n",
    "for i in tqdm(os.listdir(TEST_DIR)):\n",
    "  start = time.time()\n",
    "  img = Image.open(os.path.join(TEST_DIR+i))\n",
    "  preds = predict(model=model, \n",
    "                  img=img, \n",
    "                  labels=labels,\n",
    "                  target_size=target_size, \n",
    "                  bayesian_inference=True, \n",
    "                  eval_steps=1,\n",
    "                  detection_threshold=0.75,\n",
    "                  prediction_max=False)  \n",
    "  try:\n",
    "    pred_ = np.argmax(preds[0])\n",
    "    if pred_ == 0 and preds[0][pred_]<0.5:\n",
    "      pred_label = preds[0][pred_]\n",
    "    elif pred_ == 0 and preds[0][pred_]>0.5:\n",
    "      pred_label = 1-preds[0][pred_]\n",
    "    elif pred_ == 1 and preds[0][pred_]<0.5:\n",
    "      pred_label = 1-preds[0][pred_]\n",
    "    elif pred_ == 1 and preds[0][pred_]>0.5:\n",
    "      pred_label = preds[0][pred_]\n",
    "    else:\n",
    "      pred_label = 0.5\n",
    "  except:\n",
    "    pred_label = 0.5\n",
    "  id_.append(i)\n",
    "  pred_labels.append(pred_label)\n",
    "  end = time.time()\n",
    "  if verbose:\n",
    "    print ('Step: {} out of: {} in: {} seconds'.format(len(id_), \n",
    "                                                       len(os.listdir(TEST_DIR)),\n",
    "                                                       (end-start)))\n",
    "    print ('Completed generating predictions for: {} as: {}'.format(i,\n",
    "                                                                    pred_label))\n",
    "    \n",
    "solution = pd.DataFrame({\"id\":id_, \n",
    "                         \"label\":pred_labels})\n",
    "cols = ['label']\n",
    "\n",
    "for col in cols:\n",
    "    solution[col] = solution[col].map(lambda x: str(x).lstrip('[').rstrip(']')).astype(str)\n",
    "\n",
    "solution.to_csv(OUTPUT_FILE, \n",
    "                index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mmSZDuF1f4S9"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "STygg7U0f_DL"
   },
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Dogs_vs_Cats_Bayesian_classifier",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
